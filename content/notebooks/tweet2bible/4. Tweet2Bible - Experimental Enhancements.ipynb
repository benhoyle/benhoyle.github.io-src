{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: 4. Tweet2Bible - Experimental Enhancements\n",
    "Tags: improving_results\n",
    "Authors: Ben Hoyle\n",
    "Summary: This post looks at some experimental approaches to improve our matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tweet2Bible - Experimental Enhancements\n",
    "\n",
    "So we know how our off-the-shelf approaches work. Now we can try to improve the relevance of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Overview\n",
    "\n",
    "Looking at research ideas in this space, the \"similarity\" score aspect is relatively fixed: most ideas still look at generating a normalised similarity score, e.g. between 0 and 1, that can be used to score and rank results. We can see this with our three off-th-shelf approaches: each naturally generates a score between 0 and 1.\n",
    "\n",
    "The innovation arises in how the similarity score is computed. Most techniques look at comparing n-dimensional vectors using some form of distance measure (e.g. cosine or Euclidean distance). Hence, techniques differ in how these vectors are calculated.\n",
    "\n",
    "Many techniques look at trying to identify \"salient\" features in the text, and to represent these in the n-dimensional vector. One challenge of doing this is to avoid averaging effects over the text portion that may \"dilute\" the salient features. For example, a bad similarity measure would generate similar n-dimensional vectors based on the occurrence of \"and\" and \"the\" in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "#### Can we turn our unsupervised task into a supervised task?\n",
    "\n",
    "At the moment we have an unsupervised task: we do not have data representing pairs of tweets and Bible passages.\n",
    "\n",
    "#### Topics for each tweet and passage\n",
    "\n",
    "How do human beings look to arrange information for better retrieval? One way is to use categories or tags. For example, Wordpress blog posts use [categories and tags as described here](http://www.wpbeginner.com/beginners-guide/categories-vs-tags-seo-best-practices-which-one-is-better/):\n",
    "\n",
    "> Categories are meant for broad grouping of your posts. Think of these as general topics or the table of contents for your site. Categories are there to help identify what your blog is really about. It is to assist readers finding the right type of content on your site. Categories are hierarchical, so you can sub-categories.\n",
    "\n",
    "> Tags are meant to describe specific details of your posts. Think of these as your site’s index words. They are the micro-data that you can use to micro-categorize your content. Tags are not hierarchical.\n",
    "\n",
    "In unsupervised machine learning approaches, we can use clustering or topic-modelling to determine categories. This is what the Gensim approaches do, when they apply LSI. However, the LSI \"topics\" are often difficult to convert to a natural language category. Other approaches may be to assign categories to each portion of text (e.g. via conventional classification techniques), to pair portions based on assigned categories, and then to look at similarity within the categories based on text content (e.g. based on \"tags\").\n",
    "\n",
    "Clustering allows us to assign categories in an unsupervised manner. Most clustering algorithms again work on n-dimensional vectors, so we still have an issue of how to convert our text into a vector. Common approaches use [Term Frequency - Inverse Document Frequency (TD-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to generate a vector representation.\n",
    "\n",
    "#### Extract and weigh features\n",
    "\n",
    "When thinking about what \"makes a match\", we concentrate on one or two core features. These may be similar to categories or tags, or may be more broadly \"things\". \n",
    "\n",
    "Onr possible approach is to use linguistic information to extract and/or weight features. For example, do we more naturally summarise based on nouns than verbs? Also, can we \"simplify\" a sentence or phrase, producing a form that sums up a frame of reference, which is then populated with the detail?\n",
    "\n",
    "One common structure for (English) sentences is that of subject, verb, object. Could we match based on a tuple containing this information?\n",
    "\n",
    "#### Word Embeddings\n",
    "\n",
    "Word embeddings are a useful recent advance that we could take advantage of. By using word embeddings we can possibly loosen the ties to specific character sequences, and better capture an abstract concept behind certain terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"processed_data.pkl\", 'rb') as f:\n",
    "    tweets, bible_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9806 tweets.\n",
      "We have 31102 Bible passages.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {0} tweets.\".format(len(tweets)))\n",
    "print(\"We have {0} Bible passages.\".format(len(bible_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Genesis 1:2',\n",
       " \"Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_eg = nlp(bible_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the earth,\n",
       " Darkness,\n",
       " the surface,\n",
       " the deep,\n",
       " God's Spirit,\n",
       " the surface,\n",
       " the waters]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nlp_eg.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earth\n",
      "Darkness\n",
      "surface\n",
      "deep\n",
      "surface\n",
      "waters\n"
     ]
    }
   ],
   "source": [
    "for token in nlp_eg:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earth\n",
      "[ -1.78159997e-01  -1.11249998e-01  -1.09540001e-01  -1.85010001e-01\n",
      "   1.04309998e-01   3.37399989e-01  -1.17749996e-01   5.97540021e-01\n",
      "  -4.31149989e-01   1.90709996e+00  -2.33080000e-01  -4.83319983e-02\n",
      "  -3.97229999e-01   3.56680006e-01  -4.26340014e-01  -6.78979978e-02\n",
      "  -8.01699981e-02   1.52180004e+00  -1.46980003e-01  -4.68849987e-02\n",
      "   1.11139998e-01   9.98940021e-02  -4.95189995e-01   5.43200016e-01\n",
      "   2.87719995e-01  -1.00050000e-02   2.40329996e-01   4.12160009e-01\n",
      "  -7.42929988e-03   3.22730005e-01  -5.77989995e-01   2.64770001e-01\n",
      "  -6.72999993e-02   1.75569996e-01   3.30100000e-01  -1.80340007e-01\n",
      "  -1.06449999e-01  -2.43560001e-01  -1.36680007e-01   3.73789996e-01\n",
      "   3.96919996e-02   8.32870007e-02   6.79880008e-02  -1.29659995e-01\n",
      "  -1.21339999e-01  -6.97109997e-01  -5.26700020e-01  -3.50389987e-01\n",
      "  -1.93609998e-01  -3.43050003e-01   1.24969997e-01  -8.07010010e-02\n",
      "  -1.79580003e-01  -5.07439971e-01   4.66470003e-01  -2.33429998e-01\n",
      "  -3.30280006e-01   6.14399984e-02  -1.59410000e-01  -1.72940001e-01\n",
      "   3.90690006e-02  -1.29299998e-01  -1.72499996e-02  -3.46640013e-02\n",
      "  -2.68169999e-01  -2.32930005e-01  -3.86770010e-01  -1.80810004e-01\n",
      "  -4.30779994e-01   8.21209997e-02  -5.99359989e-01  -1.68300003e-01\n",
      "   2.52420008e-02  -2.29560003e-01  -1.82270005e-01   6.72190011e-01\n",
      "  -2.50330001e-01  -1.78939998e-01   5.78410029e-01   2.67999992e-03\n",
      "  -3.69150013e-01   4.08659995e-01  -2.28190005e-01  -1.43669993e-01\n",
      "   7.50850022e-01  -1.00549996e+00   1.44600004e-01  -3.30350012e-01\n",
      "  -8.61200020e-02   7.83969983e-02   3.10649991e-01  -2.77880013e-01\n",
      "  -5.87000012e-01   2.63410002e-01  -8.92490000e-02   1.93130001e-01\n",
      "   1.83889996e-02  -2.10479996e-03   7.17400014e-02   1.02830000e-01\n",
      "   7.74140000e-01   1.75940007e-01  -2.91500002e-01   2.43650004e-02\n",
      "  -3.86920005e-01  -1.45840001e+00  -3.51880014e-01   2.70179987e-01\n",
      "  -4.04410005e-01  -8.00699964e-02  -1.95209995e-01  -1.79769993e-01\n",
      "   1.40780002e-01  -1.36329994e-01   3.12110007e-01   3.39729995e-01\n",
      "   3.77570003e-01   4.47820008e-01  -4.20089990e-01  -2.79190004e-01\n",
      "  -6.29689991e-01  -1.35910004e-01  -1.44769996e-01   1.68539993e-02\n",
      "   1.63780004e-01  -3.92520018e-02  -6.46340027e-02  -3.11560005e-01\n",
      "   5.09109974e-01  -2.38560006e-01   4.53380011e-02   1.81710005e-01\n",
      "   4.26459983e-02  -5.62039971e-01  -2.04370007e-01   7.67629966e-02\n",
      "  -1.98789999e-01   3.23420018e-02   1.69220008e-02   5.79290017e-02\n",
      "  -2.24020004e+00   2.31790006e-01   1.26220003e-01   2.98220009e-01\n",
      "   2.50530005e-01   2.64360011e-01   1.52899995e-01  -6.11689985e-02\n",
      "  -2.25569993e-01  -5.46410024e-01   9.64049995e-02  -3.05170000e-01\n",
      "   6.23680018e-02   6.58089995e-01  -1.23130001e-01   4.01930004e-01\n",
      "  -1.12930000e-01   1.79000005e-01   1.79279998e-01   2.75810003e-01\n",
      "   1.05219996e+00  -1.21270001e-01   2.34070003e-01   6.80649996e-01\n",
      "  -1.87390000e-01   3.51500005e-01   2.26879999e-01  -1.60089999e-01\n",
      "  -1.01769999e-01   6.42239988e-01  -3.47860008e-02  -1.59079999e-01\n",
      "   3.43769997e-01  -4.62660015e-01  -1.42140001e-01   1.94159999e-01\n",
      "  -2.06200004e-01   1.68779999e-01  -1.39379993e-01  -6.91299975e-01\n",
      "   4.87080008e-01   2.16399997e-01   1.75980002e-01   1.28879994e-01\n",
      "   3.91720012e-02  -3.30769986e-01  -2.41150007e-01  -3.50580007e-01\n",
      "   6.35320008e-01  -1.41269997e-01  -2.87849993e-01  -3.11129987e-01\n",
      "  -1.71340004e-01   1.87350005e-01   4.44130003e-02   2.33689994e-01\n",
      "   3.86350006e-02   8.02220032e-02   1.04189999e-01  -1.34069994e-01\n",
      "  -2.19730005e-01   1.26980007e-01   2.52480000e-01   7.59329975e-01\n",
      "   2.60100007e-01  -1.74759999e-02   1.57529995e-01  -1.36490002e-01\n",
      "   1.29170001e-01  -7.70990014e-01  -1.01870000e-01  -5.63939989e-01\n",
      "   2.60569990e-01  -2.83199996e-01  -2.06340000e-01  -3.23610008e-01\n",
      "  -9.32860002e-02  -2.75819987e-01   5.69569990e-02   1.11270003e-01\n",
      "   2.97969997e-01  -4.00579989e-01  -7.30230007e-03   1.15390003e-01\n",
      "   3.83509994e-01  -4.44200002e-02   3.50430012e-02   8.14879984e-02\n",
      "  -6.46309972e-01   3.00300002e-01   3.95179987e-01   1.10919997e-01\n",
      "  -4.46660012e-01   6.40759990e-02  -4.12370004e-02   3.64859998e-02\n",
      "  -4.93809991e-02  -2.49599993e-01  -3.89759988e-01  -6.85760021e-01\n",
      "   4.36980009e-01  -1.18000001e-01  -2.62490004e-01   2.55120005e-02\n",
      "  -1.32320002e-01  -7.52350017e-02  -2.52640009e-01  -1.12549998e-01\n",
      "   1.22630000e-01  -1.30989999e-01   4.59270000e-01  -1.59449995e-01\n",
      "   2.39869997e-01  -2.62309998e-01   1.32449996e-02   7.20319986e-01\n",
      "   1.15120001e-01   8.27719986e-01   5.65840006e-02   6.64730012e-01\n",
      "  -1.87930003e-01   8.08100030e-02   1.06430002e-01  -3.25029999e-01\n",
      "   2.43249997e-01  -7.25399971e-01   7.36740008e-02   1.70080006e-01\n",
      "  -8.45609978e-02   8.01150024e-01  -8.25750008e-02   3.69500011e-01\n",
      "  -5.87880015e-02   1.62570000e-01  -5.26369989e-01   4.62849997e-02\n",
      "  -3.56620014e-01   2.66240001e-01   2.74140000e-01   4.99569997e-02\n",
      "   1.53870001e-01  -5.66240013e-01  -4.29520011e-02  -1.35639995e-01\n",
      "  -5.47540002e-02  -3.35370004e-01   3.98479998e-01   3.58010009e-02\n",
      "  -2.20369990e-03   2.62199998e-01  -3.74139994e-01   1.89109996e-01\n",
      "   1.16789997e-01  -2.69369990e-01  -1.59209996e-01   3.15829992e-01\n",
      "   1.99410006e-01   4.07059997e-01  -3.02029997e-01  -5.47739983e-01]\n",
      "Darkness\n",
      "[  4.20540005e-01  -2.82209992e-01   4.31910008e-01   1.01089999e-01\n",
      "  -1.34310007e-01  -5.97379982e-01  -1.70130000e-01   6.50390029e-01\n",
      "  -2.08489999e-01   1.86140001e+00   3.66820008e-01  -7.53109977e-02\n",
      "  -7.83680022e-01   1.46029994e-01  -2.66369998e-01   2.62980014e-01\n",
      "   1.83699995e-01   8.85029972e-01  -4.27450001e-01  -1.11110002e-01\n",
      "   9.22920033e-02  -4.62980002e-01  -3.16659987e-01   2.54099995e-01\n",
      "  -6.82219982e-01  -1.88590005e-01   3.64609987e-01   3.48379999e-01\n",
      "  -1.70169994e-01  -3.22990008e-02   1.21780001e-01   5.77449985e-02\n",
      "  -1.37450004e+00  -1.72790006e-01  -1.65420002e-03   2.79210005e-02\n",
      "  -1.30710006e-01   2.23000005e-01   5.76319993e-02  -5.13480008e-02\n",
      "   3.70409995e-01  -3.07350010e-01  -2.68900007e-01   2.16809995e-02\n",
      "   4.95400012e-01  -6.13120019e-01  -9.87149999e-02   2.61729985e-01\n",
      "   1.16050001e-02  -1.06470004e-01   5.78389987e-02   5.94540015e-02\n",
      "  -1.31899998e-01   2.09570006e-01  -2.22410006e-03   1.28099993e-01\n",
      "  -2.41380006e-01   2.08679996e-02   3.67319994e-02  -6.07990026e-01\n",
      "  -2.21549999e-02  -3.97760011e-02   1.92120001e-01  -6.91519976e-01\n",
      "  -7.71049976e-01   2.12679997e-01  -8.54900032e-02  -5.37490010e-01\n",
      "  -6.62480015e-03  -9.49869975e-02   1.13210000e-01  -1.74810007e-01\n",
      "   1.47009999e-01   2.90920008e-02  -1.66649997e-01   1.59639999e-01\n",
      "  -1.19790003e-01  -2.84299999e-01   2.30000004e-01   1.68219998e-01\n",
      "  -5.69069982e-01   3.20569992e-01  -4.47200000e-01  -8.61560032e-02\n",
      "   9.72790003e-01  -5.53170025e-01   3.66860002e-01  -6.76079988e-01\n",
      "   1.85690001e-01   3.84440005e-01   6.41670004e-02   5.04669994e-02\n",
      "  -4.11200002e-02  -1.35559998e-02   2.16830000e-01   5.36390007e-01\n",
      "   3.10069998e-03   2.45079994e-01  -8.62540007e-02   3.58829997e-03\n",
      "   6.82980001e-01  -2.01869994e-01  -3.80479991e-01  -1.08879998e-01\n",
      "   1.29869998e-01  -1.06470001e+00  -1.40310004e-01   5.42959988e-01\n",
      "  -6.46619976e-01  -2.63399988e-01   3.62060010e-01   6.67100027e-02\n",
      "   7.70640001e-02  -2.15000004e-01   8.29819962e-03   4.24659997e-01\n",
      "  -1.17030002e-01   4.06399995e-01   1.79930001e-01  -2.01619998e-01\n",
      "  -4.94419992e-01  -4.36980009e-01  -2.01740004e-02   2.62600005e-01\n",
      "  -2.92700008e-02  -7.52210021e-01   1.23829998e-01  -2.05310002e-01\n",
      "  -1.16169997e-01   4.78460006e-02  -6.04620017e-02  -1.71969995e-01\n",
      "   5.30579984e-01  -9.48269963e-02   2.10840002e-01  -7.51980022e-02\n",
      "   3.18529993e-01  -1.74050003e-01  -2.88899988e-01  -2.71710008e-01\n",
      "  -2.40840006e+00  -3.65189999e-01  -2.04510003e-01  -1.82290003e-01\n",
      "   1.91860003e-04   8.67529988e-01   1.71859995e-01   2.52849996e-01\n",
      "  -1.24310002e-01  -7.63310015e-01  -1.28330007e-01  -8.55180025e-01\n",
      "   5.93900025e-01   1.55819997e-01   1.85080007e-01   1.84749998e-02\n",
      "  -2.66240001e-01  -1.71570003e-01  -9.00809988e-02   2.31409997e-01\n",
      "   6.62849993e-02  -1.80409998e-01  -8.33069980e-02   6.64910018e-01\n",
      "   6.49510026e-01  -2.41610005e-01  -1.58990007e-02  -2.77799994e-01\n",
      "  -5.05890012e-01  -8.96269977e-02   2.29340002e-01  -7.30419978e-02\n",
      "   4.50800002e-01  -2.81599998e-01   3.23670000e-01   4.50830013e-01\n",
      "   2.62670010e-01  -7.52669990e-01   7.69150034e-02  -7.55270004e-01\n",
      "  -7.19619989e-02  -3.30839992e-01   1.75280005e-01   2.57829994e-01\n",
      "   2.29589999e-01  -5.39680012e-02  -5.66770017e-01  -3.90379995e-01\n",
      "   4.58799988e-01   8.90960000e-05  -2.30489999e-01   1.68750003e-01\n",
      "   1.40320003e-01  -2.17879996e-01  -2.38670006e-01   2.06709996e-01\n",
      "   1.22550003e-01   2.12239996e-02  -4.27399993e-01   3.07270009e-02\n",
      "  -1.90329999e-01   1.87050000e-01  -2.92039990e-01   1.94360003e-01\n",
      "  -2.71569997e-01  -3.86079997e-01   2.72359997e-01  -7.32339993e-02\n",
      "  -6.58629984e-02  -3.26719999e-01   3.91259998e-01  -7.22109973e-01\n",
      "  -1.93360001e-02  -1.91420004e-01   2.64070004e-01  -7.55079985e-01\n",
      "   2.70390004e-01  -2.78329998e-01   4.07759994e-01  -5.10890000e-02\n",
      "   5.09230018e-01  -3.11540008e-01  -1.58790007e-01   3.05689991e-01\n",
      "  -5.29500008e-01   1.70320004e-01   1.70230001e-01  -5.63769996e-01\n",
      "  -6.06589973e-01  -1.05970003e-01   1.99939996e-01   8.48700032e-02\n",
      "  -5.54109998e-02   2.36049995e-01   1.43209994e-01   1.85859993e-01\n",
      "  -2.57220000e-01  -3.74859989e-01  -4.28710014e-01  -4.42099988e-01\n",
      "   3.24780010e-02   4.66140002e-01  -3.79970014e-01  -1.94639996e-01\n",
      "  -2.50759989e-01  -2.60300003e-02  -4.74119991e-01   5.45400009e-02\n",
      "   5.16110003e-01  -1.57720000e-01  -1.27550006e-01  -4.64969993e-01\n",
      "   4.25309986e-01   1.19719997e-01  -2.81269997e-02   3.85109991e-01\n",
      "   1.62709996e-01  -1.09049998e-01  -2.34819993e-01   2.82990009e-01\n",
      "  -1.84380002e-02   1.06729999e-01  -1.00129999e-01  -5.26859984e-02\n",
      "   3.98799986e-01  -2.74850011e-01  -2.30230004e-01  -1.92159992e-02\n",
      "  -3.42339993e-01  -2.81610012e-01  -2.31480002e-02   3.97280008e-01\n",
      "  -4.16850001e-01  -1.68409999e-02   1.43030003e-01   1.82410002e-01\n",
      "  -2.30269998e-01   1.96429998e-01  -2.57239997e-01   5.38990013e-02\n",
      "   5.93490005e-01  -3.36070001e-01  -9.37800035e-02  -4.34700012e-01\n",
      "  -5.74689992e-02   2.50070002e-02   3.63299996e-01   1.43999994e-01\n",
      "   7.07579970e-01  -6.02249980e-01  -5.99749982e-01   3.46080005e-01\n",
      "   7.99719989e-02  -2.00749993e-01  -6.43819988e-01   5.64270020e-01\n",
      "   1.18079998e-01   8.20919991e-01  -3.75059992e-01  -7.85169974e-02]\n",
      "surface\n",
      "[  4.38439995e-01   2.89880008e-01  -2.10270002e-01   2.92699993e-01\n",
      "  -7.73999989e-02   1.41880006e-01  -3.21980000e-01   4.52380002e-01\n",
      "  -1.09959997e-01   9.75430012e-01   6.27889991e-01   1.81789994e-01\n",
      "   8.89720023e-02   1.26190007e-01  -2.49430001e-01   4.58829999e-01\n",
      "   3.23240012e-01   3.24699998e+00  -5.14440000e-01  -1.77159995e-01\n",
      "  -1.73860006e-02   6.77810013e-02  -1.35130003e-01   4.38140005e-01\n",
      "   4.30510014e-01   7.53320009e-02   8.10329989e-02   3.80400002e-01\n",
      "  -6.98450029e-01   5.84740005e-02  -3.55910003e-01   8.76879990e-02\n",
      "  -4.54699993e-01  -4.03569996e-01   1.64030001e-01  -1.83530003e-01\n",
      "  -2.08920002e-01   1.37529999e-01   6.77839994e-01  -5.60249984e-01\n",
      "   2.26569995e-01   1.42529994e-01   7.76740015e-02  -2.72219986e-01\n",
      "  -3.37029994e-01  -3.24779987e-01  -4.80740011e-01  -2.80860007e-01\n",
      "  -4.38459992e-01  -1.17760003e-01   3.87840003e-01  -2.29359999e-01\n",
      "  -3.30430001e-01   1.05010003e-01  -2.66229987e-01  -2.50559986e-01\n",
      "  -4.23900008e-01   1.28099993e-01   1.26049995e-01   5.19280016e-01\n",
      "   4.00339991e-01   6.74300015e-01  -2.59389997e-01   7.69500017e-01\n",
      "  -5.97469985e-01  -1.75819993e-01  -5.72790027e-01   2.34410003e-01\n",
      "   1.29060000e-02   2.61790007e-01  -1.38939999e-03   1.39469996e-01\n",
      "  -5.32729983e-01   7.26900017e-03  -4.66639996e-01   2.38370001e-01\n",
      "  -4.44519997e-01  -5.38649976e-01   3.08800012e-01  -6.36600018e-01\n",
      "   2.47130007e-01  -3.00939996e-02  -1.99379995e-01  -2.34490007e-01\n",
      "   2.72240013e-01  -3.79799992e-01   4.88460004e-01   8.39339972e-01\n",
      "  -4.43420000e-02   4.60260004e-01   1.91369995e-01  -1.89060003e-01\n",
      "  -5.51209986e-01   3.70209992e-01  -1.85990006e-01  -7.02999979e-02\n",
      "  -2.20890000e-01   3.42319995e-01  -3.97319999e-03  -1.60760000e-01\n",
      "   6.85890019e-01  -1.46770000e-01  -4.61690009e-01   8.65300000e-03\n",
      "  -2.70809997e-02  -2.03509998e+00  -3.46559994e-02  -4.83039990e-02\n",
      "  -1.13530003e-01  -3.10040009e-03  -1.44209996e-01  -4.33919996e-01\n",
      "   5.53910017e-01   1.50600001e-01   9.72189978e-02   2.79359996e-01\n",
      "   8.90500024e-02  -6.34090006e-01   1.51690006e-01  -1.57900006e-01\n",
      "  -2.27210000e-01   2.75370002e-01  -1.01379998e-01  -1.88050002e-01\n",
      "   4.47579995e-02   7.78959990e-02   7.03720003e-02  -1.01750001e-01\n",
      "  -8.56160000e-02  -8.65600035e-02  -3.07150006e-01  -5.42609990e-01\n",
      "  -4.96950001e-02   5.79680018e-02   1.43289998e-01   5.89139998e-01\n",
      "  -4.48069990e-01   8.22210014e-02   3.09340000e-01  -2.64699996e-01\n",
      "  -8.42639983e-01   2.84680009e-01  -1.33880004e-01   3.17660004e-01\n",
      "   2.46250004e-01   2.71939993e-01  -5.16399980e-01  -4.45600003e-02\n",
      "   3.97610009e-01   3.33519995e-01  -3.16060007e-01  -2.02179998e-01\n",
      "  -1.46290004e-01   1.39719993e-01  -3.22670013e-01  -1.65500008e-02\n",
      "  -1.25190005e-01   8.61989975e-01  -3.05920005e-01  -1.00440003e-01\n",
      "   5.65050006e-01  -5.37710011e-01   2.83459991e-01   9.45590019e-01\n",
      "   4.25220013e-01  -1.51450001e-02   1.16020001e-01  -9.10300016e-02\n",
      "   4.11280006e-01   5.69999993e-01   3.56299996e-01  -3.05170000e-01\n",
      "  -7.32119977e-01  -5.01470029e-01   1.42910004e-01   2.79460013e-01\n",
      "   1.05800003e-01  -3.34650010e-01  -1.97319999e-01  -4.41469997e-01\n",
      "  -1.60630003e-01   4.06060010e-01  -2.67190009e-01  -2.07969993e-01\n",
      "   2.23680004e-01  -8.06249976e-02   4.31719989e-01   5.16870022e-01\n",
      "  -2.21180007e-01   2.29920000e-02   2.78629988e-01  -2.85120010e-01\n",
      "  -9.73350033e-02   1.05800003e-01   1.56039998e-01   4.01789993e-01\n",
      "   5.79569995e-01   4.17600013e-02  -3.89039993e-01   1.12859998e-02\n",
      "  -5.85009996e-03   2.06860006e-01   3.72060001e-01   4.07409996e-01\n",
      "   2.95249999e-01   3.00299991e-02  -2.96050012e-01  -3.79570015e-02\n",
      "  -5.15030026e-01  -2.49200001e-01   3.03199999e-02   3.12159985e-01\n",
      "  -4.42820013e-01  -1.01170003e+00   5.31510003e-02   3.05819988e-01\n",
      "  -8.50550011e-02   1.02040000e-01   4.44729999e-02  -1.38909996e-01\n",
      "   3.41210008e-01   5.99419978e-03  -5.35650015e-01  -1.30070001e-01\n",
      "   7.82629997e-02   1.64399996e-01   9.63850021e-02  -5.38689971e-01\n",
      "  -1.97080001e-01   4.88330007e-01   8.93829986e-02   2.50429988e-01\n",
      "   5.02030015e-01  -1.80009995e-02   3.37139994e-01   4.57340002e-01\n",
      "   3.28049988e-01   2.81300008e-01   2.04449996e-01  -1.12510003e-01\n",
      "   9.16059971e-01   6.50550006e-03   1.09400004e-02   1.60260007e-01\n",
      "  -2.21330002e-01  -2.18060002e-01  -3.48170012e-01  -4.36300009e-01\n",
      "  -1.49230003e-01   3.76210004e-01   1.98019996e-01  -3.62190008e-01\n",
      "   6.82449996e-01  -6.90710008e-01  -2.76829988e-01   6.77510023e-01\n",
      "   4.60029989e-01   1.88659996e-01   2.84130007e-01   4.10540015e-01\n",
      "   5.57829976e-01   2.29340002e-01  -2.60960013e-01  -4.51020002e-01\n",
      "   2.89310008e-01   2.25970000e-01  -1.47679999e-01  -9.47539974e-03\n",
      "   3.39870006e-01  -1.07809998e-01   6.09109998e-01   1.49560004e-01\n",
      "   1.11199997e-01  -3.01820010e-01  -9.47970003e-02  -4.85169999e-02\n",
      "  -1.91860005e-01  -4.35319990e-01  -7.04760015e-01   2.32109994e-01\n",
      "   5.11969984e-01   2.74320006e-01  -1.84560001e-01  -7.82980025e-01\n",
      "  -3.14449996e-01  -4.79490012e-01   1.84450001e-01  -1.19080000e-01\n",
      "  -3.60339999e-01  -9.52970013e-02  -5.71289994e-02   8.09499994e-02\n",
      "  -3.77810001e-02  -9.23769996e-02   4.29419987e-02   2.93650001e-01\n",
      "   5.70789985e-02   2.72220001e-02  -6.44460022e-02   1.08709998e-01]\n",
      "deep\n",
      "[  6.49420023e-02   3.52959991e-01  -7.87660033e-02   3.76700014e-01\n",
      "   8.31219971e-01  -2.16250002e-01  -3.42729986e-01   3.62430006e-01\n",
      "   3.09029996e-01   1.88450003e+00   3.31609994e-01  -3.19919996e-02\n",
      "   1.35670006e-01  -2.32920006e-01  -4.00819987e-01   4.78309989e-01\n",
      "   2.36789994e-02   1.61740005e+00  -4.60500002e-01  -1.16449997e-01\n",
      "   6.53169975e-02   6.39140010e-02  -1.92279994e-01  -8.41680020e-02\n",
      "   8.03280026e-02  -1.64480001e-01   7.80120015e-01   1.10969996e+00\n",
      "  -1.79600000e-01   3.14900018e-02  -2.61449993e-01   1.97559997e-01\n",
      "  -3.10860008e-01  -5.40250003e-01   3.78580004e-01  -4.92680013e-01\n",
      "  -3.94800007e-01  -3.00980002e-01   1.21059999e-01  -2.63990015e-01\n",
      "   6.42030001e-01  -2.62439996e-01  -5.06360009e-02   1.24750003e-01\n",
      "   3.71349990e-01  -6.80029988e-02  -2.65329987e-01   4.41610008e-01\n",
      "  -4.49490011e-01   1.89909991e-02   2.82260001e-01   6.85959995e-01\n",
      "  -2.22680002e-01   1.55980006e-01   4.70429987e-01   1.75329998e-01\n",
      "  -4.06129986e-01  -1.49159998e-01   2.40720008e-02  -2.04799995e-01\n",
      "   4.62080017e-02   4.31120008e-01   8.39210004e-02  -4.12719995e-02\n",
      "  -1.59060000e-03   2.03099996e-01   3.45369995e-01  -2.82669999e-02\n",
      "  -2.15719998e-01   5.41869998e-01   1.23190001e-01  -7.88000003e-02\n",
      "   3.23139995e-01  -2.51399994e-01  -2.35049993e-01  -1.50659993e-01\n",
      "  -4.20509994e-01   6.85340017e-02  -6.11899972e-01  -2.16759995e-01\n",
      "   4.73419987e-02  -9.61500034e-02  -1.93629995e-01  -3.33660007e-01\n",
      "   1.59899995e-01  -6.66980028e-01   3.34320009e-01   4.40470010e-01\n",
      "   1.33340001e-01   1.08520001e-01   2.10170001e-01  -1.87989995e-01\n",
      "  -9.22079980e-02   2.62169987e-01   1.92780003e-01   4.37339991e-01\n",
      "  -1.73079997e-01   1.72279999e-01  -1.44040003e-01  -7.64620006e-01\n",
      "   3.90379995e-01  -7.89470002e-02  -1.49810001e-01  -1.18120000e-01\n",
      "   3.76989990e-01  -1.22309995e+00  -1.26780003e-01   2.33830005e-01\n",
      "   2.20390007e-01   3.91249992e-02   1.59319997e-01  -2.70839989e-01\n",
      "  -6.37849987e-01  -1.04280002e-02   4.17639986e-02   4.44319993e-01\n",
      "  -1.96180008e-02   2.34860003e-01   5.75810015e-01   1.37759997e-02\n",
      "   3.98130000e-01   2.51450002e-01  -4.94639985e-02   1.10789999e-01\n",
      "  -6.42199963e-02   1.89549997e-01   2.50699997e-01  -4.75190014e-01\n",
      "  -3.28689992e-01   5.40920012e-02   1.20949998e-01  -9.83050019e-02\n",
      "   2.64860000e-02   1.16570003e-01   2.69140005e-01   4.25300002e-01\n",
      "   2.27689996e-01  -5.64459980e-01   2.87499994e-01   2.65269995e-01\n",
      "  -2.20749998e+00   7.30459988e-02  -4.23500016e-02  -2.67619994e-02\n",
      "  -1.83190003e-01  -2.50609994e-01  -4.62489992e-01   3.31660002e-01\n",
      "   2.44910002e-01  -2.31099993e-01   4.64159995e-02  -9.38189998e-02\n",
      "   2.84040004e-01   4.08600003e-01  -2.48300001e-01   3.75900000e-01\n",
      "  -3.36690009e-01   3.93970013e-01  -1.71890005e-01   2.38839999e-01\n",
      "   1.75530002e-01   9.66089964e-02  -3.18910003e-01   8.72439981e-01\n",
      "   1.77129999e-01   1.84780002e-01   4.31070000e-01   1.49590001e-01\n",
      "   3.89700010e-02  -2.70209998e-01  -1.29449993e-01  -1.04319997e-01\n",
      "  -5.04509985e-01  -3.73340011e-01   3.35130006e-01   5.57470024e-01\n",
      "  -2.42050007e-01  -3.81719992e-02   3.12929988e-01  -5.66410005e-01\n",
      "  -1.74079999e-01   2.62199998e-01  -2.38639995e-01  -1.57289997e-01\n",
      "   3.46350014e-01   5.54849999e-03  -2.22959995e-01  -1.81549996e-01\n",
      "   1.77680001e-01   5.05159974e-01  -1.53180003e-01   3.41109991e-01\n",
      "   9.10789967e-02  -2.74890006e-01  -3.24479997e-01  -2.47280002e-01\n",
      "   4.10160005e-01   3.13309997e-01   2.89039989e-03   1.64000005e-01\n",
      "   8.81019980e-02  -1.99680001e-01  -5.58659971e-01   5.04249990e-01\n",
      "  -9.82500017e-02  -3.71479988e-01  -1.12450002e-02   2.51679987e-01\n",
      "  -3.47700000e-01  -3.85439992e-01  -4.73529994e-01   8.80490020e-02\n",
      "  -1.70550004e-01  -4.50749993e-01   3.51150006e-01   7.07010031e-02\n",
      "   1.66370004e-01  -3.63460004e-01   3.00639987e-01  -4.08270001e-01\n",
      "  -1.15699999e-01   3.51310015e-01  -1.47939995e-01   1.24399997e-02\n",
      "   2.18640000e-01  -2.98889995e-01   1.47119999e-01  -4.22910005e-01\n",
      "   6.37419969e-02   2.14279994e-01   3.19499999e-01  -5.24980016e-02\n",
      "   2.10380003e-01   2.54090011e-01  -3.18979993e-02  -6.05439991e-02\n",
      "  -2.42310002e-01  -3.36270005e-01   1.34279996e-01   6.60400018e-02\n",
      "   2.69490004e-01  -1.51030004e-01  -3.64030004e-01  -1.47129998e-01\n",
      "   5.20609975e-01   1.34100005e-01  -1.16789997e+00   8.81490007e-04\n",
      "  -2.23810002e-01   1.30140007e-01   2.62890011e-01  -2.95749992e-01\n",
      "  -3.97340000e-01   3.35869998e-01   2.28490005e-03   2.97439992e-01\n",
      "   6.93039969e-02  -1.84660003e-01  -6.68959990e-02   3.09769988e-01\n",
      "  -2.45220006e-01   1.53980002e-01   4.05710012e-01  -2.58850008e-01\n",
      "   2.58069992e-01  -1.52129993e-01  -4.53119993e-01  -8.94630030e-02\n",
      "  -4.78659987e-01   3.31559986e-01   2.76450008e-01   4.58330005e-01\n",
      "  -5.46320021e-01  -4.82490003e-01  -1.99689995e-02  -1.04630001e-01\n",
      "   1.74750000e-01   1.81070000e-01   1.30539998e-01  -1.23159997e-01\n",
      "   2.95359999e-01  -3.23290005e-02  -1.88610002e-01  -2.50220001e-01\n",
      "   1.86560005e-01  -4.47209999e-02  -4.10789996e-02  -1.70839995e-01\n",
      "   4.68230009e-01  -9.98380035e-02   5.07129990e-02  -2.31429994e-01\n",
      "  -1.03200004e-01  -2.92199999e-01   7.24079981e-02  -2.10810006e-01\n",
      "   1.16020001e-01   2.76540011e-01  -3.45369995e-01  -7.65550017e-01]\n",
      "Spirit\n",
      "[ 0.55983001  0.37257999  0.27553999 -0.19140001  0.17952999  0.029715\n",
      "  0.32429999  0.75461    -0.059113    2.59520006  0.25246    -0.24437\n",
      " -0.52078998 -0.020039    0.26916999  0.055852   -0.18956999  0.47817001\n",
      " -0.28946     0.28837001  0.27939001  0.076694    0.11322    -0.17465\n",
      "  0.20308     0.31667     0.11115     0.42781001 -0.52195001 -0.041209\n",
      " -0.17659999  0.096223   -0.28431001  0.25301     0.35100999 -0.14001\n",
      "  0.051212   -0.2263     -0.2929     -0.0070192  -0.33487999  0.18831\n",
      "  0.38089001 -0.094437    0.16845    -0.34380999  0.079268    0.033978\n",
      "  0.0382     -0.1762      0.015844   -0.24549    -0.031994   -0.11498\n",
      "  0.30502    -0.055876   -0.089996    0.57687002  0.15985    -0.087202\n",
      " -0.15442     0.10285     0.18465    -0.51787001 -0.49568999 -0.27000999\n",
      " -0.1198     -0.089181   -0.25453001 -0.099148   -0.28196001 -0.29655001\n",
      " -0.61027002 -0.07661    -0.17111     0.55597001 -0.30818     0.20265\n",
      " -0.2811      0.21514    -0.17581999  0.20683999 -0.22312    -0.10465\n",
      "  0.14432999 -0.66821003 -0.56834    -0.15624    -0.33474001  0.19453\n",
      " -0.32435     0.50461     0.19505     0.40188     0.10206    -0.015741\n",
      " -0.19783001 -0.42271    -0.53753    -0.27739999  0.29692999 -0.30447999\n",
      " -0.51222998 -0.37616    -0.34468001 -0.99900001 -0.18634     0.60211998\n",
      " -0.069521   -0.19825999 -0.32064    -0.005895   -0.12009    -0.87880999\n",
      " -0.25373     0.33914     0.57994002  0.15542001  0.13935     0.13175\n",
      "  0.12471    -0.52632999 -0.25718999  0.94657999  0.36102    -0.29260999\n",
      " -0.235      -0.045984    0.045171   -0.43118    -0.014077   -0.10943\n",
      " -0.82453001 -0.39757001  0.16024999 -0.080571   -0.18251     0.17619\n",
      " -0.026736   -0.17747    -1.91540003  0.085386    0.26151001 -0.062847\n",
      "  0.18425     0.0075087   0.29764     0.37898001 -0.1673     -0.25264999\n",
      " -0.20813     0.089678    0.40244001  0.21898     0.15612    -0.21419001\n",
      " -0.60699999 -0.2987     -0.24710999 -0.056841    0.41448     0.35115001\n",
      " -0.14835     0.16764     0.64806002 -0.33605     0.34952    -0.1971\n",
      " -0.58240998 -0.16945     0.26629999  0.63056999  0.48352    -0.10865\n",
      " -0.021704    0.30649999  0.18700001 -0.47608    -0.19284    -0.49520999\n",
      " -0.37386    -0.019852    0.83285999  0.18190999 -0.31169999  0.007269\n",
      " -0.56547999  0.12496     0.19293    -0.077969   -0.40665999  0.21315999\n",
      "  0.014332   -0.21076     0.44        0.41215    -0.25472999 -0.43726\n",
      "  0.19199     0.084961    0.09546     0.51001    -0.52293998  0.29969999\n",
      "  0.08676     0.12164    -0.14271     0.14045    -0.13855    -0.53192002\n",
      " -0.35538    -0.16621     0.24981     0.24468    -0.06249     0.19548\n",
      "  0.44185001 -0.16811    -0.43110999  0.41543001  0.1759      0.031706\n",
      "  0.076651    0.1666     -0.18682     0.039091    0.47624999  0.41602001\n",
      " -0.31413001 -0.24996001 -0.12647    -0.1751     -0.90943003  0.39445001\n",
      "  0.25271001  0.18054    -0.057657   -0.02123    -0.26535001 -0.24412\n",
      " -0.22175001  0.041019    0.13962001 -0.024871    0.024162   -0.024134\n",
      " -0.12142    -0.31687999  0.052855   -0.29014999  0.66254997  0.12771\n",
      " -0.35348001 -0.043419   -0.058784    0.51796001  0.21707    -0.58825999\n",
      "  0.36781001 -0.2502     -0.26956999 -0.28788999 -0.037281    0.15737\n",
      " -0.12763999  0.17848     0.011424    0.26835001 -0.061674    0.14923\n",
      " -0.016057    0.38574001 -0.40487     0.10215     0.46059999 -0.2455\n",
      " -0.27750999 -0.052887    0.076151    0.48328999 -0.61789    -0.4052\n",
      "  0.23135    -0.26921001 -0.057953   -0.25428     0.55893999  0.025742\n",
      "  0.52677     0.010956   -0.46981999 -0.024511    0.022628    0.13239001\n",
      " -0.40856001  0.25812     0.055096    0.33677    -0.035194   -0.099379  ]\n",
      "surface\n",
      "[  4.38439995e-01   2.89880008e-01  -2.10270002e-01   2.92699993e-01\n",
      "  -7.73999989e-02   1.41880006e-01  -3.21980000e-01   4.52380002e-01\n",
      "  -1.09959997e-01   9.75430012e-01   6.27889991e-01   1.81789994e-01\n",
      "   8.89720023e-02   1.26190007e-01  -2.49430001e-01   4.58829999e-01\n",
      "   3.23240012e-01   3.24699998e+00  -5.14440000e-01  -1.77159995e-01\n",
      "  -1.73860006e-02   6.77810013e-02  -1.35130003e-01   4.38140005e-01\n",
      "   4.30510014e-01   7.53320009e-02   8.10329989e-02   3.80400002e-01\n",
      "  -6.98450029e-01   5.84740005e-02  -3.55910003e-01   8.76879990e-02\n",
      "  -4.54699993e-01  -4.03569996e-01   1.64030001e-01  -1.83530003e-01\n",
      "  -2.08920002e-01   1.37529999e-01   6.77839994e-01  -5.60249984e-01\n",
      "   2.26569995e-01   1.42529994e-01   7.76740015e-02  -2.72219986e-01\n",
      "  -3.37029994e-01  -3.24779987e-01  -4.80740011e-01  -2.80860007e-01\n",
      "  -4.38459992e-01  -1.17760003e-01   3.87840003e-01  -2.29359999e-01\n",
      "  -3.30430001e-01   1.05010003e-01  -2.66229987e-01  -2.50559986e-01\n",
      "  -4.23900008e-01   1.28099993e-01   1.26049995e-01   5.19280016e-01\n",
      "   4.00339991e-01   6.74300015e-01  -2.59389997e-01   7.69500017e-01\n",
      "  -5.97469985e-01  -1.75819993e-01  -5.72790027e-01   2.34410003e-01\n",
      "   1.29060000e-02   2.61790007e-01  -1.38939999e-03   1.39469996e-01\n",
      "  -5.32729983e-01   7.26900017e-03  -4.66639996e-01   2.38370001e-01\n",
      "  -4.44519997e-01  -5.38649976e-01   3.08800012e-01  -6.36600018e-01\n",
      "   2.47130007e-01  -3.00939996e-02  -1.99379995e-01  -2.34490007e-01\n",
      "   2.72240013e-01  -3.79799992e-01   4.88460004e-01   8.39339972e-01\n",
      "  -4.43420000e-02   4.60260004e-01   1.91369995e-01  -1.89060003e-01\n",
      "  -5.51209986e-01   3.70209992e-01  -1.85990006e-01  -7.02999979e-02\n",
      "  -2.20890000e-01   3.42319995e-01  -3.97319999e-03  -1.60760000e-01\n",
      "   6.85890019e-01  -1.46770000e-01  -4.61690009e-01   8.65300000e-03\n",
      "  -2.70809997e-02  -2.03509998e+00  -3.46559994e-02  -4.83039990e-02\n",
      "  -1.13530003e-01  -3.10040009e-03  -1.44209996e-01  -4.33919996e-01\n",
      "   5.53910017e-01   1.50600001e-01   9.72189978e-02   2.79359996e-01\n",
      "   8.90500024e-02  -6.34090006e-01   1.51690006e-01  -1.57900006e-01\n",
      "  -2.27210000e-01   2.75370002e-01  -1.01379998e-01  -1.88050002e-01\n",
      "   4.47579995e-02   7.78959990e-02   7.03720003e-02  -1.01750001e-01\n",
      "  -8.56160000e-02  -8.65600035e-02  -3.07150006e-01  -5.42609990e-01\n",
      "  -4.96950001e-02   5.79680018e-02   1.43289998e-01   5.89139998e-01\n",
      "  -4.48069990e-01   8.22210014e-02   3.09340000e-01  -2.64699996e-01\n",
      "  -8.42639983e-01   2.84680009e-01  -1.33880004e-01   3.17660004e-01\n",
      "   2.46250004e-01   2.71939993e-01  -5.16399980e-01  -4.45600003e-02\n",
      "   3.97610009e-01   3.33519995e-01  -3.16060007e-01  -2.02179998e-01\n",
      "  -1.46290004e-01   1.39719993e-01  -3.22670013e-01  -1.65500008e-02\n",
      "  -1.25190005e-01   8.61989975e-01  -3.05920005e-01  -1.00440003e-01\n",
      "   5.65050006e-01  -5.37710011e-01   2.83459991e-01   9.45590019e-01\n",
      "   4.25220013e-01  -1.51450001e-02   1.16020001e-01  -9.10300016e-02\n",
      "   4.11280006e-01   5.69999993e-01   3.56299996e-01  -3.05170000e-01\n",
      "  -7.32119977e-01  -5.01470029e-01   1.42910004e-01   2.79460013e-01\n",
      "   1.05800003e-01  -3.34650010e-01  -1.97319999e-01  -4.41469997e-01\n",
      "  -1.60630003e-01   4.06060010e-01  -2.67190009e-01  -2.07969993e-01\n",
      "   2.23680004e-01  -8.06249976e-02   4.31719989e-01   5.16870022e-01\n",
      "  -2.21180007e-01   2.29920000e-02   2.78629988e-01  -2.85120010e-01\n",
      "  -9.73350033e-02   1.05800003e-01   1.56039998e-01   4.01789993e-01\n",
      "   5.79569995e-01   4.17600013e-02  -3.89039993e-01   1.12859998e-02\n",
      "  -5.85009996e-03   2.06860006e-01   3.72060001e-01   4.07409996e-01\n",
      "   2.95249999e-01   3.00299991e-02  -2.96050012e-01  -3.79570015e-02\n",
      "  -5.15030026e-01  -2.49200001e-01   3.03199999e-02   3.12159985e-01\n",
      "  -4.42820013e-01  -1.01170003e+00   5.31510003e-02   3.05819988e-01\n",
      "  -8.50550011e-02   1.02040000e-01   4.44729999e-02  -1.38909996e-01\n",
      "   3.41210008e-01   5.99419978e-03  -5.35650015e-01  -1.30070001e-01\n",
      "   7.82629997e-02   1.64399996e-01   9.63850021e-02  -5.38689971e-01\n",
      "  -1.97080001e-01   4.88330007e-01   8.93829986e-02   2.50429988e-01\n",
      "   5.02030015e-01  -1.80009995e-02   3.37139994e-01   4.57340002e-01\n",
      "   3.28049988e-01   2.81300008e-01   2.04449996e-01  -1.12510003e-01\n",
      "   9.16059971e-01   6.50550006e-03   1.09400004e-02   1.60260007e-01\n",
      "  -2.21330002e-01  -2.18060002e-01  -3.48170012e-01  -4.36300009e-01\n",
      "  -1.49230003e-01   3.76210004e-01   1.98019996e-01  -3.62190008e-01\n",
      "   6.82449996e-01  -6.90710008e-01  -2.76829988e-01   6.77510023e-01\n",
      "   4.60029989e-01   1.88659996e-01   2.84130007e-01   4.10540015e-01\n",
      "   5.57829976e-01   2.29340002e-01  -2.60960013e-01  -4.51020002e-01\n",
      "   2.89310008e-01   2.25970000e-01  -1.47679999e-01  -9.47539974e-03\n",
      "   3.39870006e-01  -1.07809998e-01   6.09109998e-01   1.49560004e-01\n",
      "   1.11199997e-01  -3.01820010e-01  -9.47970003e-02  -4.85169999e-02\n",
      "  -1.91860005e-01  -4.35319990e-01  -7.04760015e-01   2.32109994e-01\n",
      "   5.11969984e-01   2.74320006e-01  -1.84560001e-01  -7.82980025e-01\n",
      "  -3.14449996e-01  -4.79490012e-01   1.84450001e-01  -1.19080000e-01\n",
      "  -3.60339999e-01  -9.52970013e-02  -5.71289994e-02   8.09499994e-02\n",
      "  -3.77810001e-02  -9.23769996e-02   4.29419987e-02   2.93650001e-01\n",
      "   5.70789985e-02   2.72220001e-02  -6.44460022e-02   1.08709998e-01]\n",
      "waters\n",
      "[  4.50560004e-01  -3.16170007e-01   3.35519999e-01  -1.24619998e-01\n",
      "   3.19709986e-01  -1.63570002e-01  -2.54819989e-01   5.88379979e-01\n",
      "  -6.79359972e-01   2.48519993e+00  -6.39429986e-02  -3.29349995e-01\n",
      "   3.92749995e-01  -1.69510007e-01  -4.86530006e-01  -7.45469984e-03\n",
      "  -1.91459998e-01   1.10839999e+00   2.54269987e-01  -4.90350008e-01\n",
      "   1.42179996e-01   3.10350001e-01  -4.37009990e-01   2.42229998e-01\n",
      "  -4.34179991e-01  -5.75410008e-01   2.33610004e-01   3.71369988e-01\n",
      "  -3.10490012e-01   7.23360032e-02  -3.59640002e-01  -5.34969987e-03\n",
      "  -3.81159991e-01  -8.25169981e-01  -3.21579993e-01  -5.67120016e-01\n",
      "   3.44370008e-01  -2.64609993e-01  -3.60269994e-01   4.00389999e-01\n",
      "  -3.91299993e-01  -4.24050003e-01   3.79740000e-01  -1.43040001e-01\n",
      "   6.55070007e-01   2.24329993e-01  -3.41490000e-01   1.92129999e-01\n",
      "   1.08400002e-01  -5.12899995e-01   1.54019997e-01   6.98090017e-01\n",
      "  -3.11150014e-01   4.43379998e-01  -1.50009999e-02  -4.51779991e-01\n",
      "  -4.64419991e-01  -6.24860004e-02   2.09639996e-01   3.41780007e-01\n",
      "   3.22710007e-01  -9.90099981e-02   3.96919996e-01   3.47250015e-01\n",
      "  -9.96429980e-01   5.50629973e-01  -1.45180002e-01  -9.46490020e-02\n",
      "  -4.25029993e-01  -4.77140009e-01  -7.41079986e-01   2.28349999e-01\n",
      "   1.19530000e-01   2.26349995e-01   6.38479963e-02  -2.09279999e-01\n",
      "  -4.32099998e-01   8.20219994e-01  -2.99830008e-02  -8.00469995e-01\n",
      "   5.03509998e-01  -1.43869996e-01  -1.96050003e-01   3.37280005e-01\n",
      "   5.32729983e-01  -3.79260004e-01   1.09080002e-01   4.62249994e-01\n",
      "  -4.75659996e-01   6.23160005e-01  -8.21129978e-02  -5.42689979e-01\n",
      "   1.15340002e-01   2.51150012e-01   5.29569983e-01   7.56340027e-02\n",
      "  -8.37199986e-02  -8.65899995e-02   6.61339983e-02   4.56200019e-02\n",
      "   3.30879986e-01  -5.95309973e-01  -5.02269983e-01  -2.64299996e-02\n",
      "  -1.51089996e-01  -1.22389996e+00  -3.56429994e-01  -3.66799999e-03\n",
      "  -3.83329997e-03  -5.71420014e-01   3.41740012e-01  -2.39669994e-01\n",
      "  -3.30819994e-01   3.40169996e-01   2.86599994e-01  -1.14720002e-01\n",
      "   6.22910023e-01   1.27419993e-01  -1.62269995e-01   1.24210000e-01\n",
      "   3.83750014e-02   2.13290006e-01  -4.45309997e-01  -7.43189991e-01\n",
      "   2.04640001e-01  -1.99300006e-01  -2.76450008e-01   2.52960008e-02\n",
      "  -1.76420003e-01  -2.13579997e-01  -7.09030032e-02  -4.50129986e-01\n",
      "   1.61740005e-01  -2.81679988e-01   2.52130002e-01   1.14719999e+00\n",
      "  -2.05259994e-01  -2.76760012e-01   4.94859993e-01   1.81449994e-01\n",
      "  -1.02320004e+00   4.12680000e-01   2.71560013e-01  -4.38290015e-02\n",
      "   2.35560000e-01  -1.66720003e-01   1.93680003e-01  -1.34739997e-02\n",
      "  -3.36779989e-02  -1.20789997e-01   1.28940001e-01  -6.44320026e-02\n",
      "   4.02399987e-01   4.19050008e-01  -2.13170007e-01  -5.24169981e-01\n",
      "  -4.06480014e-01   1.11450005e+00  -3.69779989e-02   2.05239996e-01\n",
      "   6.80679977e-01  -3.16000015e-01  -1.80419996e-01   8.98769975e-01\n",
      "   3.70519996e-01   3.93049985e-01  -1.43169993e-04  -2.96290010e-01\n",
      "   2.61020005e-01   3.59780014e-01   1.06990002e-01   3.80030006e-01\n",
      "   1.93230007e-02  -4.72020000e-01   1.55499995e-01   2.90850013e-01\n",
      "   7.78660029e-02  -6.29480004e-01  -9.80819985e-02  -2.33119994e-01\n",
      "  -1.35319993e-01  -2.47270003e-01  -1.05700001e-01  -4.49400008e-01\n",
      "   6.61979973e-01  -1.97119996e-01   3.62060010e-01  -2.07979996e-02\n",
      "  -2.40349993e-01  -5.23920000e-01   6.05159998e-01  -5.83649985e-02\n",
      "  -1.98369995e-01  -1.62450001e-01   5.52849993e-02   1.17179997e-01\n",
      "   3.17640007e-01   8.46570015e-01   3.53749990e-01  -4.34620008e-02\n",
      "  -4.41029996e-01   9.00780022e-01   2.66330004e-01   4.26679999e-01\n",
      "   3.47420007e-01  -2.25060001e-01   2.83729993e-02   6.14419997e-01\n",
      "  -1.60549998e-01  -2.44980007e-01  -4.23689991e-01  -2.24160001e-01\n",
      "   2.23900005e-01   6.40939996e-02  -6.75399974e-02  -3.69509995e-01\n",
      "   3.04870009e-01  -6.47300035e-02   4.64280009e-01   2.27440000e-01\n",
      "  -4.54370007e-02   6.27780020e-01   3.00709993e-01  -2.41999999e-01\n",
      "  -8.99920017e-02  -4.86909986e-01   5.20789981e-01  -2.62439996e-01\n",
      "  -5.01980007e-01   1.21380001e-01   4.66540009e-01   1.04070000e-01\n",
      "   6.30420029e-01   1.70990005e-01   2.91759998e-01   9.68469977e-02\n",
      "   1.65800005e-01  -2.85580009e-01  -2.26270005e-01  -6.07270002e-01\n",
      "   4.12289985e-02  -4.88909990e-01   4.96740013e-01  -1.86629996e-01\n",
      "  -1.93430007e-01   6.71069995e-02  -2.63370007e-01  -2.63390005e-01\n",
      "  -2.23949999e-01  -5.60679995e-02   1.83840007e-01  -1.46210000e-01\n",
      "  -1.24289997e-01   1.42110005e-01   3.12050015e-01   7.71130025e-01\n",
      "   9.56050009e-02   3.39599997e-02  -5.98919988e-02   1.77100003e-02\n",
      "   8.72640014e-02   1.43989995e-01   1.94549993e-01  -1.91500001e-02\n",
      "   5.73199987e-01  -5.01299977e-01   1.33120000e-01   4.44620013e-01\n",
      "  -4.79699999e-01  -1.89740002e-01  -1.12530001e-01   1.63729995e-01\n",
      "  -8.78010020e-02   3.00089985e-01   6.85450017e-01  -4.53209996e-01\n",
      "  -4.05380018e-02  -1.08379997e-01  -4.80379984e-02  -7.65189976e-02\n",
      "   3.20140004e-01  -6.42970026e-01   1.40699995e-04  -2.92910010e-01\n",
      "   1.99689999e-01   4.34850007e-02   4.94489998e-01   7.09429989e-03\n",
      "   7.52229989e-01  -6.80510029e-02  -2.29640007e-01  -4.54189986e-01\n",
      "  -3.52809995e-01  -6.65859997e-01   1.22630000e-01   4.36699986e-01\n",
      "  -3.71399999e-01   3.09779998e-02  -1.42600000e-01  -9.68320012e-01]\n"
     ]
    }
   ],
   "source": [
    "for nc in nlp_eg.noun_chunks:\n",
    "    print(nc.root.text)\n",
    "    print(nc.root.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the vectors numpy arrays?\n",
    "noun_chunks = list(nlp_eg.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(noun_chunks[0].root.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_chunks[0].root.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list = [nc.root.vector for nc in noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_np = np.array(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vl = np.mean(vl_np, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we also here want to weight each vector based on a normalised occurence in the corpus?\n",
    "\n",
    "Where can we get a frequency weighting from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_vector(spacy_passage):\n",
    "    \"\"\"Generate a vector representation of the passage based on nouns.\"\"\"\n",
    "    np.concatenate([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_doc = \"\\n\".join([p for v, p in bible_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4009678"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bible_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 4009678 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-05ba3254cf4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy_bible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbible_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[0;32m--> 345\u001b[0;31m                                                 max_length=self.max_length))\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 4009678 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "spacy_bible = nlp(bible_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a big document doesn't work either. Best to stick to the smaller passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difflib SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    \"\"\"Get a similarity metric for strings a and b\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_matches(tweet, bible_data):\n",
    "    \"\"\"Match a tweet against the bible_data.\"\"\"\n",
    "    # Get matches\n",
    "    scores = [\n",
    "        (verse, passage, similar(tweet, passage)) \n",
    "        for verse, passage in bible_data\n",
    "    ]\n",
    "    # Sort by descending score\n",
    "    scores.sort(key=lambda tup: tup[2], reverse = True) \n",
    "    return scores\n",
    "\n",
    "def test_random_tweets(tweets, bible_data, n=5, k=5):\n",
    "    \"\"\"Print n examples for k tweets selected at random.\"\"\"\n",
    "    import random\n",
    "    num_tweets = len(tweets)\n",
    "    indices = random.sample(range(0, num_tweets), k)\n",
    "    for i in indices:\n",
    "        tweet = tweets[i]\n",
    "        print(\"-----------------\")\n",
    "        print(\"Tweet text: {}\".format(tweet))\n",
    "        scores = get_matches(tweet, bible_data)\n",
    "        for verse, passage, score in scores[0:n]:\n",
    "            print(\"\\n{0}, {1}, {2}\".format(verse, passage, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Tweet text: \"In addition, along with the advance of the electronic information society, a variety of electronic devices are utilized.\" #thetimeswelivein\n",
      "\n",
      "Mark 8:19, When I broke the five loaves among the five thousand, how many baskets full of broken pieces did you take up? They told him, Twelve., 0.4338235294117647\n",
      "\n",
      "Exodus 6:16, These are the names of the sons of Levi according to their generations: Gershon, and Kohath, and Merari; and the years of the life of Levi were one hundred thirty-seven years., 0.43174603174603177\n",
      "\n",
      "Numbers 3:21, Of Gershon was the family of the Libnites, and the family of the Shimeites: these are the families of the Gershonites., 0.4186046511627907\n",
      "\n",
      "Job 4:10, The roaring of the lion, and the voice of the fierce lion, the teeth of the young lions, are broken., 0.4166666666666667\n",
      "\n",
      "2 Corinthians 3:9, For if the service of condemnation has glory, the service of righteousness exceeds much more in glory., 0.4132231404958678\n",
      "-----------------\n",
      "Tweet text: RT @Dr_Cuspy: Why Watson and Siri Are Not Real AI http://t.co/s5MsxRK7Nd via @PopMech [Hofstadter pops up again; a renaissance?]\n",
      "\n",
      "Exodus 2:22, She bore a son, and he named him Gershom, for he said, I have lived as a foreigner in a foreign land., 0.35807860262008734\n",
      "\n",
      "2 Kings 3:22, They rose up early in the morning, and the sun shone on the water, and the Moabites saw the water over against them as red as blood., 0.35384615384615387\n",
      "\n",
      "Job 30:20, I cry to you, and you do not answer me. I stand up, and you gaze at me., 0.35175879396984927\n",
      "\n",
      "Job 38:26, To cause it to rain on a land where no man is; on the wilderness, in which there is no man;, 0.3470319634703196\n",
      "\n",
      "Genesis 34:31, They said, Should he deal with our sister as with a prostitute?, 0.34554973821989526\n",
      "-----------------\n",
      "Tweet text: EPO - The Administrative Council has been busy: updates concerning international supplementary searches, fees & search sharing coming up...\n",
      "\n",
      "Judges 1:21, The children of Benjamin did not drive out the Jebusites who inhabited Jerusalem; but the Jebusites dwell with the children of Benjamin in Jerusalem to this day., 0.3933333333333333\n",
      "\n",
      "2 Timothy 2:18, men who have erred concerning the truth, saying that the resurrection is already past, and overthrowing the faith of some., 0.39080459770114945\n",
      "\n",
      "Acts 15:9, He made no distinction between us and them, cleansing their hearts by faith., 0.39069767441860465\n",
      "\n",
      "Hebrews 11:38, (of whom the world was not worthy), wandering in deserts, mountains, caves, and the holes of the earth., 0.3884297520661157\n",
      "\n",
      "Joshua 13:28, This is the inheritance of the children of Gad according to their families, the cities and its villages., 0.3868312757201646\n",
      "-----------------\n",
      "Tweet text: Historians (and journalists) are always going to be important. https://t.co/o4s7DJEYwE\n",
      "\n",
      "Leviticus 9:16, He presented the burnt offering, and offered it according to the ordinance., 0.40993788819875776\n",
      "\n",
      "Job 36:33, Its noise tells about him, and the livestock also concerning the storm that comes up., 0.4093567251461988\n",
      "\n",
      "Proverbs 16:11, Honest balances and scales are Yahweh's; all the weights in the bag are his work., 0.40718562874251496\n",
      "\n",
      "John 20:10, So the disciples went away again to their own homes., 0.4057971014492754\n",
      "\n",
      "Acts 3:1, Peter and John were going up into the temple at the hour of prayer, the ninth hour., 0.40236686390532544\n",
      "-----------------\n",
      "Tweet text: Tip: use Chromium on Karmic EeePC in full screen (F11): excellent use of limited space.\n",
      "\n",
      "Isaiah 28:29, This also comes forth from Yahweh of Armies, who is wonderful in counsel, and excellent in wisdom., 0.44324324324324327\n",
      "\n",
      "Psalm 139:15, My frame wasn't hidden from you, when I was made in secret, woven together in the depths of the earth., 0.4126984126984127\n",
      "\n",
      "Hebrews 1:4, having become so much better than the angels, as he has inherited a more excellent name than they have., 0.4105263157894737\n",
      "\n",
      "Job 12:21, He pours contempt on princes, and loosens the belt of the strong., 0.40789473684210525\n",
      "\n",
      "Deuteronomy 25:10, His name shall be called in Israel, The house of him who has his shoe untied., 0.4024390243902439\n"
     ]
    }
   ],
   "source": [
    "test_random_tweets(tweets, bible_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy String Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'en_core_web_lg' file crashed my Jupyter kernel but the 'en_core_web_sm' file loaded okay. I'll try the medium-sized file 'en_core_web_md'. Yes - 'md' file loaded okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8MB)\n",
      "\u001b[K    100% |################################| 120.9MB 7.0MB/s ta 0:00:011 0% |                                | 1.2MB 2.2MB/s eta 0:00:56    2% |                                | 2.8MB 4.0MB/s eta 0:00:30    2% |                                | 3.3MB 3.9MB/s eta 0:00:31    3% |#                               | 4.1MB 5.6MB/s eta 0:00:21    3% |#                               | 4.4MB 3.7MB/s eta 0:00:32    5% |#                               | 6.8MB 7.2MB/s eta 0:00:16    8% |##                              | 9.8MB 6.0MB/s eta 0:00:19    8% |##                              | 10.8MB 6.1MB/s eta 0:00:19    9% |##                              | 11.2MB 6.3MB/s eta 0:00:18    9% |###                             | 11.5MB 5.1MB/s eta 0:00:22    12% |###                             | 14.7MB 4.9MB/s eta 0:00:22    12% |###                             | 15.0MB 4.9MB/s eta 0:00:22    12% |####                            | 15.3MB 6.0MB/s eta 0:00:18    13% |####                            | 16.0MB 5.2MB/s eta 0:00:21    13% |####                            | 16.4MB 5.6MB/s eta 0:00:19    15% |####                            | 18.1MB 7.9MB/s eta 0:00:13    16% |#####                           | 19.6MB 7.2MB/s eta 0:00:15    16% |#####                           | 20.0MB 6.4MB/s eta 0:00:16    17% |#####                           | 21.4MB 5.7MB/s eta 0:00:18    19% |######                          | 23.4MB 8.2MB/s eta 0:00:12    19% |######                          | 24.0MB 6.3MB/s eta 0:00:16    20% |######                          | 24.4MB 9.9MB/s eta 0:00:10    20% |######                          | 25.0MB 5.3MB/s eta 0:00:19    21% |######                          | 25.7MB 3.7MB/s eta 0:00:26    22% |#######                         | 26.7MB 4.6MB/s eta 0:00:21    25% |########                        | 30.3MB 5.2MB/s eta 0:00:18    26% |########                        | 31.6MB 6.3MB/s eta 0:00:15    27% |########                        | 33.6MB 4.5MB/s eta 0:00:20    28% |#########                       | 35.0MB 8.4MB/s eta 0:00:11    29% |#########                       | 36.0MB 8.7MB/s eta 0:00:10    30% |#########                       | 37.4MB 7.6MB/s eta 0:00:12    33% |##########                      | 40.2MB 7.2MB/s eta 0:00:12    33% |##########                      | 40.7MB 8.6MB/s eta 0:00:10    35% |###########                     | 43.1MB 4.7MB/s eta 0:00:17    39% |############                    | 47.7MB 5.6MB/s eta 0:00:14    46% |##############                  | 55.8MB 9.0MB/s eta 0:00:08    46% |##############                  | 56.2MB 7.3MB/s eta 0:00:09    47% |###############                 | 57.5MB 4.9MB/s eta 0:00:13    50% |################                | 61.4MB 4.5MB/s eta 0:00:14    52% |################                | 63.4MB 7.0MB/s eta 0:00:09    52% |################                | 63.7MB 7.7MB/s eta 0:00:08    53% |#################               | 64.3MB 8.3MB/s eta 0:00:07    54% |#################               | 65.3MB 6.3MB/s eta 0:00:09    54% |#################               | 65.6MB 5.4MB/s eta 0:00:11    55% |#################               | 67.3MB 5.8MB/s eta 0:00:10    56% |##################              | 68.4MB 6.2MB/s eta 0:00:09    56% |##################              | 68.7MB 6.0MB/s eta 0:00:09    57% |##################              | 69.0MB 5.2MB/s eta 0:00:10    57% |##################              | 69.5MB 8.3MB/s eta 0:00:07    57% |##################              | 69.9MB 7.6MB/s eta 0:00:07    59% |###################             | 72.0MB 7.6MB/s eta 0:00:07    59% |###################             | 72.3MB 4.7MB/s eta 0:00:11    61% |###################             | 74.3MB 4.9MB/s eta 0:00:10    62% |###################             | 75.0MB 5.7MB/s eta 0:00:09    62% |###################             | 75.3MB 5.6MB/s eta 0:00:09    63% |####################            | 76.3MB 6.1MB/s eta 0:00:08    64% |####################            | 77.4MB 5.6MB/s eta 0:00:08    67% |#####################           | 82.2MB 4.0MB/s eta 0:00:10    70% |######################          | 84.7MB 5.0MB/s eta 0:00:08    71% |######################          | 86.8MB 5.9MB/s eta 0:00:06    72% |#######################         | 87.4MB 5.2MB/s eta 0:00:07    76% |########################        | 91.9MB 5.2MB/s eta 0:00:06    76% |########################        | 92.2MB 5.6MB/s eta 0:00:06    76% |########################        | 92.9MB 8.0MB/s eta 0:00:04    78% |#########################       | 94.9MB 6.9MB/s eta 0:00:04    78% |#########################       | 95.3MB 5.2MB/s eta 0:00:05    79% |#########################       | 95.7MB 6.9MB/s eta 0:00:04    82% |##########################      | 99.6MB 5.6MB/s eta 0:00:04    83% |##########################      | 100.6MB 5.7MB/s eta 0:00:04    83% |##########################      | 101.0MB 6.1MB/s eta 0:00:04    84% |##########################      | 101.7MB 6.4MB/s eta 0:00:03    85% |###########################     | 103.1MB 4.9MB/s eta 0:00:04    85% |###########################     | 103.5MB 7.5MB/s eta 0:00:03    85% |###########################     | 103.8MB 7.0MB/s eta 0:00:03    86% |###########################     | 104.1MB 7.3MB/s eta 0:00:03    87% |############################    | 105.9MB 10.5MB/s eta 0:00:02    88% |############################    | 106.6MB 5.4MB/s eta 0:00:03    89% |############################    | 108.3MB 6.4MB/s eta 0:00:02    93% |#############################   | 112.6MB 8.4MB/s eta 0:00:01    93% |#############################   | 113.0MB 6.2MB/s eta 0:00:02    93% |##############################  | 113.4MB 4.9MB/s eta 0:00:02    95% |##############################  | 115.0MB 5.2MB/s eta 0:00:02    96% |##############################  | 116.0MB 5.4MB/s eta 0:00:01    97% |############################### | 118.2MB 8.0MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "  Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-md-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_md\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_md')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    \"\"\"Get a similarity metric for strings a and b\"\"\"\n",
    "    spacy_a = nlp(a)\n",
    "    spacy_b = nlp(b)\n",
    "    return spacy_a.similarity(spacy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Tweet text: Next-Gen Bluetooth Bulb Controllable Via Smartphone | Freshome http://t.co/5oEjvL6c [A great patented idea - get it to market!]\n",
      "\n",
      "Nehemiah 3:32, Between the ascent of the corner and the sheep gate repaired the goldsmiths and the merchants., 0.38009049773755654\n",
      "\n",
      "Jeremiah 27:5, I have made the earth, the men and the animals that are on the surface of the earth, by my great power and by my outstretched arm; and I give it to whom it seems right to me., 0.3588039867109635\n",
      "\n",
      "1 Corinthians 10:7, Neither be idolaters, as some of them were. As it is written, The people sat down to eat and drink, and rose up to play., 0.3562753036437247\n",
      "\n",
      "Acts 2:20, The sun will be turned into darkness, and the moon into blood, before the great and glorious day of the Lord comes., 0.35537190082644626\n",
      "\n",
      "Matthew 18:4, Whoever therefore humbles himself as this little child, the same is the greatest in the Kingdom of Heaven., 0.351931330472103\n",
      "-----------------\n",
      "Tweet text: Ask Yourself: Are You Happier Now Than You Were 10 Years Ago? https://t.co/1MeEzAkHcT [+ report on happiness &amp; parenting: -ve ~ w/ GDP]\n",
      "\n",
      "Hosea 8:8, Israel is swallowed up. Now they are among the nations like a worthless thing., 0.35023041474654376\n",
      "\n",
      "Deuteronomy 29:4, but Yahweh has not given you a heart to know, and eyes to see, and ears to hear, to this day., 0.33620689655172414\n",
      "\n",
      "Amos 6:12, Do horses run on the rocky crags? Does one plow there with oxen? But you have turned justice into poison, and the fruit of righteousness into bitterness;, 0.3356164383561644\n",
      "\n",
      "Ephesians 4:28, Let him who stole steal no more; but rather let him labor, working with his hands the thing that is good, that he may have something to give to him who has need., 0.3333333333333333\n",
      "\n",
      "Psalm 112:1, Praise Yah! Blessed is the man who fears Yahweh, who delights greatly in his commandments., 0.3318777292576419\n",
      "-----------------\n",
      "Tweet text: The more possessions a person has, &amp; the more orderly the society, the greater the frequency of corporal punishment for children.\n",
      "\n",
      "1 Chronicles 26:19, These were the divisions of the doorkeepers; of the sons of the Korahites, and of the sons of Merari., 0.4700854700854701\n",
      "\n",
      "1 Samuel 17:31, When the words were heard which David spoke, they rehearsed them before Saul; and he sent for him., 0.4588744588744589\n",
      "\n",
      "2 Kings 20:4, It happened, before Isaiah had gone out into the middle part of the city, that the word of Yahweh came to him, saying,, 0.4541832669322709\n",
      "\n",
      "Exodus 39:38, the golden altar, the anointing oil, the sweet incense, the screen for the door of the Tent,, 0.4533333333333333\n",
      "\n",
      "Ezekiel 47:15, This shall be the border of the land: On the north side, from the great sea, by the way of Hethlon, to the entrance of Zedad;, 0.4496124031007752\n",
      "-----------------\n",
      "Tweet text: Rather than use subordinate clauses, old languages (~ C10 BC) juxtapose events according to temporal order.\n",
      "\n",
      "Genesis 30:34, Laban said, Behold, let it be according to your word., 0.475\n",
      "\n",
      "Numbers 15:12, According to the number that you shall prepare, so you shall do to everyone according to their number., 0.45933014354066987\n",
      "\n",
      "Numbers 26:53, To these the land shall be divided for an inheritance according to the number of names., 0.4329896907216495\n",
      "\n",
      "2 Kings 4:44, So he set it before them, and they ate, and left some of it, according to the word of Yahweh., 0.43\n",
      "\n",
      "2 Kings 7:16, The people went out, and plundered the camp of the Syrians. So a measure of fine flour was [sold] for a shekel, and two measures of barley for a shekel, according to the word of Yahweh., 0.4246575342465753\n",
      "-----------------\n",
      "Tweet text: The Economist | Amazon: http://t.co/wBm32F26yw [the world's 9th biggest retailer didn't exist 20 years ago] http://t.co/JY6Uq3iSIB\n",
      "\n",
      "2 Thessalonians 2:6, Now you know what is restraining him, to the end that he may be revealed in his own season., 0.39819004524886875\n",
      "\n",
      "Mark 16:13, They went away and told it to the rest. They didn't believe them, either., 0.39408866995073893\n",
      "\n",
      "John 14:24, He who doesn't love me doesn't keep my words. The word which you hear isn't mine, but the Father's who sent me., 0.37344398340248963\n",
      "\n",
      "1 Timothy 6:7, For we brought nothing into the world, and we certainly can't carry anything out., 0.3696682464454976\n",
      "\n",
      "Proverbs 13:1, A wise son listens to his father's instruction, but a scoffer doesn't listen to rebuke., 0.3686635944700461\n"
     ]
    }
   ],
   "source": [
    "test_random_tweets(tweets, bible_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Gensim needs a little bit of pre-processing to convert our texts into vector form. We need to get a bag of words that represents each portion of text.\n",
    "\n",
    "First we need to tokenise our text. We can use spaCy or NLTK to do this. (The method above involves generating a spaCy doc for each Bible passage - we can maybe do this once and then use elsewhere.)\n",
    "\n",
    "Then we filter the text and convert it into a vector form.\n",
    "\n",
    "The procedure below mirrors the [Gensim tutorial](https://radimrehurek.com/gensim/tut1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This took quite a long time so I might go for the quicker word_tokenize from nltk\n",
    "# spacy_bible = [(verse, nlp(passage)) for verse, passage in bible_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenised = [(verse, word_tokenize(passage)) for verse, passage in bible_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Genesis 1:4',\n",
       " ['God',\n",
       "  'saw',\n",
       "  'the',\n",
       "  'light',\n",
       "  ',',\n",
       "  'and',\n",
       "  'saw',\n",
       "  'that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'good',\n",
       "  '.',\n",
       "  'God',\n",
       "  'divided',\n",
       "  'the',\n",
       "  'light',\n",
       "  'from',\n",
       "  'the',\n",
       "  'darkness',\n",
       "  '.'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(tokens):\n",
    "    \"\"\" Remove digits and punctuation from text and convert to lower case. \"\"\"\n",
    "    # Alternative for complete text is re.sub('\\W+', '', text)\n",
    "    return [w.lower() for w in tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = [(verse, process_words(tokens)) for verse, tokens in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Genesis 1:4',\n",
       " ['god',\n",
       "  'saw',\n",
       "  'the',\n",
       "  'light',\n",
       "  'and',\n",
       "  'saw',\n",
       "  'that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'good',\n",
       "  'god',\n",
       "  'divided',\n",
       "  'the',\n",
       "  'light',\n",
       "  'from',\n",
       "  'the',\n",
       "  'darkness'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [tokens for _, tokens in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK modules\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Load stopwords\n",
    "ENG_STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def text_preprocessing(original_text):\n",
    "    \"\"\"Clean and process texts for Gensim methods.\"\"\" \n",
    "    # Tokenise\n",
    "    tokenised = word_tokenize(original_text) \n",
    "    \n",
    "    # Convert to lowercase and remove non-text / stopwords\n",
    "    tokenised = [w.lower() for w in tokenised if (w.isalpha() and w not in ENG_STOPWORDS)]\n",
    "    return tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['god', 'saw', 'light', 'saw', 'good', 'god', 'divided', 'light', 'darkness']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(bible_data[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text_preprocessing(passage) for _, passage in bible_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['god',\n",
       " 'said',\n",
       " 'let',\n",
       " 'expanse',\n",
       " 'middle',\n",
       " 'waters',\n",
       " 'let',\n",
       " 'divide',\n",
       " 'waters',\n",
       " 'waters']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-21 12:50:20,527 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-21 12:50:20,799 : INFO : adding document #10000 to Dictionary(6375 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 12:50:21,016 : INFO : adding document #20000 to Dictionary(9840 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 12:50:21,242 : INFO : adding document #30000 to Dictionary(12041 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 12:50:21,272 : INFO : built Dictionary(12255 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...) from 31102 documents (total 370556 corpus positions)\n",
      "2018-06-21 12:50:21,276 : INFO : saving Dictionary object under bible.dict, separately None\n",
      "2018-06-21 12:50:21,288 : INFO : saved bible.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12255 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from our processed bible texts\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary that maps numbers to words\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# Save dictionary\n",
    "dictionary.save('bible.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-21 12:50:26,077 : INFO : storing corpus in Matrix Market format to bible.mm\n",
      "2018-06-21 12:50:26,082 : INFO : saving sparse matrix to bible.mm\n",
      "2018-06-21 12:50:26,085 : INFO : PROGRESS: saving document #0\n",
      "2018-06-21 12:50:26,122 : INFO : PROGRESS: saving document #1000\n",
      "2018-06-21 12:50:26,162 : INFO : PROGRESS: saving document #2000\n",
      "2018-06-21 12:50:26,199 : INFO : PROGRESS: saving document #3000\n",
      "2018-06-21 12:50:26,237 : INFO : PROGRESS: saving document #4000\n",
      "2018-06-21 12:50:26,274 : INFO : PROGRESS: saving document #5000\n",
      "2018-06-21 12:50:26,313 : INFO : PROGRESS: saving document #6000\n",
      "2018-06-21 12:50:26,350 : INFO : PROGRESS: saving document #7000\n",
      "2018-06-21 12:50:26,391 : INFO : PROGRESS: saving document #8000\n",
      "2018-06-21 12:50:26,429 : INFO : PROGRESS: saving document #9000\n",
      "2018-06-21 12:50:26,470 : INFO : PROGRESS: saving document #10000\n",
      "2018-06-21 12:50:26,504 : INFO : PROGRESS: saving document #11000\n",
      "2018-06-21 12:50:26,549 : INFO : PROGRESS: saving document #12000\n",
      "2018-06-21 12:50:26,596 : INFO : PROGRESS: saving document #13000\n",
      "2018-06-21 12:50:26,629 : INFO : PROGRESS: saving document #14000\n",
      "2018-06-21 12:50:26,660 : INFO : PROGRESS: saving document #15000\n",
      "2018-06-21 12:50:26,695 : INFO : PROGRESS: saving document #16000\n",
      "2018-06-21 12:50:26,727 : INFO : PROGRESS: saving document #17000\n",
      "2018-06-21 12:50:26,769 : INFO : PROGRESS: saving document #18000\n",
      "2018-06-21 12:50:26,807 : INFO : PROGRESS: saving document #19000\n",
      "2018-06-21 12:50:26,858 : INFO : PROGRESS: saving document #20000\n",
      "2018-06-21 12:50:26,901 : INFO : PROGRESS: saving document #21000\n",
      "2018-06-21 12:50:26,940 : INFO : PROGRESS: saving document #22000\n",
      "2018-06-21 12:50:26,978 : INFO : PROGRESS: saving document #23000\n",
      "2018-06-21 12:50:27,011 : INFO : PROGRESS: saving document #24000\n",
      "2018-06-21 12:50:27,043 : INFO : PROGRESS: saving document #25000\n",
      "2018-06-21 12:50:27,075 : INFO : PROGRESS: saving document #26000\n",
      "2018-06-21 12:50:27,107 : INFO : PROGRESS: saving document #27000\n",
      "2018-06-21 12:50:27,141 : INFO : PROGRESS: saving document #28000\n",
      "2018-06-21 12:50:27,179 : INFO : PROGRESS: saving document #29000\n",
      "2018-06-21 12:50:27,213 : INFO : PROGRESS: saving document #30000\n",
      "2018-06-21 12:50:27,252 : INFO : PROGRESS: saving document #31000\n",
      "2018-06-21 12:50:27,259 : INFO : saved 31102x12255 matrix, density=0.089% (339121/381155010)\n",
      "2018-06-21 12:50:27,262 : INFO : saving MmCorpus index to bible.mm.index\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# Save corpus for later\n",
    "corpora.MmCorpus.serialize('bible.mm', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a first run of this we note that most topics are defined by common stopwords. Let's get rid of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-21 12:51:15,457 : INFO : using serial LSI version on this node\n",
      "2018-06-21 12:51:15,458 : INFO : updating model with new documents\n",
      "2018-06-21 12:51:15,460 : INFO : preparing a new chunk of documents\n",
      "2018-06-21 12:51:15,620 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-21 12:51:15,626 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-21 12:51:15,866 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-21 12:51:16,872 : INFO : 2nd phase: running dense svd on (200, 20000) matrix\n",
      "2018-06-21 12:51:18,142 : INFO : computing the final decomposition\n",
      "2018-06-21 12:51:18,146 : INFO : keeping 100 factors (discarding 18.228% of energy spectrum)\n",
      "2018-06-21 12:51:18,326 : INFO : processed documents up to #20000\n",
      "2018-06-21 12:51:18,329 : INFO : topic #0(128.237): 0.579*\"shall\" + 0.450*\"yahweh\" + 0.419*\"i\" + 0.176*\"said\" + 0.173*\"god\" + 0.131*\"israel\" + 0.108*\"king\" + 0.106*\"the\" + 0.087*\"he\" + 0.085*\"house\"\n",
      "2018-06-21 12:51:18,333 : INFO : topic #1(94.911): -0.741*\"shall\" + 0.569*\"i\" + 0.178*\"yahweh\" + 0.174*\"said\" + 0.089*\"king\" + 0.081*\"god\" + 0.069*\"israel\" + -0.056*\"you\" + 0.049*\"son\" + -0.045*\"offering\"\n",
      "2018-06-21 12:51:18,336 : INFO : topic #2(83.248): 0.656*\"i\" + -0.593*\"yahweh\" + 0.255*\"shall\" + -0.159*\"israel\" + -0.158*\"god\" + -0.138*\"king\" + -0.103*\"the\" + -0.081*\"house\" + -0.081*\"children\" + -0.076*\"son\"\n",
      "2018-06-21 12:51:18,338 : INFO : topic #3(70.173): -0.513*\"yahweh\" + 0.490*\"king\" + 0.355*\"son\" + 0.249*\"the\" + 0.243*\"said\" + 0.167*\"israel\" + 0.139*\"he\" + -0.124*\"i\" + 0.117*\"children\" + 0.107*\"men\"\n",
      "2018-06-21 12:51:18,342 : INFO : topic #4(57.722): 0.504*\"said\" + -0.451*\"son\" + -0.379*\"children\" + -0.349*\"israel\" + 0.339*\"he\" + -0.131*\"i\" + 0.129*\"let\" + 0.118*\"us\" + 0.109*\"go\" + 0.108*\"god\"\n",
      "2018-06-21 12:51:18,345 : INFO : preparing a new chunk of documents\n",
      "2018-06-21 12:51:18,454 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-21 12:51:18,460 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-21 12:51:18,589 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-21 12:51:19,492 : INFO : 2nd phase: running dense svd on (200, 11102) matrix\n",
      "2018-06-21 12:51:20,134 : INFO : computing the final decomposition\n",
      "2018-06-21 12:51:20,138 : INFO : keeping 100 factors (discarding 19.942% of energy spectrum)\n",
      "2018-06-21 12:51:20,313 : INFO : merging projections: (12255, 100) + (12255, 100)\n",
      "2018-06-21 12:51:20,720 : INFO : keeping 100 factors (discarding 5.936% of energy spectrum)\n",
      "2018-06-21 12:51:20,956 : INFO : processed documents up to #31102\n",
      "2018-06-21 12:51:20,961 : INFO : topic #0(152.944): 0.600*\"i\" + 0.513*\"shall\" + 0.354*\"yahweh\" + 0.169*\"said\" + 0.157*\"god\" + 0.104*\"israel\" + 0.092*\"the\" + 0.082*\"king\" + 0.082*\"he\" + 0.075*\"land\"\n",
      "2018-06-21 12:51:20,965 : INFO : topic #1(116.465): -0.735*\"shall\" + 0.651*\"i\" + 0.093*\"said\" + -0.062*\"yahweh\" + -0.050*\"you\" + -0.049*\"offering\" + -0.038*\"the\" + -0.025*\"priest\" + 0.023*\"know\" + 0.021*\"for\"\n",
      "2018-06-21 12:51:20,966 : INFO : topic #2(97.426): 0.592*\"yahweh\" + -0.415*\"i\" + -0.394*\"shall\" + 0.258*\"god\" + 0.177*\"said\" + 0.167*\"israel\" + 0.167*\"king\" + 0.144*\"the\" + 0.118*\"son\" + 0.107*\"house\"\n",
      "2018-06-21 12:51:20,968 : INFO : topic #3(79.928): -0.604*\"yahweh\" + 0.379*\"said\" + 0.303*\"king\" + 0.284*\"son\" + 0.245*\"the\" + 0.245*\"he\" + 0.145*\"man\" + 0.144*\"one\" + 0.096*\"came\" + -0.095*\"i\"\n",
      "2018-06-21 12:51:20,974 : INFO : topic #4(69.851): 0.857*\"god\" + -0.237*\"king\" + -0.213*\"yahweh\" + -0.165*\"son\" + -0.162*\"the\" + -0.105*\"israel\" + 0.103*\"us\" + -0.093*\"children\" + 0.077*\"for\" + 0.064*\"jesus\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import models, similarities\n",
    "# We'll start with LSI and a 100D vector\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-21 12:53:38,112 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-06-21 12:53:39,874 : INFO : creating matrix with 31102 documents and 100 features\n",
      "2018-06-21 12:53:43,628 : INFO : saving MatrixSimilarity object under bible.index, separately None\n",
      "2018-06-21 12:53:43,786 : INFO : saved bible.index\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "index.save('bible.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text, dictionary, lsi):\n",
    "    \"\"\"Convert a portion of text to an LSI vector.\"\"\"\n",
    "    processed = text_preprocessing(text)\n",
    "    vec_bow = dictionary.doc2bow(processed)\n",
    "    vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "    return vec_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_lsi = text2vec(tweets[5], dictionary, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.026685458), (1, 0.010693545), (2, 0.02526992), (3, 0.018537477), (4, -0.0064496454)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))[0:5]) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14471, 0.98052335), (16335, 0.78113711), (5388, 0.75273919), (17916, 0.74159586), (26053, 0.73570257)]\n"
     ]
    }
   ],
   "source": [
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims_sorted[0:5]) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Psalm 37:21',\n",
       " \"The wicked borrow, and don't pay back, but the righteous give generously.\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_data[sims_sorted[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“The ungovernable metropolis, with its fluid population and ethnic and occupational enclaves, is an affront to a mindset that envisions a world of harmony, purity, and organic wholeness.” - to thrive you need to give up unattainable perfection and unquestioning agreement'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fold all this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zipped = [(p, v, s) for (p, v), s in zip(bible_data, sims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Psalm 37:21',\n",
       " \"The wicked borrow, and don't pay back, but the righteous give generously.\",\n",
       " 0.98052335)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped[14471]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-23 08:15:34,349 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "# Import gensim modules\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Import NLTK modules\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Load stopwords\n",
    "ENG_STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def text_preprocessing(original_text):\n",
    "    \"\"\"Clean and process texts for Gensim methods.\"\"\" \n",
    "    # Tokenise\n",
    "    tokenised = word_tokenize(original_text) \n",
    "    \n",
    "    # Convert to lowercase and remove non-text / stopwords\n",
    "    tokenised = [w.lower() for w in tokenised if (w.isalpha() and w not in ENG_STOPWORDS)]\n",
    "    return tokenised\n",
    "\n",
    "def text2vec(text, dictionary, lsi):\n",
    "    \"\"\"Convert a portion of text to an LSI vector.\"\"\"\n",
    "    processed = text_preprocessing(text)\n",
    "    vec_bow = dictionary.doc2bow(processed)\n",
    "    vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "    return vec_lsi\n",
    "\n",
    "def build_data(tweets, bible_data):\n",
    "    \"\"\"Generate variables for matching.\"\"\"\n",
    "    # Process text\n",
    "    texts = [text_preprocessing(passage) for _, passage in bible_data]\n",
    "    # Build dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Convert bible data to corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)\n",
    "    # Create index\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    # Save all of these\n",
    "    dictionary.save('bible.dict')\n",
    "    corpora.MmCorpus.serialize('bible.mm', corpus)\n",
    "    lsi.save('bible.lsi')\n",
    "    index.save('bible.index')\n",
    "    return dictionary, corpus, lsi, index\n",
    "\n",
    "def get_matches(tweet, bible_data, dictionary, lsi, index):\n",
    "    \"\"\"Match a tweet against the bible_data.\"\"\"\n",
    "    # To run this we need dictionary, lsi, and index variables\n",
    "    # Get matches\n",
    "    vec_lsi = text2vec(tweet, dictionary, lsi)\n",
    "    sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "    scores = [(p, v, s) for (p, v), s in zip(bible_data, sims)]\n",
    "    # Sort by descending score\n",
    "    scores.sort(key=lambda tup: tup[2], reverse = True) \n",
    "    return scores\n",
    "\n",
    "def test_random_tweets(tweets, bible_data, n=5, k=5):\n",
    "    \"\"\"Print n examples for k tweets selected at random.\"\"\"\n",
    "    try:\n",
    "        dictionary = corpora.Dictionary.load('bible.dict')\n",
    "        corpus = corpora.MmCorpus('bible.mm')\n",
    "        lsi = models.LsiModel.load('bible.lsi')\n",
    "        index = similarities.MatrixSimilarity.load('bible.index')\n",
    "    except FileNotFoundError:\n",
    "        dictionary, corpus, lsi, index = build_data(tweets, bible_data)\n",
    "        \n",
    "    import random\n",
    "    num_tweets = len(tweets)\n",
    "    indices = random.sample(range(0, num_tweets), k)\n",
    "    for i in indices:\n",
    "        tweet = tweets[i]\n",
    "        print(\"-----------------\")\n",
    "        print(\"Tweet text: {}\".format(tweet))\n",
    "        scores = get_matches(tweet, bible_data, dictionary, lsi, index)\n",
    "        for verse, passage, score in scores[0:n]:\n",
    "            print(\"\\n{0}, {1}, {2}\".format(verse, passage, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-21 13:41:59,527 : INFO : loading Dictionary object from bible.dict\n",
      "2018-06-21 13:41:59,541 : INFO : loaded bible.dict\n",
      "2018-06-21 13:41:59,552 : INFO : loaded corpus index from bible.mm.index\n",
      "2018-06-21 13:41:59,557 : INFO : initializing cython corpus reader from bible.mm\n",
      "2018-06-21 13:41:59,562 : INFO : accepted corpus with 31102 documents, 12255 features, 339121 non-zero entries\n",
      "2018-06-21 13:41:59,564 : INFO : loading LsiModel object from bible.lsi\n",
      "2018-06-21 13:42:09,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-21 13:42:09,432 : INFO : adding document #10000 to Dictionary(6375 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 13:42:09,651 : INFO : adding document #20000 to Dictionary(9840 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 13:42:09,875 : INFO : adding document #30000 to Dictionary(12041 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-21 13:42:09,905 : INFO : built Dictionary(12255 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...) from 31102 documents (total 370556 corpus positions)\n",
      "2018-06-21 13:42:10,369 : INFO : using serial LSI version on this node\n",
      "2018-06-21 13:42:10,372 : INFO : updating model with new documents\n",
      "2018-06-21 13:42:10,375 : INFO : preparing a new chunk of documents\n",
      "2018-06-21 13:42:10,533 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-21 13:42:10,536 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-21 13:42:10,752 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-21 13:42:11,558 : INFO : 2nd phase: running dense svd on (200, 20000) matrix\n",
      "2018-06-21 13:42:12,703 : INFO : computing the final decomposition\n",
      "2018-06-21 13:42:12,707 : INFO : keeping 100 factors (discarding 18.237% of energy spectrum)\n",
      "2018-06-21 13:42:12,892 : INFO : processed documents up to #20000\n",
      "2018-06-21 13:42:12,900 : INFO : topic #0(128.237): 0.579*\"shall\" + 0.450*\"yahweh\" + 0.419*\"i\" + 0.176*\"said\" + 0.173*\"god\" + 0.131*\"israel\" + 0.108*\"king\" + 0.106*\"the\" + 0.087*\"he\" + 0.085*\"house\"\n",
      "2018-06-21 13:42:12,906 : INFO : topic #1(94.911): -0.741*\"shall\" + 0.569*\"i\" + 0.178*\"yahweh\" + 0.174*\"said\" + 0.089*\"king\" + 0.081*\"god\" + 0.069*\"israel\" + -0.056*\"you\" + 0.049*\"son\" + -0.045*\"offering\"\n",
      "2018-06-21 13:42:12,910 : INFO : topic #2(83.248): -0.656*\"i\" + 0.593*\"yahweh\" + -0.255*\"shall\" + 0.159*\"israel\" + 0.158*\"god\" + 0.138*\"king\" + 0.103*\"the\" + 0.081*\"house\" + 0.081*\"children\" + 0.076*\"son\"\n",
      "2018-06-21 13:42:12,913 : INFO : topic #3(70.173): -0.513*\"yahweh\" + 0.490*\"king\" + 0.355*\"son\" + 0.249*\"the\" + 0.243*\"said\" + 0.167*\"israel\" + 0.139*\"he\" + -0.124*\"i\" + 0.117*\"children\" + 0.107*\"men\"\n",
      "2018-06-21 13:42:12,919 : INFO : topic #4(57.722): -0.504*\"said\" + 0.451*\"son\" + 0.379*\"children\" + 0.350*\"israel\" + -0.339*\"he\" + 0.131*\"i\" + -0.129*\"let\" + -0.118*\"us\" + -0.109*\"go\" + -0.108*\"god\"\n",
      "2018-06-21 13:42:12,922 : INFO : preparing a new chunk of documents\n",
      "2018-06-21 13:42:13,026 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-21 13:42:13,031 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-21 13:42:13,162 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-21 13:42:14,004 : INFO : 2nd phase: running dense svd on (200, 11102) matrix\n",
      "2018-06-21 13:42:14,555 : INFO : computing the final decomposition\n",
      "2018-06-21 13:42:14,558 : INFO : keeping 100 factors (discarding 20.022% of energy spectrum)\n",
      "2018-06-21 13:42:14,734 : INFO : merging projections: (12255, 100) + (12255, 100)\n",
      "2018-06-21 13:42:15,064 : INFO : keeping 100 factors (discarding 5.910% of energy spectrum)\n",
      "2018-06-21 13:42:15,312 : INFO : processed documents up to #31102\n",
      "2018-06-21 13:42:15,316 : INFO : topic #0(152.944): 0.600*\"i\" + 0.513*\"shall\" + 0.354*\"yahweh\" + 0.169*\"said\" + 0.157*\"god\" + 0.104*\"israel\" + 0.092*\"the\" + 0.082*\"king\" + 0.082*\"he\" + 0.075*\"land\"\n",
      "2018-06-21 13:42:15,322 : INFO : topic #1(116.465): -0.735*\"shall\" + 0.651*\"i\" + 0.093*\"said\" + -0.062*\"yahweh\" + -0.050*\"you\" + -0.049*\"offering\" + -0.038*\"the\" + -0.025*\"priest\" + 0.023*\"know\" + 0.021*\"for\"\n",
      "2018-06-21 13:42:15,325 : INFO : topic #2(97.426): 0.592*\"yahweh\" + -0.415*\"i\" + -0.394*\"shall\" + 0.258*\"god\" + 0.177*\"said\" + 0.167*\"israel\" + 0.167*\"king\" + 0.144*\"the\" + 0.118*\"son\" + 0.107*\"house\"\n",
      "2018-06-21 13:42:15,328 : INFO : topic #3(79.927): -0.604*\"yahweh\" + 0.379*\"said\" + 0.303*\"king\" + 0.284*\"son\" + 0.245*\"the\" + 0.245*\"he\" + 0.145*\"man\" + 0.144*\"one\" + 0.096*\"came\" + -0.095*\"i\"\n",
      "2018-06-21 13:42:15,330 : INFO : topic #4(69.851): 0.857*\"god\" + -0.237*\"king\" + -0.213*\"yahweh\" + -0.164*\"son\" + -0.162*\"the\" + -0.105*\"israel\" + 0.103*\"us\" + -0.093*\"children\" + 0.077*\"for\" + 0.064*\"jesus\"\n",
      "2018-06-21 13:42:15,337 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-06-21 13:42:16,933 : INFO : creating matrix with 31102 documents and 100 features\n",
      "2018-06-21 13:42:20,744 : INFO : saving Dictionary object under bible.dict, separately None\n",
      "2018-06-21 13:42:20,756 : INFO : saved bible.dict\n",
      "2018-06-21 13:42:20,762 : INFO : storing corpus in Matrix Market format to bible.mm\n",
      "2018-06-21 13:42:20,768 : INFO : saving sparse matrix to bible.mm\n",
      "2018-06-21 13:42:20,770 : INFO : PROGRESS: saving document #0\n",
      "2018-06-21 13:42:20,809 : INFO : PROGRESS: saving document #1000\n",
      "2018-06-21 13:42:20,846 : INFO : PROGRESS: saving document #2000\n",
      "2018-06-21 13:42:20,884 : INFO : PROGRESS: saving document #3000\n",
      "2018-06-21 13:42:20,923 : INFO : PROGRESS: saving document #4000\n",
      "2018-06-21 13:42:20,961 : INFO : PROGRESS: saving document #5000\n",
      "2018-06-21 13:42:21,002 : INFO : PROGRESS: saving document #6000\n",
      "2018-06-21 13:42:21,041 : INFO : PROGRESS: saving document #7000\n",
      "2018-06-21 13:42:21,084 : INFO : PROGRESS: saving document #8000\n",
      "2018-06-21 13:42:21,125 : INFO : PROGRESS: saving document #9000\n",
      "2018-06-21 13:42:21,167 : INFO : PROGRESS: saving document #10000\n",
      "2018-06-21 13:42:21,203 : INFO : PROGRESS: saving document #11000\n",
      "2018-06-21 13:42:21,245 : INFO : PROGRESS: saving document #12000\n",
      "2018-06-21 13:42:21,286 : INFO : PROGRESS: saving document #13000\n",
      "2018-06-21 13:42:21,314 : INFO : PROGRESS: saving document #14000\n",
      "2018-06-21 13:42:21,345 : INFO : PROGRESS: saving document #15000\n",
      "2018-06-21 13:42:21,377 : INFO : PROGRESS: saving document #16000\n",
      "2018-06-21 13:42:21,405 : INFO : PROGRESS: saving document #17000\n",
      "2018-06-21 13:42:21,448 : INFO : PROGRESS: saving document #18000\n",
      "2018-06-21 13:42:21,488 : INFO : PROGRESS: saving document #19000\n",
      "2018-06-21 13:42:21,530 : INFO : PROGRESS: saving document #20000\n",
      "2018-06-21 13:42:21,583 : INFO : PROGRESS: saving document #21000\n",
      "2018-06-21 13:42:21,627 : INFO : PROGRESS: saving document #22000\n",
      "2018-06-21 13:42:21,680 : INFO : PROGRESS: saving document #23000\n",
      "2018-06-21 13:42:21,727 : INFO : PROGRESS: saving document #24000\n",
      "2018-06-21 13:42:21,762 : INFO : PROGRESS: saving document #25000\n",
      "2018-06-21 13:42:21,795 : INFO : PROGRESS: saving document #26000\n",
      "2018-06-21 13:42:21,827 : INFO : PROGRESS: saving document #27000\n",
      "2018-06-21 13:42:21,868 : INFO : PROGRESS: saving document #28000\n",
      "2018-06-21 13:42:21,903 : INFO : PROGRESS: saving document #29000\n",
      "2018-06-21 13:42:21,935 : INFO : PROGRESS: saving document #30000\n",
      "2018-06-21 13:42:21,970 : INFO : PROGRESS: saving document #31000\n",
      "2018-06-21 13:42:21,977 : INFO : saved 31102x12255 matrix, density=0.089% (339121/381155010)\n",
      "2018-06-21 13:42:21,980 : INFO : saving MmCorpus index to bible.mm.index\n",
      "2018-06-21 13:42:21,985 : INFO : saving Projection object under bibl.lsi.projection, separately None\n",
      "2018-06-21 13:42:22,092 : INFO : saved bibl.lsi.projection\n",
      "2018-06-21 13:42:22,096 : INFO : saving LsiModel object under bibl.lsi, separately None\n",
      "2018-06-21 13:42:22,098 : INFO : not storing attribute projection\n",
      "2018-06-21 13:42:22,101 : INFO : not storing attribute dispatcher\n",
      "2018-06-21 13:42:22,110 : INFO : saved bibl.lsi\n",
      "2018-06-21 13:42:22,115 : INFO : saving MatrixSimilarity object under bible.index, separately None\n",
      "2018-06-21 13:42:22,250 : INFO : saved bible.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Tweet text: @sustrans Kids to Newbridge primary in Bath have river cycle path close - but no safe way to travel 200m up hill & across A-road or to path\n",
      "\n",
      "Genesis 49:17, Dan will be a serpent in the way, an adder in the path, That bites the horse's heels, so that his rider falls backward., 0.9940089583396912\n",
      "\n",
      "Psalm 80:12, Why have you broken down its walls, so that all those who pass by the way pluck it?, 0.9881158471107483\n",
      "\n",
      "Proverbs 13:6, Righteousness guards the way of integrity, but wickedness overthrows the sinner., 0.9859569668769836\n",
      "\n",
      "Genesis 35:19, Rachel died, and was buried in the way to Ephrath (the same is Bethlehem)., 0.9790574312210083\n",
      "\n",
      "Ezekiel 12:5, Dig through the wall in their sight, and carry your stuff out that way., 0.9741089344024658\n",
      "-----------------\n",
      "Tweet text: Stand-Up Comics Have to Censor Their Jokes on (US) College Campuses - The Atlantic http://t.co/W998v8oahs\n",
      "\n",
      "Lamentations 5:16, The crown is fallen from our head: Woe to us! for we have sinned., 0.9783570766448975\n",
      "\n",
      "Acts 28:2, The natives showed us uncommon kindness; for they kindled a fire, and received us all, because of the present rain, and because of the cold., 0.9126466512680054\n",
      "\n",
      "Deuteronomy 26:6, The Egyptians dealt ill with us, and afflicted us, and laid on us hard bondage:, 0.89407879114151\n",
      "\n",
      "Ezra 4:18, The letter which you sent to us has been plainly read before me., 0.863696277141571\n",
      "\n",
      "Judges 9:12, The trees said to the vine, 'Come and reign over us.', 0.8148699998855591\n",
      "-----------------\n",
      "Tweet text: Ha - was just reminded of the 90s Internet time limits, e.g. 5 hours online per week. Could do with that now.\n",
      "\n",
      "Ecclesiastes 3:4, a time to weep, and a time to laugh; a time to mourn, and a time to dance;, 0.9986615180969238\n",
      "\n",
      "Ecclesiastes 3:3, a time to kill, and a time to heal; a time to break down, and a time to build up;, 0.9981067180633545\n",
      "\n",
      "Matthew 26:16, From that time he sought opportunity to betray him., 0.9979551434516907\n",
      "\n",
      "Ecclesiastes 3:2, a time to be born, and a time to die; a time to plant, and a time to pluck up that which is planted;, 0.9977869987487793\n",
      "\n",
      "Hebrews 9:10, being only (with meats and drinks and various washings) fleshly ordinances, imposed until a time of reformation., 0.9975525736808777\n",
      "-----------------\n",
      "Tweet text: Here's What Happened To All 53 of Marissa Mayer's Yahoo Acquisitions https://t.co/0YT2TXncHN\n",
      "\n",
      "Luke 24:51, It happened, while he blessed them, that he withdrew from them, and was carried up into heaven., 0.8907666802406311\n",
      "\n",
      "Joshua 5:8, It happened, when they were done circumcising all the nation, that they stayed in their places in the camp until they were healed., 0.8737368583679199\n",
      "\n",
      "1 Kings 15:21, It happened, when Baasha heard of it, that he left off building Ramah, and lived in Tirzah., 0.8584436178207397\n",
      "\n",
      "Joshua 19:26, Allammelech, Amad, Mishal. It reached to Carmel westward, and to Shihorlibnath., 0.8564523458480835\n",
      "\n",
      "2 Corinthians 9:1, It is indeed unnecessary for me to write to you concerning the service to the saints,, 0.852521538734436\n",
      "-----------------\n",
      "Tweet text: Even with more complex deep architectures you can get a surprising amount done with simple ngram approaches - a future in hybrids? https://t.co/jD3pIhnIbP\n",
      "\n",
      "Daniel 9:5, we have sinned, and have dealt perversely, and have done wickedly, and have rebelled, even turning aside from your precepts and from your ordinances;, 0.9942108988761902\n",
      "\n",
      "Ephesians 5:11, Have no fellowship with the unfruitful works of darkness, but rather even reprove them., 0.9036127924919128\n",
      "\n",
      "Joshua 10:41, Joshua struck them from Kadesh Barnea even to Gaza, and all the country of Goshen, even to Gibeon., 0.885510265827179\n",
      "\n",
      "Numbers 21:30, We have shot at them. Heshbon has perished even to Dibon. We have laid waste even to Nophah, Which reaches to Medeba., 0.8847762942314148\n",
      "\n",
      "Deuteronomy 4:48, from Aroer, which is on the edge of the valley of the Arnon, even to Mount Sion (the same is Hermon),, 0.882093071937561\n"
     ]
    }
   ],
   "source": [
    "test_random_tweets(tweets, bible_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Approaches\n",
    "\n",
    "To compare the approaches, let's generate 200 random examples and take the top match for each of the three techniques. We will then manually score each match on a scale of 0 to 5 where 0 = no match and 5 = perfect match. Then we can see which technique comes up on top.\n",
    "\n",
    "The easiest way to quickly compare the results is to export to a spreadsheet, with columns for the scores of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difflib_matches(tweet, bible_data):\n",
    "    \"\"\"Match a tweet against the bible_data.\"\"\"\n",
    "    # Get matches\n",
    "    scores = [\n",
    "        (verse, passage, SequenceMatcher(None, tweet, passage).ratio()) \n",
    "        for verse, passage in bible_data\n",
    "    ]\n",
    "    # Sort by descending score\n",
    "    scores.sort(key=lambda tup: tup[2], reverse = True) \n",
    "    return scores\n",
    "\n",
    "def get_spacy_matches(spacy_tweet, spacy_bible):\n",
    "    \"\"\"Perform matches on text as spacy docs\"\"\"\n",
    "    # Get matches\n",
    "    scores = [\n",
    "        (verse, passage, spacy_tweet.similarity(passage)) \n",
    "        for verse, passage in spacy_bible\n",
    "    ]\n",
    "    # Sort by descending score\n",
    "    scores.sort(key=lambda tup: tup[2], reverse = True) \n",
    "    return scores\n",
    "\n",
    "def get_gensim_matches(tweet, bible_data, dictionary, lsi, index):\n",
    "    \"\"\"Match a tweet against the bible_data.\"\"\"\n",
    "    # To run this we need dictionary, lsi, and index variables\n",
    "    # Get matches\n",
    "    vec_lsi = text2vec(tweet, dictionary, lsi)\n",
    "    sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "    scores = [(v, p, s) for (v, p), s in zip(bible_data, sims)]\n",
    "    # Sort by descending score\n",
    "    scores.sort(key=lambda tup: tup[2], reverse = True) \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy_bible = [(verse, nlp(passage)) for verse, passage in bible_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-23 09:02:25,811 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-23 09:02:26,052 : INFO : adding document #10000 to Dictionary(6375 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-23 09:02:26,272 : INFO : adding document #20000 to Dictionary(9840 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-23 09:02:26,491 : INFO : adding document #30000 to Dictionary(12041 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...)\n",
      "2018-06-23 09:02:26,516 : INFO : built Dictionary(12255 unique tokens: ['beginning', 'created', 'earth', 'god', 'heavens']...) from 31102 documents (total 370556 corpus positions)\n",
      "2018-06-23 09:02:26,953 : INFO : using serial LSI version on this node\n",
      "2018-06-23 09:02:26,954 : INFO : updating model with new documents\n",
      "2018-06-23 09:02:26,955 : INFO : preparing a new chunk of documents\n",
      "2018-06-23 09:02:27,111 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-23 09:02:27,112 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-23 09:02:27,349 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-23 09:02:28,118 : INFO : 2nd phase: running dense svd on (200, 20000) matrix\n",
      "2018-06-23 09:02:29,147 : INFO : computing the final decomposition\n",
      "2018-06-23 09:02:29,161 : INFO : keeping 100 factors (discarding 18.228% of energy spectrum)\n",
      "2018-06-23 09:02:29,352 : INFO : processed documents up to #20000\n",
      "2018-06-23 09:02:29,357 : INFO : topic #0(128.237): 0.579*\"shall\" + 0.450*\"yahweh\" + 0.419*\"i\" + 0.176*\"said\" + 0.173*\"god\" + 0.131*\"israel\" + 0.108*\"king\" + 0.106*\"the\" + 0.087*\"he\" + 0.085*\"house\"\n",
      "2018-06-23 09:02:29,359 : INFO : topic #1(94.911): 0.741*\"shall\" + -0.569*\"i\" + -0.178*\"yahweh\" + -0.174*\"said\" + -0.089*\"king\" + -0.081*\"god\" + -0.069*\"israel\" + 0.056*\"you\" + -0.049*\"son\" + 0.045*\"offering\"\n",
      "2018-06-23 09:02:29,361 : INFO : topic #2(83.248): -0.656*\"i\" + 0.593*\"yahweh\" + -0.255*\"shall\" + 0.159*\"israel\" + 0.158*\"god\" + 0.138*\"king\" + 0.103*\"the\" + 0.081*\"house\" + 0.081*\"children\" + 0.076*\"son\"\n",
      "2018-06-23 09:02:29,363 : INFO : topic #3(70.173): 0.513*\"yahweh\" + -0.490*\"king\" + -0.355*\"son\" + -0.249*\"the\" + -0.243*\"said\" + -0.167*\"israel\" + -0.139*\"he\" + 0.124*\"i\" + -0.117*\"children\" + -0.107*\"men\"\n",
      "2018-06-23 09:02:29,365 : INFO : topic #4(57.722): 0.504*\"said\" + -0.451*\"son\" + -0.379*\"children\" + -0.350*\"israel\" + 0.339*\"he\" + -0.131*\"i\" + 0.129*\"let\" + 0.118*\"us\" + 0.109*\"go\" + 0.108*\"god\"\n",
      "2018-06-23 09:02:29,367 : INFO : preparing a new chunk of documents\n",
      "2018-06-23 09:02:29,467 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-23 09:02:29,469 : INFO : 1st phase: constructing (12255, 200) action matrix\n",
      "2018-06-23 09:02:29,587 : INFO : orthonormalizing (12255, 200) action matrix\n",
      "2018-06-23 09:02:30,386 : INFO : 2nd phase: running dense svd on (200, 11102) matrix\n",
      "2018-06-23 09:02:30,731 : INFO : computing the final decomposition\n",
      "2018-06-23 09:02:30,732 : INFO : keeping 100 factors (discarding 19.942% of energy spectrum)\n",
      "2018-06-23 09:02:30,935 : INFO : merging projections: (12255, 100) + (12255, 100)\n",
      "2018-06-23 09:02:31,243 : INFO : keeping 100 factors (discarding 5.902% of energy spectrum)\n",
      "2018-06-23 09:02:31,483 : INFO : processed documents up to #31102\n",
      "2018-06-23 09:02:31,485 : INFO : topic #0(152.944): 0.600*\"i\" + 0.513*\"shall\" + 0.354*\"yahweh\" + 0.169*\"said\" + 0.157*\"god\" + 0.104*\"israel\" + 0.092*\"the\" + 0.082*\"king\" + 0.082*\"he\" + 0.075*\"land\"\n",
      "2018-06-23 09:02:31,486 : INFO : topic #1(116.465): -0.735*\"shall\" + 0.651*\"i\" + 0.093*\"said\" + -0.062*\"yahweh\" + -0.050*\"you\" + -0.049*\"offering\" + -0.038*\"the\" + -0.025*\"priest\" + 0.023*\"know\" + 0.021*\"for\"\n",
      "2018-06-23 09:02:31,487 : INFO : topic #2(97.426): 0.592*\"yahweh\" + -0.415*\"i\" + -0.394*\"shall\" + 0.258*\"god\" + 0.177*\"said\" + 0.167*\"israel\" + 0.167*\"king\" + 0.144*\"the\" + 0.118*\"son\" + 0.107*\"house\"\n",
      "2018-06-23 09:02:31,489 : INFO : topic #3(79.928): -0.604*\"yahweh\" + 0.379*\"said\" + 0.303*\"king\" + 0.284*\"son\" + 0.245*\"the\" + 0.245*\"he\" + 0.145*\"man\" + 0.144*\"one\" + 0.096*\"came\" + -0.095*\"i\"\n",
      "2018-06-23 09:02:31,491 : INFO : topic #4(69.852): 0.857*\"god\" + -0.237*\"king\" + -0.213*\"yahweh\" + -0.165*\"son\" + -0.162*\"the\" + -0.105*\"israel\" + 0.103*\"us\" + -0.093*\"children\" + 0.077*\"for\" + 0.064*\"jesus\"\n",
      "2018-06-23 09:02:31,492 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-06-23 09:02:32,948 : INFO : creating matrix with 31102 documents and 100 features\n",
      "2018-06-23 09:02:36,645 : INFO : saving Dictionary object under bible.dict, separately None\n",
      "2018-06-23 09:02:36,713 : INFO : saved bible.dict\n",
      "2018-06-23 09:02:36,714 : INFO : storing corpus in Matrix Market format to bible.mm\n",
      "2018-06-23 09:02:36,715 : INFO : saving sparse matrix to bible.mm\n",
      "2018-06-23 09:02:36,716 : INFO : PROGRESS: saving document #0\n",
      "2018-06-23 09:02:36,753 : INFO : PROGRESS: saving document #1000\n",
      "2018-06-23 09:02:36,789 : INFO : PROGRESS: saving document #2000\n",
      "2018-06-23 09:02:36,832 : INFO : PROGRESS: saving document #3000\n",
      "2018-06-23 09:02:36,863 : INFO : PROGRESS: saving document #4000\n",
      "2018-06-23 09:02:36,898 : INFO : PROGRESS: saving document #5000\n",
      "2018-06-23 09:02:36,933 : INFO : PROGRESS: saving document #6000\n",
      "2018-06-23 09:02:36,970 : INFO : PROGRESS: saving document #7000\n",
      "2018-06-23 09:02:37,005 : INFO : PROGRESS: saving document #8000\n",
      "2018-06-23 09:02:37,041 : INFO : PROGRESS: saving document #9000\n",
      "2018-06-23 09:02:37,081 : INFO : PROGRESS: saving document #10000\n",
      "2018-06-23 09:02:37,123 : INFO : PROGRESS: saving document #11000\n",
      "2018-06-23 09:02:37,169 : INFO : PROGRESS: saving document #12000\n",
      "2018-06-23 09:02:37,200 : INFO : PROGRESS: saving document #13000\n",
      "2018-06-23 09:02:37,224 : INFO : PROGRESS: saving document #14000\n",
      "2018-06-23 09:02:37,251 : INFO : PROGRESS: saving document #15000\n",
      "2018-06-23 09:02:37,277 : INFO : PROGRESS: saving document #16000\n",
      "2018-06-23 09:02:37,311 : INFO : PROGRESS: saving document #17000\n",
      "2018-06-23 09:02:37,350 : INFO : PROGRESS: saving document #18000\n",
      "2018-06-23 09:02:37,386 : INFO : PROGRESS: saving document #19000\n",
      "2018-06-23 09:02:37,421 : INFO : PROGRESS: saving document #20000\n",
      "2018-06-23 09:02:37,453 : INFO : PROGRESS: saving document #21000\n",
      "2018-06-23 09:02:37,488 : INFO : PROGRESS: saving document #22000\n",
      "2018-06-23 09:02:37,522 : INFO : PROGRESS: saving document #23000\n",
      "2018-06-23 09:02:37,563 : INFO : PROGRESS: saving document #24000\n",
      "2018-06-23 09:02:37,591 : INFO : PROGRESS: saving document #25000\n",
      "2018-06-23 09:02:37,621 : INFO : PROGRESS: saving document #26000\n",
      "2018-06-23 09:02:37,648 : INFO : PROGRESS: saving document #27000\n",
      "2018-06-23 09:02:37,680 : INFO : PROGRESS: saving document #28000\n",
      "2018-06-23 09:02:37,719 : INFO : PROGRESS: saving document #29000\n",
      "2018-06-23 09:02:37,758 : INFO : PROGRESS: saving document #30000\n",
      "2018-06-23 09:02:37,790 : INFO : PROGRESS: saving document #31000\n",
      "2018-06-23 09:02:37,795 : INFO : saved 31102x12255 matrix, density=0.089% (339121/381155010)\n",
      "2018-06-23 09:02:37,797 : INFO : saving MmCorpus index to bible.mm.index\n",
      "2018-06-23 09:02:37,799 : INFO : saving Projection object under bible.lsi.projection, separately None\n",
      "2018-06-23 09:02:37,944 : INFO : saved bible.lsi.projection\n",
      "2018-06-23 09:02:37,945 : INFO : saving LsiModel object under bible.lsi, separately None\n",
      "2018-06-23 09:02:37,946 : INFO : not storing attribute projection\n",
      "2018-06-23 09:02:37,947 : INFO : not storing attribute dispatcher\n",
      "2018-06-23 09:02:37,953 : INFO : saved bible.lsi\n",
      "2018-06-23 09:02:37,954 : INFO : saving MatrixSimilarity object under bible.index, separately None\n",
      "2018-06-23 09:02:38,053 : INFO : saved bible.index\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus, lsi, index = build_data(tweets, bible_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805\n",
      "6338\n",
      "1954\n",
      "187\n",
      "643\n",
      "7804\n",
      "6135\n",
      "164\n",
      "6627\n",
      "8612\n",
      "4862\n",
      "2978\n",
      "9167\n",
      "6449\n",
      "3845\n",
      "1993\n",
      "3365\n",
      "5952\n",
      "941\n",
      "313\n",
      "314\n",
      "9059\n",
      "8760\n",
      "1841\n",
      "5401\n",
      "2997\n",
      "8512\n",
      "5086\n",
      "6803\n",
      "2831\n",
      "2655\n",
      "9023\n",
      "3000\n",
      "476\n",
      "2465\n",
      "783\n",
      "2912\n",
      "7930\n",
      "7052\n",
      "4218\n",
      "707\n",
      "6193\n",
      "3530\n",
      "3201\n",
      "3774\n",
      "5127\n",
      "9574\n",
      "2833\n",
      "9105\n",
      "8807\n",
      "2772\n",
      "8165\n",
      "7355\n",
      "2892\n",
      "9623\n",
      "8057\n",
      "6258\n",
      "9365\n",
      "6321\n",
      "6285\n",
      "971\n",
      "6196\n",
      "7150\n",
      "3677\n",
      "8989\n",
      "4857\n",
      "5877\n",
      "4966\n",
      "3596\n",
      "4438\n",
      "4617\n",
      "526\n",
      "1084\n",
      "2951\n",
      "9232\n",
      "2905\n",
      "7975\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rows = []\n",
    "k = 200\n",
    "num_tweets = len(tweets)\n",
    "indices = random.sample(range(0, num_tweets), k)\n",
    "for i in indices:\n",
    "    print(i)\n",
    "    tweet = tweets[i]\n",
    "    dl_match = get_difflib_matches(tweet, bible_data)[0]\n",
    "    spacy_tweet = nlp(tweet)\n",
    "    sp_match = get_spacy_matches(spacy_tweet, spacy_bible)[0]\n",
    "    gs_match = get_gensim_matches(tweet, bible_data, dictionary, lsi, index)[0]\n",
    "    comparison_row = (\n",
    "        tweet, \n",
    "        dl_match[1], dl_match[2], \"\", \n",
    "        sp_match[1], sp_match[2], \"\", \n",
    "        gs_match[1], gs_match[2], \"\"\n",
    "    )\n",
    "    rows.append(comparison_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that I may reveal it as I ought to speak.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"rows.pkl\", 'wb') as f:\n",
    "    pickle.dump(rows, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
