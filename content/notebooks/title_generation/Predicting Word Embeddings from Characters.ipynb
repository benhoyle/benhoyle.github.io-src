{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we create an encoder-decoder model for character data that acts as an embedding layer?\n",
    "\n",
    "If we could, our models could just predict the embedding, which may then be decoded to generate the word. For example, our decoder could just predict a 100D vector for each timestamp, which represents the word.\n",
    "\n",
    "Don't reset our GRU state as we read in characters?\n",
    "\n",
    "Plan:\n",
    "\n",
    "* Load Glove Embeddings.\n",
    "* Load text.\n",
    "* Tokenize text > words.\n",
    "* For each word - build an encoder:\n",
    "    * Clean characters.\n",
    "    * Add control characters to start and end > convert to numbers.\n",
    "    * Batch by length of word - 0-5,5-10,10-20, 20+ (or cap /arbitrarily break at 20?)\n",
    "    * Set as GRU sequences;\n",
    "    * Set Y as embedding for word;\n",
    "    * Use MSE loss and train.\n",
    "* Also build a decoder:\n",
    "    * Input initial state = embedding. Token = start sequence.\n",
    "    * Train with input sequence char + output sequence char shifted by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "30000 samples loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "PIK = \"claim_and_title.data\"\n",
    "\n",
    "if not os.path.isfile(PIK):\n",
    "    # Download file\n",
    "    !wget https://benhoyle.github.io/notebooks/title_generation/claim_and_title.data\n",
    "\n",
    "with open(PIK, \"rb\") as f:\n",
    "    print(\"Loading data\")\n",
    "    data = pickle.load(f)\n",
    "    print(\"{0} samples loaded\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "data_as_words = [word_tokenize(d[0].lower().strip()) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'managing',\n",
       " 'a',\n",
       " 'backup',\n",
       " 'service',\n",
       " 'gateway',\n",
       " '(']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_as_words[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out non-words\n",
    "data_as_words = [word for claim in data_as_words for word in claim if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 6268278 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} words.\".format(len(data_as_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'managing',\n",
       " 'a',\n",
       " 'backup',\n",
       " 'service',\n",
       " 'gateway',\n",
       " 'sgw',\n",
       " 'associated']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_as_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe 100d embeddings from file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load Glove Embeddings\n",
    "print(\"Loading GloVe 100d embeddings from file\")\n",
    "GLOVE_DIR = \"glove/\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    # Tweaked to decode the binary text values\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 character tokens\n"
     ]
    }
   ],
   "source": [
    "# Setup character vocab \n",
    "import string\n",
    "char_vocab = [\"_PAD_\", \"_SOW_\", \"_EOW_\"] + list(string.ascii_lowercase)\n",
    "print(\"There are {} character tokens\".format(len(char_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert words to ints\n",
    "# Build dictionaries\n",
    "reverse_dict = {i: c for i, c in enumerate(char_vocab)}\n",
    "forward_dict = {v: k for k, v in reverse_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate embedding - character list pairs\n",
    "dataset = []\n",
    "for word in data_as_words:\n",
    "    embedding = embeddings_index.get(word, None)\n",
    "    if embedding is not None:\n",
    "        try:\n",
    "            word_as_int = [forward_dict[\"_SOW_\"]] + [forward_dict[c] for c in word] + [forward_dict[\"_EOW_\"]]\n",
    "            dataset.append((embedding, word_as_int))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 6251296 entries in the dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-4.1796e-01,  5.3241e-01, -3.3693e-01,  6.9262e-01,  6.8686e-02,\n",
       "        -8.7566e-02,  2.7663e-01, -7.1203e-02, -3.3530e-01,  3.3365e-01,\n",
       "        -5.2407e-01, -5.3341e-01,  4.0460e-02, -1.6659e-01,  4.5481e-01,\n",
       "        -3.4785e-01,  2.4926e-01,  2.2734e-02,  4.2120e-01,  6.2452e-03,\n",
       "        -1.3359e-01, -5.1309e-01,  2.1382e-01, -2.4943e-01, -3.6553e-01,\n",
       "        -1.0690e-02,  2.5961e-01, -2.9090e-01, -7.6780e-01,  3.7387e-02,\n",
       "        -4.6895e-01,  8.9278e-01, -5.3101e-01, -3.8841e-01,  3.5913e-01,\n",
       "        -4.7930e-01, -3.3601e-01, -2.0371e-01,  3.9182e-01, -9.5175e-01,\n",
       "        -6.2779e-01,  1.7753e-01, -3.5282e-02, -1.2463e-01, -8.4179e-01,\n",
       "        -3.4632e-01, -1.9631e-01, -8.4428e-01, -5.4253e-01, -1.9322e-01,\n",
       "         2.5287e-01, -1.2990e-02, -1.4000e-03,  1.4574e+00, -7.6291e-01,\n",
       "        -1.5731e+00, -2.3982e-01,  2.2269e-01,  8.7785e-01, -3.4078e-02,\n",
       "        -3.6702e-02, -3.1008e-01,  8.0324e-01,  6.1032e-01,  1.3546e+00,\n",
       "        -1.4522e-01, -1.9501e-01, -6.8654e-01, -9.6186e-02, -5.0487e-01,\n",
       "        -1.0144e+00,  3.7278e-01,  6.3403e-01,  6.6473e-01,  3.3144e-01,\n",
       "        -6.2283e-01, -1.1496e-01, -6.0323e-01, -4.6632e-01, -1.7908e-01,\n",
       "         2.3923e-01,  2.5257e-01, -1.1826e+00, -6.0126e-01, -1.8759e+00,\n",
       "         9.2491e-01,  8.7735e-01, -6.2709e-01, -2.6522e-01, -1.0025e-01,\n",
       "        -5.8459e-01,  1.2821e-01,  3.4710e-01,  5.2632e-01,  7.4769e-01,\n",
       "        -3.3396e-01, -3.6954e-01, -7.5431e-01,  3.4056e-01,  2.8563e-01],\n",
       "       dtype=float32), [1, 15, 7, 22, 10, 17, 6, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"There are now {0} entries in the dataset\".format(len(dataset)))\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# char_emb_dim = 8\n",
    "#decoder_embedding = Embedding(input_dim = num_decoder_tokens, output_dim=char_emb_dim)(decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cut a long story short - we need to use one-hot encodings of our character data for our decoder inputs and outputs. As there are around 6m samples, we need to break up our training data into batches (we have matrices of 6m, 20, 29). As we need to break our training data into batches, we can maybe create batches of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_dim = 100\n",
    "num_decoder_tokens = len(char_vocab)\n",
    "# Limit word length to 20\n",
    "decoder_seq_length = 20\n",
    "\n",
    "encoded_state = Input(shape=(embedding_dim,), name=\"EncodedState\")\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name=\"DecoderInputs\")\n",
    "decoder_gru = GRU(embedding_dim, return_sequences=True, name=\"Decoder\")\n",
    "decoder_outputs = decoder_gru(decoder_inputs, initial_state=encoded_state)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name=\"VocabProjection\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoded_state, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1862\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1489\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1491\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-512d6967aa33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# pydot raises a generic Exception here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# so no specific class can be caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     32\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image\n",
    "Image(model_to_dot(model, show_shapes=True).create_png(prog='dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_to_one_hot(input_seqs, decoder_seq_length, num_decoder_tokens):\n",
    "    \"\"\" Convert a sequence of integers to normal and shifted one-hot representations a one element shifted\n",
    "        sequence of one-hot vectors.\"\"\"\n",
    "    length = len(input_seqs)\n",
    "    one_hot_in = np.zeros(\n",
    "            (length, decoder_seq_length, num_decoder_tokens)\n",
    "            )\n",
    "    one_hot_out = np.zeros(\n",
    "            (length, decoder_seq_length, num_decoder_tokens)\n",
    "            )\n",
    "    for i, sequence in enumerate(input_seqs):\n",
    "        for timestamp, word_int in enumerate(sequence):\n",
    "            one_hot_in[i, timestamp, word_int] = 1\n",
    "            if timestamp > 0:\n",
    "                # Shift decoder target get so it is one ahead\n",
    "                one_hot_out[i, timestamp-1, word_int] = 1\n",
    "    return one_hot_in, one_hot_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With embedding for characters.\n",
    "```\n",
    "from keras.layers import GRU, Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_dim = 100\n",
    "num_decoder_tokens = len(char_vocab)\n",
    "# Limit word length to 20\n",
    "decoder_seq_length = 20\n",
    "# Character embedding dimension\n",
    "char_emb_dim = 8\n",
    "\n",
    "encoded_state = Input(shape=(embedding_dim,), name=\"EncodedState\")\n",
    "decoder_inputs = Input(shape=(None, ), name=\"DecoderInputs\")\n",
    "decoder_embedding = Embedding(input_dim = num_decoder_tokens, output_dim=char_emb_dim)(decoder_inputs)\n",
    "decoder_gru = GRU(embedding_dim, return_sequences=True, name=\"Decoder\")\n",
    "decoder_outputs = decoder_gru(decoder_embedding, initial_state=encoded_state)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name=\"VocabProjection\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoded_state, decoder_inputs], decoder_outputs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our longest sequence is 24 tokens long.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d3b90ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Let's have a look at our word length distribution\n",
    "Y_length = [len(y) for e, y in dataset]\n",
    "max_y_length = max(Y_length)\n",
    "print(\"Our longest sequence is {0} tokens long.\".format(max_y_length))\n",
    "\n",
    "bins = np.linspace(0, max_y_length, 25)\n",
    "plt.hist(Y_length, bins)\n",
    "plt.title('Title - Sequence Length')\n",
    "plt.ylabel('No. of samples')\n",
    "plt.xlabel('Sequence Length (words)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we try using the keras to_categorial method and then shift this as before\n",
    "# To test our model works we'll just select the first 500k sampes\n",
    "decoder_in_seqs = [y for e,y in dataset[0:50000]]\n",
    "decoder_in_seqs, decoder_out_seqs = inputs_to_one_hot(decoder_in_seqs, decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20, 29) (50000, 20, 29)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_in_seqs.shape, decoder_out_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we split our data into batches of different lengths - we can then compute our shifted output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note - with ints we can use just array slicing to shift\n",
    "# Could we build one hot conversion into model as a layer? Would save having to convert all the data first\n",
    "# decoder_out_seqs = np.zeros(decoder_in_seqs.shape, dtype='int32')\n",
    "# decoder_out_seqs[:, 0:19] = decoder_in_seqs[:, 1:20]\n",
    "# decoder_out_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_states = np.array([e for e, y in dataset[0:50000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 24s 523us/step - loss: 0.3078 - acc: 0.2275 - val_loss: 0.2953 - val_acc: 0.2339\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 23s 519us/step - loss: 0.2238 - acc: 0.2523 - val_loss: 0.2541 - val_acc: 0.2459\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 24s 524us/step - loss: 0.1766 - acc: 0.2657 - val_loss: 0.2285 - val_acc: 0.2539\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 23s 522us/step - loss: 0.1460 - acc: 0.2744 - val_loss: 0.2124 - val_acc: 0.2604\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 23s 521us/step - loss: 0.1247 - acc: 0.2808 - val_loss: 0.2050 - val_acc: 0.2630\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 24s 523us/step - loss: 0.1091 - acc: 0.2850 - val_loss: 0.1967 - val_acc: 0.2672\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 23s 522us/step - loss: 0.0972 - acc: 0.2882 - val_loss: 0.1954 - val_acc: 0.2696\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 23s 520us/step - loss: 0.0874 - acc: 0.2910 - val_loss: 0.1912 - val_acc: 0.2711\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 23s 518us/step - loss: 0.0797 - acc: 0.2930 - val_loss: 0.1935 - val_acc: 0.2714\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 23s 516us/step - loss: 0.0734 - acc: 0.2947 - val_loss: 0.1907 - val_acc: 0.2738\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 23s 516us/step - loss: 0.0679 - acc: 0.2961 - val_loss: 0.1920 - val_acc: 0.2746\n",
      "Epoch 12/100\n",
      "42688/45000 [===========================>..] - ETA: 1s - loss: 0.0629 - acc: 0.2973"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-1d2d7ece8517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_in_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_out_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callback = model.fit([encoded_states, decoder_in_seqs], decoder_out_seqs, validation_split=0.1, batch_size=64, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Initial Training\n",
    "\n",
    "This seems to reach a local minima within 10 epochs.\n",
    "\n",
    "Now we need to code up our loop through the 6million examples in sets of 100k. We'll need to initially create different test and training datasets that we keep separate. Can we use / edit our existing models?\n",
    "\n",
    "What does training do? It captures information about two things:\n",
    "\n",
    "* The data values themselves (e.g. character sequences per word); and\n",
    "* The distribution of the words in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [e for e, y in dataset]\n",
    "output_data = [y for e, y in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from char_gen import CharS2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build an abstract class that has the encoder and decoder systems as separate replaceable modules. E.g. process_encoder_input, process_decoder_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Loaded weights\n",
      "Generating training and test data\n"
     ]
    }
   ],
   "source": [
    "char_s2s = CharS2S(\n",
    "    encoder_states=input_data,\n",
    "    decoder_seqs=output_data,\n",
    "    decoder_seq_length=20,\n",
    "    latent_dim=100,\n",
    "    weights_file=\"charstate2seq.hdf5\",\n",
    "    training_set_size=50000,  # Due to memory we need to train in sets\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 50000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 22s 439us/step - loss: 0.6185 - acc: 0.1368 - val_loss: 0.4443 - val_acc: 0.1886\n",
      "Training on batch 50000 to 100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.3761 - acc: 0.2077 - val_loss: 0.3261 - val_acc: 0.2227\n",
      "Training on batch 100000 to 150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 0.2954 - acc: 0.2294 - val_loss: 0.2736 - val_acc: 0.2381\n",
      "Training on batch 150000 to 200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 0.2524 - acc: 0.2438 - val_loss: 0.2303 - val_acc: 0.2492\n",
      "Training on batch 200000 to 250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.2182 - acc: 0.2527 - val_loss: 0.2063 - val_acc: 0.2558\n",
      "Training on batch 250000 to 300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.1979 - acc: 0.2589 - val_loss: 0.1833 - val_acc: 0.2622\n",
      "Training on batch 300000 to 350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.1813 - acc: 0.2640 - val_loss: 0.1764 - val_acc: 0.2681\n",
      "Training on batch 350000 to 400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 381us/step - loss: 0.1678 - acc: 0.2662 - val_loss: 0.1661 - val_acc: 0.2702\n",
      "Training on batch 400000 to 450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 381us/step - loss: 0.1570 - acc: 0.2716 - val_loss: 0.1524 - val_acc: 0.2698\n",
      "Training on batch 450000 to 500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.1485 - acc: 0.2734 - val_loss: 0.1445 - val_acc: 0.2731\n",
      "Training on batch 500000 to 550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.1405 - acc: 0.2736 - val_loss: 0.1398 - val_acc: 0.2772\n",
      "Training on batch 550000 to 600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.1344 - acc: 0.2752 - val_loss: 0.1312 - val_acc: 0.2765\n",
      "Training on batch 600000 to 650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.1286 - acc: 0.2773 - val_loss: 0.1255 - val_acc: 0.2753\n",
      "Training on batch 650000 to 700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.1258 - acc: 0.2797 - val_loss: 0.1195 - val_acc: 0.2815\n",
      "Training on batch 700000 to 750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.1207 - acc: 0.2810 - val_loss: 0.1176 - val_acc: 0.2825\n",
      "Training on batch 750000 to 800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.1144 - acc: 0.2796 - val_loss: 0.1138 - val_acc: 0.2828\n",
      "Training on batch 800000 to 850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.1148 - acc: 0.2816 - val_loss: 0.1134 - val_acc: 0.2834\n",
      "Training on batch 850000 to 900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.1092 - acc: 0.2825 - val_loss: 0.1092 - val_acc: 0.2827\n",
      "Training on batch 900000 to 950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.1069 - acc: 0.2830 - val_loss: 0.1058 - val_acc: 0.2833\n",
      "Training on batch 950000 to 1000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.1048 - acc: 0.2833 - val_loss: 0.1068 - val_acc: 0.2833\n",
      "Training on batch 1000000 to 1050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.1024 - acc: 0.2837 - val_loss: 0.0973 - val_acc: 0.2854\n",
      "Training on batch 1050000 to 1100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.1008 - acc: 0.2835 - val_loss: 0.0974 - val_acc: 0.2860\n",
      "Training on batch 1100000 to 1150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0977 - acc: 0.2855 - val_loss: 0.0962 - val_acc: 0.2838\n",
      "Training on batch 1150000 to 1200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 381us/step - loss: 0.0985 - acc: 0.2864 - val_loss: 0.0947 - val_acc: 0.2864\n",
      "Training on batch 1200000 to 1250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0948 - acc: 0.2875 - val_loss: 0.0970 - val_acc: 0.2866\n",
      "Training on batch 1250000 to 1300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 376us/step - loss: 0.0927 - acc: 0.2873 - val_loss: 0.0911 - val_acc: 0.2879\n",
      "Training on batch 1300000 to 1350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0919 - acc: 0.2882 - val_loss: 0.0856 - val_acc: 0.2880\n",
      "Training on batch 1350000 to 1400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0908 - acc: 0.2871 - val_loss: 0.0904 - val_acc: 0.2897\n",
      "Training on batch 1400000 to 1450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0882 - acc: 0.2877 - val_loss: 0.0897 - val_acc: 0.2887\n",
      "Training on batch 1450000 to 1500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 0.0898 - acc: 0.2882 - val_loss: 0.0884 - val_acc: 0.2878\n",
      "Training on batch 1500000 to 1550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 377us/step - loss: 0.0870 - acc: 0.2881 - val_loss: 0.0885 - val_acc: 0.2890\n",
      "Training on batch 1550000 to 1600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0868 - acc: 0.2900 - val_loss: 0.0856 - val_acc: 0.2882\n",
      "Training on batch 1600000 to 1650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 376us/step - loss: 0.0849 - acc: 0.2882 - val_loss: 0.0847 - val_acc: 0.2917\n",
      "Training on batch 1650000 to 1700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 0.0860 - acc: 0.2896 - val_loss: 0.0794 - val_acc: 0.2902\n",
      "Training on batch 1700000 to 1750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0844 - acc: 0.2899 - val_loss: 0.0819 - val_acc: 0.2903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1750000 to 1800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.0844 - acc: 0.2892 - val_loss: 0.0857 - val_acc: 0.2902\n",
      "Training on batch 1800000 to 1850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 373us/step - loss: 0.0806 - acc: 0.2912 - val_loss: 0.0788 - val_acc: 0.2912\n",
      "Training on batch 1850000 to 1900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 376us/step - loss: 0.0802 - acc: 0.2903 - val_loss: 0.0801 - val_acc: 0.2916\n",
      "Training on batch 1900000 to 1950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0796 - acc: 0.2908 - val_loss: 0.0789 - val_acc: 0.2917\n",
      "Training on batch 1950000 to 2000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0787 - acc: 0.2917 - val_loss: 0.0845 - val_acc: 0.2929\n",
      "Training on batch 2000000 to 2050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0772 - acc: 0.2912 - val_loss: 0.0818 - val_acc: 0.2922\n",
      "Training on batch 2050000 to 2100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 382us/step - loss: 0.0773 - acc: 0.2911 - val_loss: 0.0827 - val_acc: 0.2896\n",
      "Training on batch 2100000 to 2150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0785 - acc: 0.2914 - val_loss: 0.0818 - val_acc: 0.2921\n",
      "Training on batch 2150000 to 2200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 399us/step - loss: 0.0787 - acc: 0.2917 - val_loss: 0.0759 - val_acc: 0.2921\n",
      "Training on batch 2200000 to 2250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 381us/step - loss: 0.0778 - acc: 0.2918 - val_loss: 0.0756 - val_acc: 0.2928\n",
      "Training on batch 2250000 to 2300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.0754 - acc: 0.2919 - val_loss: 0.0794 - val_acc: 0.2920\n",
      "Training on batch 2300000 to 2350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.0745 - acc: 0.2919 - val_loss: 0.0732 - val_acc: 0.2922\n",
      "Training on batch 2350000 to 2400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0744 - acc: 0.2929 - val_loss: 0.0755 - val_acc: 0.2896\n",
      "Training on batch 2400000 to 2450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0760 - acc: 0.2929 - val_loss: 0.0738 - val_acc: 0.2929\n",
      "Training on batch 2450000 to 2500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0736 - acc: 0.2925 - val_loss: 0.0712 - val_acc: 0.2936\n",
      "Training on batch 2500000 to 2550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0758 - acc: 0.2923 - val_loss: 0.0736 - val_acc: 0.2927\n",
      "Training on batch 2550000 to 2600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 382us/step - loss: 0.0739 - acc: 0.2928 - val_loss: 0.0715 - val_acc: 0.2928\n",
      "Training on batch 2600000 to 2650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0739 - acc: 0.2925 - val_loss: 0.0721 - val_acc: 0.2917\n",
      "Training on batch 2650000 to 2700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.0732 - acc: 0.2934 - val_loss: 0.0691 - val_acc: 0.2946\n",
      "Training on batch 2700000 to 2750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0723 - acc: 0.2929 - val_loss: 0.0727 - val_acc: 0.2921\n",
      "Training on batch 2750000 to 2800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0712 - acc: 0.2927 - val_loss: 0.0757 - val_acc: 0.2918\n",
      "Training on batch 2800000 to 2850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0743 - acc: 0.2927 - val_loss: 0.0690 - val_acc: 0.2947\n",
      "Training on batch 2850000 to 2900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0704 - acc: 0.2936 - val_loss: 0.0719 - val_acc: 0.2942\n",
      "Training on batch 2900000 to 2950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0706 - acc: 0.2931 - val_loss: 0.0716 - val_acc: 0.2915\n",
      "Training on batch 2950000 to 3000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0699 - acc: 0.2931 - val_loss: 0.0702 - val_acc: 0.2924\n",
      "Training on batch 3000000 to 3050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0709 - acc: 0.2932 - val_loss: 0.0714 - val_acc: 0.2940\n",
      "Training on batch 3050000 to 3100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0702 - acc: 0.2938 - val_loss: 0.0695 - val_acc: 0.2936\n",
      "Training on batch 3100000 to 3150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0729 - acc: 0.2930 - val_loss: 0.0716 - val_acc: 0.2955\n",
      "Training on batch 3150000 to 3200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0682 - acc: 0.2945 - val_loss: 0.0659 - val_acc: 0.2936\n",
      "Training on batch 3200000 to 3250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0679 - acc: 0.2940 - val_loss: 0.0668 - val_acc: 0.2954\n",
      "Training on batch 3250000 to 3300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0686 - acc: 0.2935 - val_loss: 0.0732 - val_acc: 0.2944\n",
      "Training on batch 3300000 to 3350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0680 - acc: 0.2946 - val_loss: 0.0690 - val_acc: 0.2943\n",
      "Training on batch 3350000 to 3400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 398us/step - loss: 0.0692 - acc: 0.2942 - val_loss: 0.0675 - val_acc: 0.2950\n",
      "Training on batch 3400000 to 3450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0685 - acc: 0.2938 - val_loss: 0.0667 - val_acc: 0.2920\n",
      "Training on batch 3450000 to 3500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0664 - acc: 0.2951 - val_loss: 0.0704 - val_acc: 0.2948\n",
      "Training on batch 3500000 to 3550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0660 - acc: 0.2944 - val_loss: 0.0688 - val_acc: 0.2928\n",
      "Training on batch 3550000 to 3600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0662 - acc: 0.2941 - val_loss: 0.0657 - val_acc: 0.2964\n",
      "Training on batch 3600000 to 3650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0665 - acc: 0.2930 - val_loss: 0.0693 - val_acc: 0.2920\n",
      "Training on batch 3650000 to 3700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0670 - acc: 0.2940 - val_loss: 0.0634 - val_acc: 0.2966\n",
      "Training on batch 3700000 to 3750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0657 - acc: 0.2937 - val_loss: 0.0653 - val_acc: 0.2930\n",
      "Training on batch 3750000 to 3800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0648 - acc: 0.2953 - val_loss: 0.0635 - val_acc: 0.2958\n",
      "Training on batch 3800000 to 3850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0646 - acc: 0.2939 - val_loss: 0.0690 - val_acc: 0.2933\n",
      "Training on batch 3850000 to 3900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0666 - acc: 0.2947 - val_loss: 0.0686 - val_acc: 0.2945\n",
      "Training on batch 3900000 to 3950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0643 - acc: 0.2945 - val_loss: 0.0661 - val_acc: 0.2969\n",
      "Training on batch 3950000 to 4000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0643 - acc: 0.2947 - val_loss: 0.0630 - val_acc: 0.2943\n",
      "Training on batch 4000000 to 4050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0663 - acc: 0.2959 - val_loss: 0.0640 - val_acc: 0.2944\n",
      "Training on batch 4050000 to 4100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0628 - acc: 0.2951 - val_loss: 0.0642 - val_acc: 0.2961\n",
      "Training on batch 4100000 to 4150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0645 - acc: 0.2949 - val_loss: 0.0620 - val_acc: 0.2945\n",
      "Training on batch 4150000 to 4200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0622 - acc: 0.2952 - val_loss: 0.0621 - val_acc: 0.2946\n",
      "Training on batch 4200000 to 4250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0613 - acc: 0.2962 - val_loss: 0.0627 - val_acc: 0.2959\n",
      "Training on batch 4250000 to 4300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0645 - acc: 0.2959 - val_loss: 0.0665 - val_acc: 0.2967\n",
      "Training on batch 4300000 to 4350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0639 - acc: 0.2947 - val_loss: 0.0610 - val_acc: 0.2973\n",
      "Training on batch 4350000 to 4400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0629 - acc: 0.2959 - val_loss: 0.0653 - val_acc: 0.2957\n",
      "Training on batch 4400000 to 4450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0640 - acc: 0.2963 - val_loss: 0.0620 - val_acc: 0.2974\n",
      "Training on batch 4450000 to 4500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0620 - acc: 0.2952 - val_loss: 0.0604 - val_acc: 0.2961\n",
      "Training on batch 4500000 to 4550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0647 - acc: 0.2956 - val_loss: 0.0651 - val_acc: 0.2962\n",
      "Training on batch 4550000 to 4600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0606 - acc: 0.2966 - val_loss: 0.0615 - val_acc: 0.2947\n",
      "Training on batch 4600000 to 4650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0631 - acc: 0.2955 - val_loss: 0.0609 - val_acc: 0.2957\n",
      "Training on batch 4650000 to 4700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0631 - acc: 0.2951 - val_loss: 0.0632 - val_acc: 0.2947\n",
      "Training on batch 4700000 to 4750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0631 - acc: 0.2953 - val_loss: 0.0599 - val_acc: 0.2947\n",
      "Training on batch 4750000 to 4800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0604 - acc: 0.2965 - val_loss: 0.0652 - val_acc: 0.2976\n",
      "Training on batch 4800000 to 4850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0624 - acc: 0.2943 - val_loss: 0.0654 - val_acc: 0.2961\n",
      "Training on batch 4850000 to 4900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0634 - acc: 0.2956 - val_loss: 0.0608 - val_acc: 0.2954\n",
      "Training on batch 4900000 to 4950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0619 - acc: 0.2969 - val_loss: 0.0597 - val_acc: 0.2938\n",
      "Training on batch 4950000 to 5000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0625 - acc: 0.2959 - val_loss: 0.0627 - val_acc: 0.2938\n",
      "Training on batch 5000000 to 5001036 of 5001036\n",
      "Train on 1036 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "1036/1036 [==============================] - 0s 467us/step - loss: 0.0579 - acc: 0.2885 - val_loss: 0.0420 - val_acc: 0.2942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c6c627c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_s2s.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training on batch 4950000 to 5000000 of 5001036\n",
    "Train on 50000 samples, validate on 12500 samples\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 20s 392us/step - loss: 0.0625 - acc: 0.2959 - val_loss: 0.0627 - val_acc: 0.2938\n",
    "```\n",
    "\n",
    "Now we need to look at what this is generating!\n",
    "\n",
    "It is noted that the accuracy is only 30% - it would be interested to see how this is manifesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model:\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "DecoderInputs (InputLayer)      (None, None, 29)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncodedState (InputLayer)       (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (GRU)                   [(None, None, 100),  39000       DecoderInputs[0][0]              \n",
      "                                                                 EncodedState[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "VocabProjection (Dense)         (None, None, 29)     2929        Decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,929\n",
      "Trainable params: 41,929\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "char_s2s.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 50000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0599 - acc: 0.2965 - val_loss: 0.0595 - val_acc: 0.2963\n",
      "Training on batch 50000 to 100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0610 - acc: 0.2957 - val_loss: 0.0595 - val_acc: 0.2956\n",
      "Training on batch 100000 to 150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0630 - acc: 0.2945 - val_loss: 0.0650 - val_acc: 0.2967\n",
      "Training on batch 150000 to 200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0632 - acc: 0.2965 - val_loss: 0.0595 - val_acc: 0.2960\n",
      "Training on batch 200000 to 250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0608 - acc: 0.2958 - val_loss: 0.0609 - val_acc: 0.2961\n",
      "Training on batch 250000 to 300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0602 - acc: 0.2964 - val_loss: 0.0567 - val_acc: 0.2958\n",
      "Training on batch 300000 to 350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0601 - acc: 0.2968 - val_loss: 0.0600 - val_acc: 0.2997\n",
      "Training on batch 350000 to 400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0602 - acc: 0.2954 - val_loss: 0.0606 - val_acc: 0.2989\n",
      "Training on batch 400000 to 450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0608 - acc: 0.2977 - val_loss: 0.0597 - val_acc: 0.2949\n",
      "Training on batch 450000 to 500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0604 - acc: 0.2974 - val_loss: 0.0592 - val_acc: 0.2959\n",
      "Training on batch 500000 to 550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0609 - acc: 0.2949 - val_loss: 0.0631 - val_acc: 0.2976\n",
      "Training on batch 550000 to 600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0613 - acc: 0.2952 - val_loss: 0.0592 - val_acc: 0.2956\n",
      "Training on batch 600000 to 650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0598 - acc: 0.2961 - val_loss: 0.0608 - val_acc: 0.2929\n",
      "Training on batch 650000 to 700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0604 - acc: 0.2975 - val_loss: 0.0581 - val_acc: 0.2981\n",
      "Training on batch 700000 to 750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0596 - acc: 0.2976 - val_loss: 0.0595 - val_acc: 0.2984\n",
      "Training on batch 750000 to 800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0583 - acc: 0.2950 - val_loss: 0.0585 - val_acc: 0.2980\n",
      "Training on batch 800000 to 850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0608 - acc: 0.2963 - val_loss: 0.0629 - val_acc: 0.2967\n",
      "Training on batch 850000 to 900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0588 - acc: 0.2960 - val_loss: 0.0601 - val_acc: 0.2955\n",
      "Training on batch 900000 to 950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0583 - acc: 0.2961 - val_loss: 0.0584 - val_acc: 0.2964\n",
      "Training on batch 950000 to 1000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0587 - acc: 0.2956 - val_loss: 0.0603 - val_acc: 0.2956\n",
      "Training on batch 1000000 to 1050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0591 - acc: 0.2953 - val_loss: 0.0556 - val_acc: 0.2963\n",
      "Training on batch 1050000 to 1100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0594 - acc: 0.2946 - val_loss: 0.0555 - val_acc: 0.2976\n",
      "Training on batch 1100000 to 1150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0568 - acc: 0.2965 - val_loss: 0.0585 - val_acc: 0.2940\n",
      "Training on batch 1150000 to 1200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0598 - acc: 0.2970 - val_loss: 0.0598 - val_acc: 0.2959\n",
      "Training on batch 1200000 to 1250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0591 - acc: 0.2971 - val_loss: 0.0613 - val_acc: 0.2966\n",
      "Training on batch 1250000 to 1300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0575 - acc: 0.2969 - val_loss: 0.0570 - val_acc: 0.2972\n",
      "Training on batch 1300000 to 1350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0575 - acc: 0.2974 - val_loss: 0.0543 - val_acc: 0.2968\n",
      "Training on batch 1350000 to 1400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0591 - acc: 0.2955 - val_loss: 0.0607 - val_acc: 0.2975\n",
      "Training on batch 1400000 to 1450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0576 - acc: 0.2961 - val_loss: 0.0589 - val_acc: 0.2967\n",
      "Training on batch 1450000 to 1500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0581 - acc: 0.2966 - val_loss: 0.0597 - val_acc: 0.2953\n",
      "Training on batch 1500000 to 1550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0589 - acc: 0.2956 - val_loss: 0.0585 - val_acc: 0.2971\n",
      "Training on batch 1550000 to 1600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0581 - acc: 0.2978 - val_loss: 0.0577 - val_acc: 0.2955\n",
      "Training on batch 1600000 to 1650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0565 - acc: 0.2959 - val_loss: 0.0567 - val_acc: 0.2992\n",
      "Training on batch 1650000 to 1700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0591 - acc: 0.2969 - val_loss: 0.0529 - val_acc: 0.2975\n",
      "Training on batch 1700000 to 1750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0575 - acc: 0.2972 - val_loss: 0.0559 - val_acc: 0.2972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1750000 to 1800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.0588 - acc: 0.2964 - val_loss: 0.0599 - val_acc: 0.2971\n",
      "Training on batch 1800000 to 1850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0577 - acc: 0.2974 - val_loss: 0.0571 - val_acc: 0.2971\n",
      "Training on batch 1850000 to 1900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0577 - acc: 0.2963 - val_loss: 0.0571 - val_acc: 0.2984\n",
      "Training on batch 1900000 to 1950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0554 - acc: 0.2973 - val_loss: 0.0561 - val_acc: 0.2978\n",
      "Training on batch 1950000 to 2000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0568 - acc: 0.2976 - val_loss: 0.0630 - val_acc: 0.2990\n",
      "Training on batch 2000000 to 2050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 382us/step - loss: 0.0565 - acc: 0.2966 - val_loss: 0.0584 - val_acc: 0.2981\n",
      "Training on batch 2050000 to 2100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.0559 - acc: 0.2968 - val_loss: 0.0599 - val_acc: 0.2958\n",
      "Training on batch 2100000 to 2150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0569 - acc: 0.2971 - val_loss: 0.0594 - val_acc: 0.2985\n",
      "Training on batch 2150000 to 2200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0583 - acc: 0.2972 - val_loss: 0.0563 - val_acc: 0.2977\n",
      "Training on batch 2200000 to 2250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0580 - acc: 0.2971 - val_loss: 0.0557 - val_acc: 0.2980\n",
      "Training on batch 2250000 to 2300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0555 - acc: 0.2971 - val_loss: 0.0604 - val_acc: 0.2968\n",
      "Training on batch 2300000 to 2350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0564 - acc: 0.2967 - val_loss: 0.0604 - val_acc: 0.2956\n",
      "Training on batch 2350000 to 2400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0579 - acc: 0.2971 - val_loss: 0.0559 - val_acc: 0.2947\n",
      "Training on batch 2400000 to 2450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0565 - acc: 0.2978 - val_loss: 0.0558 - val_acc: 0.2978\n",
      "Training on batch 2450000 to 2500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0555 - acc: 0.2973 - val_loss: 0.0531 - val_acc: 0.2983\n",
      "Training on batch 2500000 to 2550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0578 - acc: 0.2972 - val_loss: 0.0560 - val_acc: 0.2974\n",
      "Training on batch 2550000 to 2600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0563 - acc: 0.2975 - val_loss: 0.0551 - val_acc: 0.2971\n",
      "Training on batch 2600000 to 2650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0563 - acc: 0.2972 - val_loss: 0.0557 - val_acc: 0.2959\n",
      "Training on batch 2650000 to 2700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0574 - acc: 0.2978 - val_loss: 0.0533 - val_acc: 0.2988\n",
      "Training on batch 2700000 to 2750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0579 - acc: 0.2967 - val_loss: 0.0582 - val_acc: 0.2960\n",
      "Training on batch 2750000 to 2800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0565 - acc: 0.2967 - val_loss: 0.0618 - val_acc: 0.2957\n",
      "Training on batch 2800000 to 2850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0586 - acc: 0.2969 - val_loss: 0.0536 - val_acc: 0.2989\n",
      "Training on batch 2850000 to 2900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0546 - acc: 0.2979 - val_loss: 0.0573 - val_acc: 0.2984\n",
      "Training on batch 2900000 to 2950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0553 - acc: 0.2973 - val_loss: 0.0564 - val_acc: 0.2952\n",
      "Training on batch 2950000 to 3000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0566 - acc: 0.2966 - val_loss: 0.0576 - val_acc: 0.2961\n",
      "Training on batch 3000000 to 3050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0566 - acc: 0.2970 - val_loss: 0.0583 - val_acc: 0.2974\n",
      "Training on batch 3050000 to 3100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0552 - acc: 0.2978 - val_loss: 0.0552 - val_acc: 0.2974\n",
      "Training on batch 3100000 to 3150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0575 - acc: 0.2971 - val_loss: 0.0589 - val_acc: 0.2987\n",
      "Training on batch 3150000 to 3200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 378us/step - loss: 0.0563 - acc: 0.2976 - val_loss: 0.0547 - val_acc: 0.2964\n",
      "Training on batch 3200000 to 3250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0554 - acc: 0.2973 - val_loss: 0.0545 - val_acc: 0.2984\n",
      "Training on batch 3250000 to 3300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 378us/step - loss: 0.0554 - acc: 0.2970 - val_loss: 0.0585 - val_acc: 0.2984\n",
      "Training on batch 3300000 to 3350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 377us/step - loss: 0.0542 - acc: 0.2982 - val_loss: 0.0585 - val_acc: 0.2969\n",
      "Training on batch 3350000 to 3400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0566 - acc: 0.2975 - val_loss: 0.0551 - val_acc: 0.2982\n",
      "Training on batch 3400000 to 3450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 376us/step - loss: 0.0566 - acc: 0.2970 - val_loss: 0.0553 - val_acc: 0.2951\n",
      "Training on batch 3450000 to 3500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 377us/step - loss: 0.0546 - acc: 0.2983 - val_loss: 0.0582 - val_acc: 0.2980\n",
      "Training on batch 3500000 to 3550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0539 - acc: 0.2977 - val_loss: 0.0576 - val_acc: 0.2960\n",
      "Training on batch 3550000 to 3600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0551 - acc: 0.2970 - val_loss: 0.0555 - val_acc: 0.2992\n",
      "Training on batch 3600000 to 3650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0548 - acc: 0.2960 - val_loss: 0.0589 - val_acc: 0.2950\n",
      "Training on batch 3650000 to 3700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0565 - acc: 0.2968 - val_loss: 0.0519 - val_acc: 0.2998\n",
      "Training on batch 3700000 to 3750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0546 - acc: 0.2966 - val_loss: 0.0551 - val_acc: 0.2959\n",
      "Training on batch 3750000 to 3800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0541 - acc: 0.2981 - val_loss: 0.0537 - val_acc: 0.2981\n",
      "Training on batch 3800000 to 3850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0551 - acc: 0.2963 - val_loss: 0.0569 - val_acc: 0.2964\n",
      "Training on batch 3850000 to 3900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0551 - acc: 0.2978 - val_loss: 0.0572 - val_acc: 0.2972\n",
      "Training on batch 3900000 to 3950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0532 - acc: 0.2974 - val_loss: 0.0547 - val_acc: 0.2998\n",
      "Training on batch 3950000 to 4000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0531 - acc: 0.2977 - val_loss: 0.0519 - val_acc: 0.2970\n",
      "Training on batch 4000000 to 4050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0563 - acc: 0.2984 - val_loss: 0.0531 - val_acc: 0.2974\n",
      "Training on batch 4050000 to 4100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0540 - acc: 0.2975 - val_loss: 0.0535 - val_acc: 0.2990\n",
      "Training on batch 4100000 to 4150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0548 - acc: 0.2976 - val_loss: 0.0533 - val_acc: 0.2968\n",
      "Training on batch 4150000 to 4200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0541 - acc: 0.2973 - val_loss: 0.0552 - val_acc: 0.2962\n",
      "Training on batch 4200000 to 4250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0539 - acc: 0.2981 - val_loss: 0.0554 - val_acc: 0.2982\n",
      "Training on batch 4250000 to 4300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0559 - acc: 0.2981 - val_loss: 0.0594 - val_acc: 0.2987\n",
      "Training on batch 4300000 to 4350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0550 - acc: 0.2972 - val_loss: 0.0518 - val_acc: 0.2998\n",
      "Training on batch 4350000 to 4400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0528 - acc: 0.2987 - val_loss: 0.0547 - val_acc: 0.2988\n",
      "Training on batch 4400000 to 4450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0545 - acc: 0.2988 - val_loss: 0.0531 - val_acc: 0.2997\n",
      "Training on batch 4450000 to 4500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0538 - acc: 0.2973 - val_loss: 0.0540 - val_acc: 0.2978\n",
      "Training on batch 4500000 to 4550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0575 - acc: 0.2977 - val_loss: 0.0554 - val_acc: 0.2990\n",
      "Training on batch 4550000 to 4600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0518 - acc: 0.2990 - val_loss: 0.0539 - val_acc: 0.2967\n",
      "Training on batch 4600000 to 4650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0539 - acc: 0.2980 - val_loss: 0.0532 - val_acc: 0.2978\n",
      "Training on batch 4650000 to 4700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.0548 - acc: 0.2973 - val_loss: 0.0545 - val_acc: 0.2970\n",
      "Training on batch 4700000 to 4750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0552 - acc: 0.2974 - val_loss: 0.0523 - val_acc: 0.2965\n",
      "Training on batch 4750000 to 4800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0526 - acc: 0.2985 - val_loss: 0.0566 - val_acc: 0.2995\n",
      "Training on batch 4800000 to 4850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0545 - acc: 0.2964 - val_loss: 0.0607 - val_acc: 0.2974\n",
      "Training on batch 4850000 to 4900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 21s 425us/step - loss: 0.0556 - acc: 0.2975 - val_loss: 0.0555 - val_acc: 0.2966\n",
      "Training on batch 4900000 to 4950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0552 - acc: 0.2987 - val_loss: 0.0527 - val_acc: 0.2954\n",
      "Training on batch 4950000 to 5000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0537 - acc: 0.2984 - val_loss: 0.0531 - val_acc: 0.2967\n",
      "Training on batch 5000000 to 5001036 of 5001036\n",
      "Train on 1036 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "1036/1036 [==============================] - 1s 533us/step - loss: 0.0512 - acc: 0.2910 - val_loss: 0.0382 - val_acc: 0.2958\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c6c626cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_s2s.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training on batch 4900000 to 4950000 of 5001036\n",
    "Train on 50000 samples, validate on 12500 samples\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 19s 390us/step - loss: 0.0552 - acc: 0.2987 - val_loss: 0.0527 - val_acc: 0.2954\n",
    "Training on batch 4950000 to 5000000 of 5001036\n",
    "Train on 50000 samples, validate on 12500 samples\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 19s 386us/step - loss: 0.0537 - acc: 0.2984 - val_loss: 0.0531 - val_acc: 0.2967\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predicted word is: c o n t e r r e s \n",
      "Actual word is: e v e n t s  \n",
      "---\n",
      "Predicted word is: s a i d \n",
      "Actual word is: s a i d  \n",
      "---\n",
      "Predicted word is: w h e r e i n \n",
      "Actual word is: w h e r e i n  \n",
      "---\n",
      "Predicted word is: d a t a \n",
      "Actual word is: d a t a  \n",
      "---\n",
      "Predicted word is: p r o g r a m m i n g \n",
      "Actual word is: p r o g r a m m i n g  \n",
      "---\n",
      "Predicted word is: r e f e r e n c e \n",
      "Actual word is: r e f e r e n c e  \n",
      "---\n",
      "Predicted word is: m o s t a l r s \n",
      "Actual word is: s e t t i n g s  \n",
      "---\n",
      "Predicted word is: w h i c h \n",
      "Actual word is: w h i c h  \n",
      "---\n",
      "Predicted word is: o f \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: b u n t l u e d \n",
      "Actual word is: m e c h a n i c  \n",
      "---\n",
      "Predicted word is: b a s e d \n",
      "Actual word is: b a s e d  \n",
      "---\n",
      "Predicted word is: a n d \n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: r a t i n g \n",
      "Actual word is: r a t i n g  \n",
      "---\n",
      "Predicted word is: i n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: c l i e n t \n",
      "Actual word is: c l i e n t  \n",
      "---\n",
      "Predicted word is: d e f i n i n g \n",
      "Actual word is: d e f i n i n g  \n",
      "---\n",
      "Predicted word is: l o g i c a l \n",
      "Actual word is: l o g i c a l  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: h o u s i n g \n",
      "Actual word is: h o u s i n g  \n",
      "---\n",
      "Predicted word is: p a r t i c u l a r \n",
      "Actual word is: p a r t i c u l a r  \n",
      "---\n",
      "Predicted word is: a t \n",
      "Actual word is: a t  \n",
      "---\n",
      "Predicted word is: f r o m \n",
      "Actual word is: f r o m  \n",
      "---\n",
      "Predicted word is: c o m p r i s i n g \n",
      "Actual word is: c o m p r i s i n g  \n",
      "---\n",
      "Predicted word is: c \n",
      "Actual word is: c  \n",
      "---\n",
      "Predicted word is: o n e \n",
      "Actual word is: o n e  \n",
      "---\n",
      "Predicted word is: t o \n",
      "Actual word is: t o  \n",
      "---\n",
      "Predicted word is: n o d e s \n",
      "Actual word is: n o d e s  \n",
      "---\n",
      "Predicted word is: d r i v e \n",
      "Actual word is: d r i v e  \n",
      "---\n",
      "Predicted word is: i n d i c a t i o n \n",
      "Actual word is: i n d i c a t i o n  \n",
      "---\n",
      "Predicted word is: b o v e s \n",
      "Actual word is: h a n d  \n",
      "---\n",
      "Predicted word is: a n d \n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: i n p u t \n",
      "Actual word is: i n p u t  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: l e e r \n",
      "Actual word is: a i r s p a c e  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: a n d \n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: c o m p o n e n t s \n",
      "Actual word is: c o m p o n e n t s  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: e x c e e d s \n",
      "Actual word is: e x c e e d s  \n",
      "---\n",
      "Predicted word is: p r o g r a m \n",
      "Actual word is: p r o g r a m  \n",
      "---\n",
      "Predicted word is: a t \n",
      "Actual word is: a t  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: i n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: e d a i t e \n",
      "Actual word is: n e g a t i v e  \n",
      "---\n",
      "Predicted word is: o r \n",
      "Actual word is: o r  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: d e f i n e s \n",
      "Actual word is: d e f i n e s  \n",
      "---\n",
      "Predicted word is: s e c o n d \n",
      "Actual word is: s e c o n d  \n",
      "---\n",
      "Predicted word is: l e a s t \n",
      "Actual word is: l e a s t  \n",
      "---\n",
      "Predicted word is: s i z e \n",
      "Actual word is: s i z e  \n",
      "---\n",
      "Predicted word is: s g r e e s \n",
      "Actual word is: f i l m s  \n",
      "---\n",
      "Predicted word is: r e s p o n s e s \n",
      "Actual word is: r e s p o n s e s  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: h a n t o r e a b l e \n",
      "Actual word is: p o r t l e t  \n",
      "---\n",
      "Predicted word is: t o \n",
      "Actual word is: t o  \n",
      "---\n",
      "Predicted word is: f o r \n",
      "Actual word is: f o r  \n",
      "---\n",
      "Predicted word is: u s i n g \n",
      "Actual word is: u s i n g  \n",
      "---\n",
      "Predicted word is: i n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: p r o g r a m \n",
      "Actual word is: p r o g r a m  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: o f \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: a l i n g i n g \n",
      "Actual word is: o p e r a t i v e l y  \n",
      "---\n",
      "Predicted word is: r e s t r e e s \n",
      "Actual word is: p r e v i o u s l y  \n",
      "---\n",
      "Predicted word is: o n e \n",
      "Actual word is: o n e  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: r e p r e s e n t s \n",
      "Actual word is: r e p r e s e n t s  \n",
      "---\n",
      "Predicted word is: e v e n t \n",
      "Actual word is: e v e n t  \n",
      "---\n",
      "Predicted word is: c o n f i g u r e d \n",
      "Actual word is: c o n f i g u r e d  \n",
      "---\n",
      "Predicted word is: b l o a d e d \n",
      "Actual word is: l o a d e r  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: i n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: d i t e \n",
      "Actual word is: t r a c k i n g  \n",
      "---\n",
      "Predicted word is: i m a g e \n",
      "Actual word is: i m a g e  \n",
      "---\n",
      "Predicted word is: p r i n t i n g \n",
      "Actual word is: p r i n t i n g  \n",
      "---\n",
      "Predicted word is: a \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: i n t o \n",
      "Actual word is: i n t o  \n",
      "---\n",
      "Predicted word is: e n d \n",
      "Actual word is: e n d  \n",
      "---\n",
      "Predicted word is: a c t i v i t y \n",
      "Actual word is: a c t i v i t y  \n",
      "---\n",
      "Predicted word is: t r a n s a c t i o n s \n",
      "Actual word is: t r a n s a c t i o n s  \n",
      "---\n",
      "Predicted word is: t y p e s \n",
      "Actual word is: t y p e s  \n",
      "---\n",
      "Predicted word is: s e t \n",
      "Actual word is: s e t  \n",
      "---\n",
      "Predicted word is: a c c e s s \n",
      "Actual word is: a c c e s s  \n",
      "---\n",
      "Predicted word is: b t a u g h t e \n",
      "Actual word is: b o a r d  \n",
      "---\n",
      "Predicted word is: s t o r a g e \n",
      "Actual word is: s t o r a g e  \n",
      "---\n",
      "Predicted word is: t h e \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: e a c h \n",
      "Actual word is: e a c h  \n",
      "---\n",
      "Predicted word is: p o i n t \n",
      "Actual word is: p o i n t  \n",
      "---\n",
      "Predicted word is: h e w t i n g \n",
      "Actual word is: h o l d i n g  \n",
      "---\n",
      "Predicted word is: f u n d l e a t s \n",
      "Actual word is: e s s e n t i a l l y  \n",
      "---\n",
      "Predicted word is: f r a w e s \n",
      "Actual word is: f r a m e s  \n",
      "---\n",
      "Predicted word is: c o m m u n i c a t i o n \n",
      "Actual word is: c o m m u n i c a t i o n  \n",
      "---\n",
      "Predicted word is: s e c o n d \n",
      "Actual word is: s e c o n d  \n",
      "---\n",
      "Predicted word is: i n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: s e t \n",
      "Actual word is: s e t  \n",
      "---\n",
      "Predicted word is: s a i d \n",
      "Actual word is: s a i d  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "char_s2s.example_output(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image\n",
    "Image(model_to_dot(self.model, show_shapes=True).create_png(prog='dot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for improving performance:\n",
    "* Adding more hidden layers.\n",
    "* Bidirectional iterations.\n",
    "* Projecting embedding to a greater dimensionality then applying a larger GRU.\n",
    "* Beam search on decoding.\n",
    "* Bidirectional GRU - https://keras.io/layers/wrappers/#bidirectional - work forwards and backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "new_latent_dim = 512\n",
    "\n",
    "class CharS2S_ext(CharS2S):\n",
    "    def _build_model(self):\n",
    "        \"\"\" Build the model. \"\"\"\n",
    "        print(\"Building model\")\n",
    "        encoded_state = Input(shape=(self.latent_dim,), name=\"EncodedState\")\n",
    "        projected_state = Dense(new_latent_dim, activation='relu', name=\"ProjectState\")(encoded_state)\n",
    "        \n",
    "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens), name=\"DecoderInputs\")\n",
    "        \n",
    "        decoder_gru = GRU(new_latent_dim, return_sequences=True, return_state=True, name=\"Decoder\")\n",
    "        decoder_outputs, decoder_state = decoder_gru(decoder_inputs, initial_state=projected_state)\n",
    "        \n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax', name=\"VocabProjection\")\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        decoder_state = Dense(self.latent_dim, name=\"ProjectDownState\")(decoder_state)\n",
    "        \n",
    "        self.model = Model([encoded_state, decoder_inputs], decoder_outputs)\n",
    "\n",
    "        # We also need an inference model\n",
    "        self.infdec = Model(inputs=[encoded_state, decoder_inputs], outputs=[decoder_outputs, decoder_state])\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "No existing weights found\n",
      "Generating training and test data\n"
     ]
    }
   ],
   "source": [
    "char_s2s_ext = CharS2S_ext(\n",
    "    encoder_states=input_data,\n",
    "    decoder_seqs=output_data,\n",
    "    decoder_seq_length=20,\n",
    "    latent_dim=100,\n",
    "    weights_file=\"charstate2seq_ext.hdf5\",\n",
    "    training_set_size=50000,  # Due to memory we need to train in sets\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 50000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 0.3926 - acc: 0.2003 - val_loss: 0.2050 - val_acc: 0.2536\n",
      "Training on batch 50000 to 100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 0.1542 - acc: 0.2682 - val_loss: 0.1181 - val_acc: 0.2782\n",
      "Training on batch 100000 to 150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 0.1019 - acc: 0.2823 - val_loss: 0.0917 - val_acc: 0.2952\n",
      "Training on batch 150000 to 200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0802 - acc: 0.2899 - val_loss: 0.0678 - val_acc: 0.2917\n",
      "Training on batch 200000 to 250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0610 - acc: 0.2947 - val_loss: 0.0557 - val_acc: 0.2958\n",
      "Training on batch 250000 to 300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0521 - acc: 0.2970 - val_loss: 0.0470 - val_acc: 0.2967\n",
      "Training on batch 300000 to 350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0456 - acc: 0.2991 - val_loss: 0.0421 - val_acc: 0.3033\n",
      "Training on batch 350000 to 400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0403 - acc: 0.2991 - val_loss: 0.0383 - val_acc: 0.3034\n",
      "Training on batch 400000 to 450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 0.0361 - acc: 0.3028 - val_loss: 0.0335 - val_acc: 0.3016\n",
      "Training on batch 450000 to 500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0332 - acc: 0.3033 - val_loss: 0.0325 - val_acc: 0.3016\n",
      "Training on batch 500000 to 550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0304 - acc: 0.3020 - val_loss: 0.0299 - val_acc: 0.3050\n",
      "Training on batch 550000 to 600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0290 - acc: 0.3023 - val_loss: 0.0282 - val_acc: 0.3026\n",
      "Training on batch 600000 to 650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0263 - acc: 0.3038 - val_loss: 0.0255 - val_acc: 0.3009\n",
      "Training on batch 650000 to 700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 0.0259 - acc: 0.3053 - val_loss: 0.0231 - val_acc: 0.3058\n",
      "Training on batch 700000 to 750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0242 - acc: 0.3056 - val_loss: 0.0221 - val_acc: 0.3067\n",
      "Training on batch 750000 to 800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0224 - acc: 0.3032 - val_loss: 0.0201 - val_acc: 0.3076\n",
      "Training on batch 800000 to 850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0219 - acc: 0.3055 - val_loss: 0.0212 - val_acc: 0.3066\n",
      "Training on batch 850000 to 900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.0187 - acc: 0.3055 - val_loss: 0.0201 - val_acc: 0.3049\n",
      "Training on batch 900000 to 950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0198 - acc: 0.3047 - val_loss: 0.0200 - val_acc: 0.3050\n",
      "Training on batch 950000 to 1000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0182 - acc: 0.3047 - val_loss: 0.0187 - val_acc: 0.3052\n",
      "Training on batch 1000000 to 1050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0187 - acc: 0.3046 - val_loss: 0.0162 - val_acc: 0.3052\n",
      "Training on batch 1050000 to 1100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0185 - acc: 0.3039 - val_loss: 0.0162 - val_acc: 0.3064\n",
      "Training on batch 1100000 to 1150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0162 - acc: 0.3059 - val_loss: 0.0172 - val_acc: 0.3038\n",
      "Training on batch 1150000 to 1200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0169 - acc: 0.3069 - val_loss: 0.0169 - val_acc: 0.3061\n",
      "Training on batch 1200000 to 1250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0158 - acc: 0.3071 - val_loss: 0.0162 - val_acc: 0.3067\n",
      "Training on batch 1250000 to 1300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0154 - acc: 0.3066 - val_loss: 0.0149 - val_acc: 0.3068\n",
      "Training on batch 1300000 to 1350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0155 - acc: 0.3071 - val_loss: 0.0163 - val_acc: 0.3052\n",
      "Training on batch 1350000 to 1400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0158 - acc: 0.3055 - val_loss: 0.0146 - val_acc: 0.3084\n",
      "Training on batch 1400000 to 1450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0143 - acc: 0.3062 - val_loss: 0.0144 - val_acc: 0.3072\n",
      "Training on batch 1450000 to 1500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0139 - acc: 0.3069 - val_loss: 0.0165 - val_acc: 0.3051\n",
      "Training on batch 1500000 to 1550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0141 - acc: 0.3059 - val_loss: 0.0140 - val_acc: 0.3076\n",
      "Training on batch 1550000 to 1600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0142 - acc: 0.3080 - val_loss: 0.0135 - val_acc: 0.3059\n",
      "Training on batch 1600000 to 1650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0135 - acc: 0.3060 - val_loss: 0.0139 - val_acc: 0.3090\n",
      "Training on batch 1650000 to 1700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0138 - acc: 0.3075 - val_loss: 0.0122 - val_acc: 0.3068\n",
      "Training on batch 1700000 to 1750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0122 - acc: 0.3078 - val_loss: 0.0124 - val_acc: 0.3075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1750000 to 1800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.0137 - acc: 0.3068 - val_loss: 0.0139 - val_acc: 0.3077\n",
      "Training on batch 1800000 to 1850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0130 - acc: 0.3078 - val_loss: 0.0126 - val_acc: 0.3077\n",
      "Training on batch 1850000 to 1900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0132 - acc: 0.3068 - val_loss: 0.0124 - val_acc: 0.3087\n",
      "Training on batch 1900000 to 1950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0124 - acc: 0.3072 - val_loss: 0.0122 - val_acc: 0.3082\n",
      "Training on batch 1950000 to 2000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.0116 - acc: 0.3081 - val_loss: 0.0133 - val_acc: 0.3107\n",
      "Training on batch 2000000 to 2050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0119 - acc: 0.3072 - val_loss: 0.0115 - val_acc: 0.3091\n",
      "Training on batch 2050000 to 2100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0113 - acc: 0.3072 - val_loss: 0.0126 - val_acc: 0.3069\n",
      "Training on batch 2100000 to 2150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0119 - acc: 0.3076 - val_loss: 0.0124 - val_acc: 0.3094\n",
      "Training on batch 2150000 to 2200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0121 - acc: 0.3079 - val_loss: 0.0121 - val_acc: 0.3079\n",
      "Training on batch 2200000 to 2250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0130 - acc: 0.3076 - val_loss: 0.0127 - val_acc: 0.3080\n",
      "Training on batch 2250000 to 2300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0116 - acc: 0.3074 - val_loss: 0.0120 - val_acc: 0.3079\n",
      "Training on batch 2300000 to 2350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0116 - acc: 0.3071 - val_loss: 0.0113 - val_acc: 0.3071\n",
      "Training on batch 2350000 to 2400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0109 - acc: 0.3083 - val_loss: 0.0107 - val_acc: 0.3053\n",
      "Training on batch 2400000 to 2450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0109 - acc: 0.3086 - val_loss: 0.0108 - val_acc: 0.3084\n",
      "Training on batch 2450000 to 2500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0109 - acc: 0.3077 - val_loss: 0.0102 - val_acc: 0.3086\n",
      "Training on batch 2500000 to 2550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0113 - acc: 0.3081 - val_loss: 0.0113 - val_acc: 0.3080\n",
      "Training on batch 2550000 to 2600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0111 - acc: 0.3080 - val_loss: 0.0098 - val_acc: 0.3076\n",
      "Training on batch 2600000 to 2650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0107 - acc: 0.3079 - val_loss: 0.0110 - val_acc: 0.3086\n",
      "Training on batch 2650000 to 2700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0120 - acc: 0.3084 - val_loss: 0.0099 - val_acc: 0.3088\n",
      "Training on batch 2700000 to 2750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0107 - acc: 0.3077 - val_loss: 0.0112 - val_acc: 0.3071\n",
      "Training on batch 2750000 to 2800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0104 - acc: 0.3075 - val_loss: 0.0120 - val_acc: 0.3072\n",
      "Training on batch 2800000 to 2850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0113 - acc: 0.3080 - val_loss: 0.0099 - val_acc: 0.3091\n",
      "Training on batch 2850000 to 2900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0103 - acc: 0.3080 - val_loss: 0.0113 - val_acc: 0.3087\n",
      "Training on batch 2900000 to 2950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0105 - acc: 0.3076 - val_loss: 0.0101 - val_acc: 0.3058\n",
      "Training on batch 2950000 to 3000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0103 - acc: 0.3074 - val_loss: 0.0105 - val_acc: 0.3068\n",
      "Training on batch 3000000 to 3050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0107 - acc: 0.3078 - val_loss: 0.0116 - val_acc: 0.3084\n",
      "Training on batch 3050000 to 3100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0107 - acc: 0.3081 - val_loss: 0.0105 - val_acc: 0.3078\n",
      "Training on batch 3100000 to 3150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0101 - acc: 0.3082 - val_loss: 0.0108 - val_acc: 0.3102\n",
      "Training on batch 3150000 to 3200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0106 - acc: 0.3082 - val_loss: 0.0094 - val_acc: 0.3072\n",
      "Training on batch 3200000 to 3250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0100 - acc: 0.3078 - val_loss: 0.0109 - val_acc: 0.3087\n",
      "Training on batch 3250000 to 3300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0103 - acc: 0.3076 - val_loss: 0.0112 - val_acc: 0.3090\n",
      "Training on batch 3300000 to 3350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0098 - acc: 0.3086 - val_loss: 0.0100 - val_acc: 0.3084\n",
      "Training on batch 3350000 to 3400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0102 - acc: 0.3083 - val_loss: 0.0106 - val_acc: 0.3088\n",
      "Training on batch 3400000 to 3450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0105 - acc: 0.3078 - val_loss: 0.0096 - val_acc: 0.3055\n",
      "Training on batch 3450000 to 3500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 116s 2ms/step - loss: 0.0096 - acc: 0.3087 - val_loss: 0.0110 - val_acc: 0.3091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3500000 to 3550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 119s 2ms/step - loss: 0.0097 - acc: 0.3079 - val_loss: 0.0105 - val_acc: 0.3068\n",
      "Training on batch 3550000 to 3600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 120s 2ms/step - loss: 0.0097 - acc: 0.3077 - val_loss: 0.0097 - val_acc: 0.3096\n",
      "Training on batch 3600000 to 3650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 120s 2ms/step - loss: 0.0104 - acc: 0.3065 - val_loss: 0.0099 - val_acc: 0.3061\n",
      "Training on batch 3650000 to 3700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0100 - acc: 0.3077 - val_loss: 0.0085 - val_acc: 0.3101\n",
      "Training on batch 3700000 to 3750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0098 - acc: 0.3070 - val_loss: 0.0100 - val_acc: 0.3063\n",
      "Training on batch 3750000 to 3800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0091 - acc: 0.3088 - val_loss: 0.0090 - val_acc: 0.3088\n",
      "Training on batch 3800000 to 3850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 0.0091 - acc: 0.3070 - val_loss: 0.0099 - val_acc: 0.3076\n",
      "Training on batch 3850000 to 3900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 115s 2ms/step - loss: 0.0093 - acc: 0.3085 - val_loss: 0.0104 - val_acc: 0.3084\n",
      "Training on batch 3900000 to 3950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0102 - acc: 0.3074 - val_loss: 0.0093 - val_acc: 0.3108\n",
      "Training on batch 3950000 to 4000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0093 - acc: 0.3079 - val_loss: 0.0083 - val_acc: 0.3075\n",
      "Training on batch 4000000 to 4050000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 117s 2ms/step - loss: 0.0095 - acc: 0.3094 - val_loss: 0.0093 - val_acc: 0.3077\n",
      "Training on batch 4050000 to 4100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0088 - acc: 0.3081 - val_loss: 0.0088 - val_acc: 0.3096\n",
      "Training on batch 4100000 to 4150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0088 - acc: 0.3081 - val_loss: 0.0085 - val_acc: 0.3073\n",
      "Training on batch 4150000 to 4200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0092 - acc: 0.3079 - val_loss: 0.0097 - val_acc: 0.3071\n",
      "Training on batch 4200000 to 4250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.0093 - acc: 0.3086 - val_loss: 0.0096 - val_acc: 0.3088\n",
      "Training on batch 4250000 to 4300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0101 - acc: 0.3087 - val_loss: 0.0106 - val_acc: 0.3100\n",
      "Training on batch 4300000 to 4350000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.0084 - acc: 0.3079 - val_loss: 0.0080 - val_acc: 0.3099\n",
      "Training on batch 4350000 to 4400000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0092 - acc: 0.3087 - val_loss: 0.0110 - val_acc: 0.3090\n",
      "Training on batch 4400000 to 4450000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0103 - acc: 0.3090 - val_loss: 0.0091 - val_acc: 0.3096\n",
      "Training on batch 4450000 to 4500000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0092 - acc: 0.3077 - val_loss: 0.0079 - val_acc: 0.3086\n",
      "Training on batch 4500000 to 4550000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0092 - acc: 0.3089 - val_loss: 0.0089 - val_acc: 0.3096\n",
      "Training on batch 4550000 to 4600000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0087 - acc: 0.3088 - val_loss: 0.0100 - val_acc: 0.3069\n",
      "Training on batch 4600000 to 4650000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0091 - acc: 0.3083 - val_loss: 0.0099 - val_acc: 0.3078\n",
      "Training on batch 4650000 to 4700000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0103 - acc: 0.3076 - val_loss: 0.0105 - val_acc: 0.3073\n",
      "Training on batch 4700000 to 4750000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0100 - acc: 0.3080 - val_loss: 0.0083 - val_acc: 0.3067\n",
      "Training on batch 4750000 to 4800000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0089 - acc: 0.3087 - val_loss: 0.0101 - val_acc: 0.3104\n",
      "Training on batch 4800000 to 4850000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0097 - acc: 0.3069 - val_loss: 0.0088 - val_acc: 0.3094\n",
      "Training on batch 4850000 to 4900000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0090 - acc: 0.3085 - val_loss: 0.0095 - val_acc: 0.3077\n",
      "Training on batch 4900000 to 4950000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0086 - acc: 0.3098 - val_loss: 0.0077 - val_acc: 0.3058\n",
      "Training on batch 4950000 to 5000000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.0095 - acc: 0.3086 - val_loss: 0.0092 - val_acc: 0.3067\n",
      "Training on batch 5000000 to 5001036 of 5001036\n",
      "Train on 1036 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 0.0086 - acc: 0.3010 - val_loss: 0.0039 - val_acc: 0.3033\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e90c3f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_s2s_ext.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors cascade - one small initial error can lead the decoding off on the wrong route. Beam search would resolve this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 50000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 0.0080 - acc: 0.3087 - val_loss: 0.0077 - val_acc: 0.3084\n",
      "Training on batch 50000 to 100000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 0.0095 - acc: 0.3079 - val_loss: 0.0103 - val_acc: 0.3074\n",
      "Training on batch 100000 to 150000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0099 - acc: 0.3071 - val_loss: 0.0100 - val_acc: 0.3099\n",
      "Training on batch 150000 to 200000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 117s 2ms/step - loss: 0.0093 - acc: 0.3091 - val_loss: 0.0084 - val_acc: 0.3080\n",
      "Training on batch 200000 to 250000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 114s 2ms/step - loss: 0.0089 - acc: 0.3080 - val_loss: 0.0089 - val_acc: 0.3085\n",
      "Training on batch 250000 to 300000 of 5001036\n",
      "Train on 50000 samples, validate on 12500 samples\n",
      "Epoch 1/1\n",
      "27904/50000 [===============>..............] - ETA: 48s - loss: 0.0088 - acc: 0.3082"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-40260a89840c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchar_s2s_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playground_data/content/notebooks/title_generation/char_gen.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, reset_metrics)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for epoch {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playground_data/content/notebooks/title_generation/base_seq2seq.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m                     )\n\u001b[1;32m    128\u001b[0m                 )\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playground_data/content/notebooks/title_generation/base_seq2seq.py\u001b[0m in \u001b[0;36m_train_set\u001b[0;34m(self, training_data, validation_data)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             )\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "char_s2s_ext.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predicted word is: o c g i s i f a b i i p m p t f j n n p \n",
      "Actual word is: o n e  \n",
      "---\n",
      "Predicted word is: c u p s s s g i t e m p i n m n n g b p \n",
      "Actual word is: c o n t r o l  \n",
      "---\n",
      "Predicted word is: s p r g d i b f p n b r p i s n l p b b \n",
      "Actual word is: s u p e r v i s o r y  \n",
      "---\n",
      "Predicted word is: l i a c s p r g t n p n t k m n s p t n \n",
      "Actual word is: l o g i c  \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/playground_data/content/notebooks/title_generation/char_gen.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word is: r r i e i e f i i t n p p p n p g o g i \n",
      "Actual word is: r e s p e c t i v e  \n",
      "---\n",
      "Predicted word is: c v d e b g m g p l p p t n g s i i g p \n",
      "Actual word is: c a u s i n g  \n",
      "---\n",
      "Predicted word is: r v p r i l p t h s p m m t s s t n n n \n",
      "Actual word is: r e p r e s e n t a t i o n s  \n",
      "---\n",
      "Predicted word is: t s p i i p p p l p i n b i t g b s p i \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: c o s s s l t n p n m k o p p o n g p b \n",
      "Actual word is: c h a n n e l s  \n",
      "---\n",
      "Predicted word is: t s m i e r p p b p n p o i m p g e i b \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: f a h p p n b b t p g e n p b p p m g s \n",
      "Actual word is: f r o m  \n",
      "---\n",
      "Predicted word is: g l s t d e p p r p s n h o b r n i e g \n",
      "Actual word is: a d m i s s i o n  \n",
      "---\n",
      "Predicted word is: e m r e p l f t p p i p b n f n p t n p \n",
      "Actual word is: e a c h  \n",
      "---\n",
      "Predicted word is: r m s d p e g e g n i g n p i g p f p p \n",
      "Actual word is: r e c o r d  \n",
      "---\n",
      "Predicted word is: w h i d l s p e n p m p n f n b c b t s \n",
      "Actual word is: w h e t h e r  \n",
      "---\n",
      "Predicted word is: w o r v a p p i p i f p p f i v g i p t \n",
      "Actual word is: w h e n  \n",
      "---\n",
      "Predicted word is: p d s s s b t n i i n p g i n n n p n m \n",
      "Actual word is: p r e s e n t i n g  \n",
      "---\n",
      "Predicted word is: p g s c b f i t a g i o n n n j o r n s \n",
      "Actual word is: p e r f o r m i n g  \n",
      "---\n",
      "Predicted word is: o c a i s s p j p n p m m p i n n s s n \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o d t t s s p n n h n p i n n p n i g g \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: f i f r p p s g l b n p e p n v n m n n \n",
      "Actual word is: f u n c t i o n  \n",
      "---\n",
      "Predicted word is: r o v r l c b p i f s p p n n n g n p b \n",
      "Actual word is: r e s u l t  \n",
      "---\n",
      "Predicted word is: i m s t s p i o p g n n g n n n k n s p \n",
      "Actual word is: i n p u t t e d  \n",
      "---\n",
      "Predicted word is: t r p w i p p d s n s p p n p b i p p o \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: m i v e s l b e e p m i i t g p n n f n \n",
      "Actual word is: m e t h o d  \n",
      "---\n",
      "Predicted word is: s t r n p i i p p n p i b m n f h t p o \n",
      "Actual word is: n o v e l t y  \n",
      "---\n",
      "Predicted word is: a r s b i i p n n p i p i n p p p g p t \n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: t s t i i i p t r p p b n i a b t s p t \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: a a p t d v r n r p k n g n \n",
      "Actual word is: o f f e n s i v e  \n",
      "---\n",
      "Predicted word is: a t p d s s o r i p n p n n g p s g n m \n",
      "Actual word is: a u d i o  \n",
      "---\n",
      "Predicted word is: i a p l l e p l b p n f n n n p w n p n \n",
      "Actual word is: i n c l u d i n g  \n",
      "---\n",
      "Predicted word is: p n d c d o p p l g e n p i i i n n i s \n",
      "Actual word is: p l u r a l i t y  \n",
      "---\n",
      "Predicted word is: r s s s l f a p i p n p n f p i p p n n \n",
      "Actual word is: r e c e i v i n g  \n",
      "---\n",
      "Predicted word is: t s t i i i p t r p p b n i a b t s p t \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: o u e o p p o f m \n",
      "Actual word is: o p e r a t e d  \n",
      "---\n",
      "Predicted word is: o c h i s s p j n h i b t u t u t p t n \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: b o c s b e p i m i m i p n h u r p p n \n",
      "Actual word is: b y  \n",
      "---\n",
      "Predicted word is: l r g p r f i g p t i g e n n i i p t i \n",
      "Actual word is: l o c a t i o n  \n",
      "---\n",
      "Predicted word is: r a s i f a i t s p i p p n g i i n i p \n",
      "Actual word is: r e c e i p t  \n",
      "---\n",
      "Predicted word is: a r s b i i p n n p i p i n p p p g t t \n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: f a p s n b t c p p p n t g t p p i p p \n",
      "Actual word is: f i r s t  \n",
      "---\n",
      "Predicted word is: o u p l p p p s s n t g n i g m s p l s \n",
      "Actual word is: o u t p u t  \n",
      "---\n",
      "Predicted word is: l a c h p g p n m i p p p p g f p f i n \n",
      "Actual word is: l o g  \n",
      "---\n",
      "Predicted word is: d p s p r e c p p n f i g g p n a p t d \n",
      "Actual word is: d e t e r m i n i n g  \n",
      "---\n",
      "Predicted word is: h u g s s g i g p e e i p p n p p n g p \n",
      "Actual word is: h o r i z o n t a l  \n",
      "---\n",
      "Predicted word is: a p p r c i b p i n n i p i g n n f l p \n",
      "Actual word is: a p p l i c a t i o n  \n",
      "---\n",
      "Predicted word is: p r c d i m p b b n n p r n t n p n g s \n",
      "Actual word is: p o w e r  \n",
      "---\n",
      "Predicted word is: a i c p f p s m i p p b p m m p p p i s \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: l s w i s b i f t p t s p p p t n p t n \n",
      "Actual word is: l o g i c a l  \n",
      "---\n",
      "Predicted word is: m r e r a i p i s n n l p b l p m n f r \n",
      "Actual word is: f o b  \n",
      "---\n",
      "Predicted word is: i e a h r p t e g n i h f r b i i p s p \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: i s s c g i b m i n n p t p p p b n n n \n",
      "Actual word is: i n t e g r a t e d  \n",
      "---\n",
      "Predicted word is: t s p i i i p d l p t n p i n h b t m p \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: c d p r h t m d s r p p n m p n p s b t \n",
      "Actual word is: c o u p l e d  \n",
      "---\n",
      "Predicted word is: b e c s e s j i p n c o r i n f b n p g \n",
      "Actual word is: b y  \n",
      "---\n",
      "Predicted word is: s m i s s i g f p p s p n n r n p t p f \n",
      "Actual word is: s p e c i f i e d  \n",
      "---\n",
      "Predicted word is: a i r r f p s p i p p g n g o p i g g n \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: p r c d i m p t b f n p k n p p p p s n \n",
      "Actual word is: p o w e r  \n",
      "---\n",
      "Predicted word is: p v p l n p p p p s n g g j i p p j p p \n",
      "Actual word is: p a r a m e t e r  \n",
      "---\n",
      "Predicted word is: e e p d b i n s p i i m n g h b i g g n \n",
      "Actual word is: e n g i n e  \n",
      "---\n",
      "Predicted word is: s t e c t h g g p i f t p b r m p r b l \n",
      "Actual word is: s p a c e  \n",
      "---\n",
      "Predicted word is: s h p l g e t q p i p f p s m t g t n n \n",
      "Actual word is: s i g n a l  \n",
      "---\n",
      "Predicted word is: r t m f l n p t g i p i i p p h i n g i \n",
      "Actual word is: r a n g e  \n",
      "---\n",
      "Predicted word is: o t t i s s g t n i s p k p b p i p g i \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: u p t s g i p c i x p t p t t f i p n n \n",
      "Actual word is: u n i t  \n",
      "---\n",
      "Predicted word is: o l a s c l f s p i p m p s n r n n p r \n",
      "Actual word is: o b j e c t s  \n",
      "---\n",
      "Predicted word is: p g o s b r i t n h p b n p b t n p n p \n",
      "Actual word is: p e r f o r m i n g  \n",
      "---\n",
      "Predicted word is: b u c b g i e e s p m m p i p n b s n n \n",
      "Actual word is: b a r  \n",
      "---\n",
      "Predicted word is: t i s p s i n b i d s i t j n s p m n l \n",
      "Actual word is: t r a n s f o r m e d  \n",
      "---\n",
      "Predicted word is: j d p s g o s t p j p n n w f t n i w r \n",
      "Actual word is: j o b  \n",
      "---\n",
      "Predicted word is: t s p i e i p p r n i g p p i n g g p t \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: e m r e p b n e p p n n t h p n g i m w \n",
      "Actual word is: e a c h  \n",
      "---\n",
      "Predicted word is: t s m i e c p p b p e p o r p n p i n g \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: o t c d s s p t n p g p i p m p p p p i \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: t r p w i p p i s n s n n f p i s p i p \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: s t i p p r b b s n n g n t m i g p m i \n",
      "Actual word is: s t e p s  \n",
      "---\n",
      "Predicted word is: o n g i s i r e p i o p n g i i p p p n \n",
      "Actual word is: o n e  \n",
      "---\n",
      "Predicted word is: t r p w i p p r s n s p i g p i u p p t \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: m m o e n s n t e e n p w p o p p r p n \n",
      "Actual word is: m o r e  \n",
      "---\n",
      "Predicted word is: c i p c r i i b p i n s n b p p t p t p \n",
      "Actual word is: c h a n g e  \n",
      "---\n",
      "Predicted word is: s c e d n p n t i p p m n g i i b t t p \n",
      "Actual word is: s e c o n d  \n",
      "---\n",
      "Predicted word is: m i p g s b p f p t n f t p m n i p p n \n",
      "Actual word is: m o v e m e n t  \n",
      "---\n",
      "Predicted word is: m l s v n t p p p p n b i n i p n r b p \n",
      "Actual word is: m e a s u r i n g  \n",
      "---\n",
      "Predicted word is: t s p i i i p d l p t n p i n h b t m p \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: e a s p i u p p i n s n n g p p t n b n \n",
      "Actual word is: e n d  \n",
      "---\n",
      "Predicted word is: a i c t n p s a i s p g p s b n n s n h \n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: p g b i i i f b b t s h p n n s i n b p \n",
      "Actual word is: p e r t a i n i n g  \n",
      "---\n",
      "Predicted word is: c n o p i s p n m n n n n p n b p i \n",
      "Actual word is: c o m p l e x i t y  \n",
      "---\n",
      "Predicted word is: l c e p b i s n g i p t m p i p g p n n \n",
      "Actual word is: l e a s t  \n",
      "---\n",
      "Predicted word is: t r t w i s p p p p n p n h s p p n w i \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: p r l v e p s p f m p p n p g t n n p p \n",
      "Actual word is: p a r t i t i o n e d  \n",
      "---\n",
      "Predicted word is: d i d s h l u f n h g b t n p n p p n p \n",
      "Actual word is: d e p l o y m e n t  \n",
      "---\n",
      "Predicted word is: u a b r g t g e i p t p n p n n n n n p \n",
      "Actual word is: u s e r  \n",
      "---\n",
      "Predicted word is: i d p s s \n",
      "Actual word is: i s  \n",
      "---\n",
      "Predicted word is: c u s p e a t b l n f t g d g b n p i n \n",
      "Actual word is: c o n s u m e r  \n",
      "---\n",
      "Predicted word is: f r a c s e g t n p p t p i g m t p b i \n",
      "Actual word is: f o u n d  \n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word is: d h a d c l d i p t s p s p p g i n f p \n",
      "Actual word is: d m a  \n",
      "---\n",
      "Predicted word is: i t a k e p i n g n i n t n p l p p p n \n",
      "Actual word is: i n  \n",
      "---\n",
      "Predicted word is: t s p i i c p p r p n f i p g p b n i n \n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: a r s b i i p n n p i p i n p p p g p t \n",
      "Actual word is: a n d  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "char_s2s_ext.example_output(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah - this isn't working at all. Why am I getting a low loss?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoding\n",
    "\n",
    "Machine Learning Mastery has an article [here](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) that describes how to implement a beam search. However, in the comments this is criticised for being too simplistic. Here is [another example](https://gist.github.com/udibr/67be473cf053d8c38730) of a beam search for keras. Here is an example slide: http://www.cs.cmu.edu/afs/cs/academic/class/46927-f97/slides/Lec3/sld023.htm.\n",
    "\n",
    "The beam search algorithm:\n",
    "\n",
    "* Initialise probabilities to 1 for start token;\n",
    "* Predict the list of probabilities for the start token;\n",
    "* Update scores as score * -log(each of k top probs);\n",
    "* Select the k top scores;\n",
    "* Build k sequences for each of the k top probabilities  = start token + prediction for 0 to k-1\n",
    "* Predict next token for each sequence - update scores and sequences.\n",
    "\n",
    "We need to keep track of states along the sequence.\n",
    "\n",
    "We need to adapt our `_predict_from_seq()` method.\n",
    "\n",
    "Is it easier to update as arrays and then only select the top k.\n",
    "\n",
    "Data structures:\n",
    "* scores\n",
    "* sequences (or (token, state) as sequence elements)\n",
    "* states\n",
    "\n",
    "Input:\n",
    "* initial_encoded_state\n",
    "* k\n",
    "\n",
    "Output:\n",
    "* k sequences with top scores\n",
    "\n",
    "For each timestamp, for each element in score, we have a set of predicted probabilities.\n",
    "\n",
    "(Aside: Transformer network - https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "import math\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\" Helper function to sample an index from a probability array. \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return probas\n",
    "\n",
    "def _predict_from_seq(self, input_data, k, temp=1.0):\n",
    "    \"\"\" Predict output text from input text. \"\"\"\n",
    "    state = input_data.reshape(1, self.latent_dim)\n",
    "    ndt = self.num_decoder_tokens  # Just shorten the variable name \n",
    "    output_sequences = []\n",
    "    # Initialise data storage - char sequence, state sequence, scores\n",
    "    sequences = [[[self.output_dictionary[\"_SOW_\"]], [state], 0.0]]\n",
    "    \n",
    "    for _ in range(self.decoder_seq_length):\n",
    "        # Create an empty array to hold the scores in each pass\n",
    "        new_scores = np.zeros(shape=(len(sequences)*ndt))\n",
    "        temp_states = []\n",
    "        #print([s[0] for s in sequences])\n",
    "        for i, seq in enumerate(sequences):\n",
    "            #print(i, seq[0])\n",
    "            # Get most recent state as last entry in state sequence\n",
    "            prev_state = seq[1][-1]\n",
    "            # Get most recent score\n",
    "            prev_score = seq[2]\n",
    "            # Get current token sequence\n",
    "            prev_seq = seq[0]\n",
    "            # Get previous token and convert to one-hot\n",
    "            prev_token = to_categorical(prev_seq[-1], num_classes=ndt).reshape(1, 1, ndt)\n",
    "            \n",
    "            # Predict token probabilities using previous state and token for sequence\n",
    "            yhat, state = self.infdec.predict([prev_state, prev_token])\n",
    "            # Unpack yhat array of probabilities\n",
    "            #yhat = sample(yhat[0, 0, :])\n",
    "            yhat = yhat[0,0,:]\n",
    "            # print(yhat)\n",
    "            # We can put a sample line in here to modify yhat\n",
    "            # Save state in temporary array\n",
    "            temp_states.append(state)\n",
    "            # As we are taking the log do we not sum the scores?\n",
    "            new_scores[i*ndt:(i+1)*ndt] = prev_score+-np.log(yhat) # We could do this inplac\n",
    "            # print(new_scores)\n",
    "            # Actually I only need to look at the top k for each sequence, then further distill these down to the top k\n",
    "            # across all sequences - maybe one for optimising later\n",
    "            \n",
    "        # Outside of loop we want to pick the k highest scores\n",
    "        # Each sequence has a set of scores equal in size to num_decoder_tokens- i.e. k*n_d_t scores in total\n",
    "        # We want to select the k highest scores across all scores in total - but then we need to know\n",
    "        # We just modulo k on the index to find the indice and int(index/num_decoder_tokens) to find k\n",
    "        \n",
    "        # Then update the sequences to reflect these k highest scores\n",
    "            \n",
    "        # select top k scores -as we are minimising these are at bottom of list\n",
    "        top_k_indices = np.argsort(new_scores)[:k].tolist()\n",
    "        #print(new_scores[top_k_indices])\n",
    "        #print(top_k_indices)\n",
    "        new_sequences = []\n",
    "        seqs_to_delete = []\n",
    "        for index in top_k_indices:\n",
    "            seq_select = int(index/ndt)\n",
    "            #print(\"Selected seq: \", seq_select)\n",
    "            #print(\"Length of sequences: \", len(sequences))\n",
    "            new_token = index % ndt # this is the token index\n",
    "            # This is equivalent to argmax - but should we actually be sampling?\n",
    "            new_seq = sequences[seq_select][0] + [new_token]\n",
    "            new_state_seq = sequences[seq_select][1] + [temp_states[seq_select]]\n",
    "            #print(new_scores[index])\n",
    "            entry = (new_seq, new_state_seq, new_scores[index])\n",
    "            # If predicted token is end token\n",
    "            if new_token == self.output_dictionary[\"_EOW_\"]:\n",
    "                # Add data for output\n",
    "                output_sequences.append(entry)\n",
    "                # Reduce k by 1\n",
    "                k -= 1\n",
    "            else:\n",
    "                # Add to list of new sequences to use\n",
    "                new_sequences.append(entry)\n",
    "        sequences = new_sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "    \n",
    "    # Sort list in reverse \"score\" order\n",
    "    output_sequences.sort(key=lambda x: x[2])\n",
    "    return output_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The - turns it into a minimisation problem! I've been picking the most unlikely sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = char_s2s.input_test_data[15]\n",
    "output_data = char_s2s.output_test_data[15]\n",
    "seqs = _predict_from_seq(char_s2s, input_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20, 3, 16, 13, 7, 6, 2]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input test data is: r a n k e d  \n",
      "\n",
      "1.661349892616272 r a n k e d \n",
      "Input test data is: r a n k e d  \n",
      "\n",
      "2.318960428237915 r e v e e \n",
      "Input test data is: r a n k e d  \n",
      "\n",
      "3.498708724975586 r e a l h i n e \n",
      "Input test data is: r a n k e d  \n",
      "\n",
      "3.629056692123413 r i s t e r \n",
      "Input test data is: r a n k e d  \n",
      "\n",
      "4.727723121643066 r a n k e n t e s \n"
     ]
    }
   ],
   "source": [
    "for seq, state, score in seqs:\n",
    "    print(\"Input test data is:\", char_s2s._seq2text(output_data), \"\\n\")\n",
    "    print(score, char_s2s._seq2text(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_examples(self, number):\n",
    "    num_test_titles = len(self.input_test_data)\n",
    "    indices = random.sample(range(0, num_test_titles), number)\n",
    "    print(\"------------------------------------------\")\n",
    "    for i in indices:\n",
    "        input_sample = self.input_test_data[i]\n",
    "        output_sample = self.output_test_data[i]\n",
    "        seqs = _predict_from_seq(self, input_sample, 5)\n",
    "        output_sample_text = self._seq2text(output_sample)\n",
    "        print(\"\\nActual word is: {} \\n---\".format(output_sample_text))\n",
    "        for seq, state, score in seqs:\n",
    "            o_string = (\n",
    "                \"Predicted word is: {0} with score {1}\"\n",
    "            )\n",
    "            \n",
    "            print(o_string.format(self._seq2text(seq), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: d e v i c e  \n",
      "---\n",
      "Predicted word is: d e v i c e  with score 0.0029975324869155884\n",
      "Predicted word is: e e t w o r  with score 7.680788993835449\n",
      "Predicted word is: d e c i c e  with score 7.939220428466797\n",
      "Predicted word is: w e m m i n g  with score 8.304058074951172\n",
      "Predicted word is: d e v i c i  with score 8.77090835571289\n",
      "\n",
      "Actual word is: f i r s t  \n",
      "---\n",
      "Predicted word is: f i r s t  with score 0.005848006810992956\n",
      "Predicted word is: f i r s n  with score 6.683882236480713\n",
      "Predicted word is: s i r e g  with score 7.598910331726074\n",
      "Predicted word is: f i r e n  with score 7.643312931060791\n",
      "Predicted word is: t i r n  with score 7.6536865234375\n",
      "\n",
      "Actual word is: i n p u t t e d  \n",
      "---\n",
      "Predicted word is: s p e c t i v e l y  with score 5.865662574768066\n",
      "Predicted word is: s p e c t i v e  with score 6.238517761230469\n",
      "Predicted word is: d e c o n c e r  with score 7.120120525360107\n",
      "Predicted word is: d e c o n c e r s c e d  with score 8.662623405456543\n",
      "Predicted word is: d e c o n c e r s c o n s  with score 10.179978370666504\n",
      "\n",
      "Actual word is: d u r i n g  \n",
      "---\n",
      "Predicted word is: d u r i n g  with score 0.185548335313797\n",
      "Predicted word is: d i r i o r  with score 4.235347270965576\n",
      "Predicted word is: d i r i i n  with score 5.181137561798096\n",
      "Predicted word is: a c t i a l  with score 5.54401969909668\n",
      "Predicted word is: l e r e n c h  with score 5.761934757232666\n",
      "\n",
      "Actual word is: p l u r a l i t y  \n",
      "---\n",
      "Predicted word is: p l u r a l i t y  with score 0.011723889037966728\n",
      "Predicted word is: p o u r e r i a l  with score 6.384951114654541\n",
      "Predicted word is: p l u r a n i t y  with score 7.605269432067871\n",
      "Predicted word is: p l u r a l i c e l  with score 7.951265811920166\n",
      "Predicted word is: p u r r e n t e n c i v e  with score 8.99815845489502\n",
      "\n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: a n d  with score 0.004450682085007429\n",
      "Predicted word is: i n g  with score 6.869454860687256\n",
      "Predicted word is: m h a l l  with score 8.27072525024414\n",
      "Predicted word is: w h r o w i d  with score 8.4688138961792\n",
      "Predicted word is: w h r r u s  with score 9.325377464294434\n",
      "\n",
      "Actual word is: i n s t r u c t i o n s  \n",
      "---\n",
      "Predicted word is: i n s t r u c t i o n s  with score 0.13035015761852264\n",
      "Predicted word is: i n s t r u c t i o n  with score 3.7944486141204834\n",
      "Predicted word is: i n s t r u c t s o n  with score 4.368583679199219\n",
      "Predicted word is: i n s t r u c t e d  with score 5.568063735961914\n",
      "Predicted word is: r e s j u n c t e d  with score 5.616182327270508\n",
      "\n",
      "Actual word is: o u t p u t  \n",
      "---\n",
      "Predicted word is: o u t p u t  with score 0.09312215447425842\n",
      "Predicted word is: o o t p u t  with score 4.732413291931152\n",
      "Predicted word is: o p p u r t s  with score 5.410976409912109\n",
      "Predicted word is: o u t p l e  with score 5.714317798614502\n",
      "Predicted word is: o g r u l a t  with score 5.960289001464844\n",
      "\n",
      "Actual word is: a n a l y t i c s  \n",
      "---\n",
      "Predicted word is: a n a l y t i c s  with score 2.168372631072998\n",
      "Predicted word is: a s s o  with score 2.283041477203369\n",
      "Predicted word is: i n a r t h  with score 2.9004833698272705\n",
      "Predicted word is: a s s e  with score 2.9252002239227295\n",
      "Predicted word is: a n a l y t i v e l y  with score 4.6914496421813965\n",
      "\n",
      "Actual word is: s a i d  \n",
      "---\n",
      "Predicted word is: s a i d  with score 0.008283196948468685\n",
      "Predicted word is: s a i d s  with score 6.796751499176025\n",
      "Predicted word is: t a i d  with score 7.198991298675537\n",
      "Predicted word is: d a s s s t  with score 7.544102668762207\n",
      "Predicted word is: m a s s s t e n  with score 9.047149658203125\n",
      "\n",
      "Actual word is: i s  \n",
      "---\n",
      "Predicted word is: i s  with score 0.009297661483287811\n",
      "Predicted word is: b e  with score 6.001955509185791\n",
      "Predicted word is: w a k  with score 6.127144813537598\n",
      "Predicted word is: i n  with score 6.52364444732666\n",
      "Predicted word is: i a  with score 7.394513130187988\n",
      "\n",
      "Actual word is: f u n c t i o n a l  \n",
      "---\n",
      "Predicted word is: f u n c t i o n a l  with score 0.2812218964099884\n",
      "Predicted word is: f u n c t i o n a  with score 4.08388090133667\n",
      "Predicted word is: f u l l i t y  with score 4.964935779571533\n",
      "Predicted word is: f u n n f i r  with score 5.613723278045654\n",
      "Predicted word is: f u n c t i o n a m  with score 5.700239658355713\n",
      "\n",
      "Actual word is: d i s c o v e r y  \n",
      "---\n",
      "Predicted word is: p e t i n a t i o n  with score 3.123701810836792\n",
      "Predicted word is: p e t i n a l  with score 3.5419552326202393\n",
      "Predicted word is: t e a d  with score 3.7183640003204346\n",
      "Predicted word is: p e t i n a n c e  with score 3.7936248779296875\n",
      "Predicted word is: p e t i n a t i v e  with score 4.326220989227295\n",
      "\n",
      "Actual word is: c o m p r i s i n g  \n",
      "---\n",
      "Predicted word is: c o m p r i s i n g  with score 0.008581986650824547\n",
      "Predicted word is: i n f l a t e a n t i n g  with score 7.627142906188965\n",
      "Predicted word is: i n i l i t e s  with score 8.246598243713379\n",
      "Predicted word is: c o m p r i e n d  with score 8.624258995056152\n",
      "Predicted word is: i n f l a t e a t i n g  with score 8.658149719238281\n",
      "\n",
      "Actual word is: n u m b e r s  \n",
      "---\n",
      "Predicted word is: n u m b e r s  with score 0.7957664132118225\n",
      "Predicted word is: n u m b e d  with score 2.951245069503784\n",
      "Predicted word is: r a m u r  with score 3.722501754760742\n",
      "Predicted word is: n u m b e r n a t  with score 4.065950393676758\n",
      "Predicted word is: m u g r e  with score 4.192705154418945\n",
      "\n",
      "Actual word is: p r o c e s s o r  \n",
      "---\n",
      "Predicted word is: p r o c e s s o r  with score 0.007582961581647396\n",
      "Predicted word is: p r o c e m s  with score 6.641809463500977\n",
      "Predicted word is: p r o c a s s  with score 7.971148490905762\n",
      "Predicted word is: h r o u s e  with score 8.06017017364502\n",
      "Predicted word is: c r o c r  with score 8.318737030029297\n",
      "\n",
      "Actual word is: f o r  \n",
      "---\n",
      "Predicted word is: f o r  with score 0.0034711030311882496\n",
      "Predicted word is: b o d e  with score 7.284790515899658\n",
      "Predicted word is: a o d  with score 7.509518146514893\n",
      "Predicted word is: f o r m  with score 7.69878625869751\n",
      "Predicted word is: t o m m  with score 8.304365158081055\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: e s t i m a t e  \n",
      "---\n",
      "Predicted word is: e s t i m a t e  with score 0.2856873571872711\n",
      "Predicted word is: a s s  with score 3.9416561126708984\n",
      "Predicted word is: e s t i m e  with score 4.305062770843506\n",
      "Predicted word is: e s t i m a y e  with score 5.172402858734131\n",
      "Predicted word is: e s t i m i t t e  with score 5.486722946166992\n",
      "\n",
      "Actual word is: f i l e  \n",
      "---\n",
      "Predicted word is: f i l e  with score 0.015984179452061653\n",
      "Predicted word is: f i l  with score 5.2508673667907715\n",
      "Predicted word is: d i l e  with score 6.531556606292725\n",
      "Predicted word is: d i f e  with score 6.9005303382873535\n",
      "Predicted word is: a i l  with score 7.142911434173584\n",
      "\n",
      "Actual word is: a c c o u n t  \n",
      "---\n",
      "Predicted word is: a c c o u n t  with score 0.09691794216632843\n",
      "Predicted word is: a c c o i n t  with score 4.055283546447754\n",
      "Predicted word is: a c c o i n a l  with score 5.838675022125244\n",
      "Predicted word is: a c c o i n y  with score 6.226454257965088\n",
      "Predicted word is: e x c u c a s a t e  with score 7.226334095001221\n",
      "\n",
      "Actual word is: c a l c u l a t i n g  \n",
      "---\n",
      "Predicted word is: c a l c u l a t i n g  with score 0.12619414925575256\n",
      "Predicted word is: c o l c c i n s  with score 4.827816486358643\n",
      "Predicted word is: c a l t s i n g  with score 5.579505443572998\n",
      "Predicted word is: c a l c u c a t i n g  with score 5.736437797546387\n",
      "Predicted word is: c u l c t i o n  with score 5.940690994262695\n",
      "\n",
      "Actual word is: l a c k s  \n",
      "---\n",
      "Predicted word is: c o n s i s t e d  with score 1.7045351266860962\n",
      "Predicted word is: r e l a y  with score 2.2402701377868652\n",
      "Predicted word is: r a l l y  with score 2.730937957763672\n",
      "Predicted word is: r e c e s s  with score 3.07576584815979\n",
      "Predicted word is: r a n t i n g s  with score 4.827898025512695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual word is: i l l u m i n a t e d  \n",
      "---\n",
      "Predicted word is: i l l u g r o p t i o n  with score 4.530892848968506\n",
      "Predicted word is: o u t l a t e d  with score 4.658166408538818\n",
      "Predicted word is: o u t l a t e  with score 4.915664196014404\n",
      "Predicted word is: i l l i g n o m e d  with score 5.16035270690918\n",
      "Predicted word is: i l l i g u t i v e  with score 5.437795162200928\n",
      "\n",
      "Actual word is: d r i v e n  \n",
      "---\n",
      "Predicted word is: d r i c e s  with score 3.088684320449829\n",
      "Predicted word is: d r i v e n  with score 3.908629894256592\n",
      "Predicted word is: d e n c e r s  with score 4.241482734680176\n",
      "Predicted word is: d r i n t e d  with score 4.340156555175781\n",
      "Predicted word is: d r i v e n t  with score 4.717126846313477\n",
      "\n",
      "Actual word is: r o u t i n e  \n",
      "---\n",
      "Predicted word is: e a n c e  with score 2.2805418968200684\n",
      "Predicted word is: i p e r a n t  with score 2.4373419284820557\n",
      "Predicted word is: o p t i m n s  with score 2.7541708946228027\n",
      "Predicted word is: a p r e c e r  with score 2.9230170249938965\n",
      "Predicted word is: a p r e c e r s  with score 2.961667060852051\n",
      "\n",
      "Actual word is: t a s k  \n",
      "---\n",
      "Predicted word is: t a s k  with score 0.5513123273849487\n",
      "Predicted word is: d e s c o n s a  with score 2.3349339962005615\n",
      "Predicted word is: t a c k  with score 3.160799026489258\n",
      "Predicted word is: d e s c o n s f  with score 3.6741349697113037\n",
      "Predicted word is: d e s c o n e a t i o n  with score 4.027970790863037\n",
      "\n",
      "Actual word is: c o n t r i b u t i o n  \n",
      "---\n",
      "Predicted word is: c o n t r i p e  with score 3.749663829803467\n",
      "Predicted word is: c o n l i n g  with score 3.851577043533325\n",
      "Predicted word is: p u m t e d  with score 3.8607113361358643\n",
      "Predicted word is: c o n t r i b u t i o n  with score 3.902695417404175\n",
      "Predicted word is: c o n t r i n s  with score 4.192532062530518\n",
      "\n",
      "Actual word is: p a y m e n t  \n",
      "---\n",
      "Predicted word is: p a y m e n t  with score 0.45065197348594666\n",
      "Predicted word is: t r a g  with score 3.1191565990448\n",
      "Predicted word is: p a r y  with score 4.00044584274292\n",
      "Predicted word is: r e a c i a l  with score 4.324132442474365\n",
      "Predicted word is: r e a c i e n  with score 5.001075744628906\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: m a t e r i a l s  \n",
      "---\n",
      "Predicted word is: m a t e r i a l s  with score 1.6089320182800293\n",
      "Predicted word is: m a t e r o r d  with score 2.263942241668701\n",
      "Predicted word is: m a t e r u r t s  with score 2.3885700702667236\n",
      "Predicted word is: m a t e r i a s c e  with score 3.43495512008667\n",
      "Predicted word is: m a t e r u e n t s  with score 3.6779019832611084\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n",
      "\n",
      "Actual word is: v a l u e  \n",
      "---\n",
      "Predicted word is: v a l u e  with score 0.013172484003007412\n",
      "Predicted word is: i m l e r  with score 6.2149658203125\n",
      "Predicted word is: v o l u e n t  with score 6.408112525939941\n",
      "Predicted word is: v a l h e  with score 6.49057149887085\n",
      "Predicted word is: v a r e e  with score 6.807107925415039\n",
      "\n",
      "Actual word is: s t o r e d  \n",
      "---\n",
      "Predicted word is: s t o r e d  with score 0.04219072312116623\n",
      "Predicted word is: s t o r e  with score 4.148935317993164\n",
      "Predicted word is: r a t r e n  with score 4.986300945281982\n",
      "Predicted word is: l a t e  with score 5.683352470397949\n",
      "Predicted word is: c a t r a t e m s  with score 6.821863174438477\n",
      "\n",
      "Actual word is: d e c o m p o s i n g  \n",
      "---\n",
      "Predicted word is: f l o w n d e d  with score 5.468647480010986\n",
      "Predicted word is: d e c o n t e n t i n g  with score 7.248527526855469\n",
      "Predicted word is: d e c o n t e r c e s  with score 8.003442764282227\n",
      "Predicted word is: d e c o n v e r t i o n  with score 8.39664077758789\n",
      "Predicted word is: d e c o n v e r t i o n s  with score 8.889068603515625\n",
      "\n",
      "Actual word is: p r o c e s s i n g  \n",
      "---\n",
      "Predicted word is: p r o c e s s i n g  with score 0.004981868900358677\n",
      "Predicted word is: p a t i m a t e  with score 6.4001946449279785\n",
      "Predicted word is: p r o c i s s i n g  with score 7.896561145782471\n",
      "Predicted word is: p a t i m a t  with score 8.291555404663086\n",
      "Predicted word is: f r o c e s t i n g  with score 8.574469566345215\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n",
      "\n",
      "Actual word is: s i n c e  \n",
      "---\n",
      "Predicted word is: r a l l  with score 2.749084234237671\n",
      "Predicted word is: e r o o  with score 3.3885459899902344\n",
      "Predicted word is: e r i n u  with score 3.3971002101898193\n",
      "Predicted word is: e f i o n s  with score 3.404493570327759\n",
      "Predicted word is: e f i r n s  with score 3.939929246902466\n",
      "\n",
      "Actual word is: r e s t o r i n g  \n",
      "---\n",
      "Predicted word is: r e s t o r i n g  with score 2.355755567550659\n",
      "Predicted word is: r e s t o v e d  with score 3.02396559715271\n",
      "Predicted word is: r e s t o r i e s  with score 3.0366344451904297\n",
      "Predicted word is: r e c t i v e  with score 3.0639967918395996\n",
      "Predicted word is: r e s t a n c e  with score 3.0677475929260254\n",
      "\n",
      "Actual word is: s y s t e m  \n",
      "---\n",
      "Predicted word is: s y s t e m  with score 0.017571931704878807\n",
      "Predicted word is: s t s e c t e r  with score 5.549953460693359\n",
      "Predicted word is: s h a r m t e r  with score 6.645102500915527\n",
      "Predicted word is: s h a r m p e  with score 7.2732014656066895\n",
      "Predicted word is: c o s t i c e s  with score 7.699917793273926\n",
      "\n",
      "Actual word is: m o d u l a t i o n  \n",
      "---\n",
      "Predicted word is: f o r c a b l e  with score 1.768713355064392\n",
      "Predicted word is: m o d u l a t i o n  with score 2.2572269439697266\n",
      "Predicted word is: a d j c c i o r y  with score 4.031432151794434\n",
      "Predicted word is: a d j c c i o p  with score 4.462343215942383\n",
      "Predicted word is: a d j c c i v a l  with score 4.554770469665527\n",
      "\n",
      "Actual word is: a s s e m b l i n g  \n",
      "---\n",
      "Predicted word is: s i n c t i v e  with score 4.403224945068359\n",
      "Predicted word is: s c a n t e r i n g  with score 5.219376087188721\n",
      "Predicted word is: s i n c u r d u c e d  with score 5.299654960632324\n",
      "Predicted word is: s i n c t i v e l y  with score 5.879504680633545\n",
      "Predicted word is: s i n c u c t  with score 6.07281494140625\n",
      "\n",
      "Actual word is: d e t e c t i o n  \n",
      "---\n",
      "Predicted word is: d e t e c t i o n  with score 0.05333971232175827\n",
      "Predicted word is: d i t e  with score 5.0716352462768555\n",
      "Predicted word is: d y n e  with score 5.4352803230285645\n",
      "Predicted word is: d i t e c t i o  with score 5.566892623901367\n",
      "Predicted word is: d y b e c e l  with score 5.701301097869873\n",
      "\n",
      "Actual word is: t h a t  \n",
      "---\n",
      "Predicted word is: t h a t  with score 0.002729880390688777\n",
      "Predicted word is: t e a t  with score 7.909205913543701\n",
      "Predicted word is: t a  with score 8.199243545532227\n",
      "Predicted word is: n e s  with score 8.271403312683105\n",
      "Predicted word is: d e  with score 8.468073844909668\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n",
      "\n",
      "Actual word is: c l o n e  \n",
      "---\n",
      "Predicted word is: t r a n s  with score 4.2195539474487305\n",
      "Predicted word is: e c t u b e r  with score 4.551996231079102\n",
      "Predicted word is: e c t u b e  with score 4.651081085205078\n",
      "Predicted word is: e c t u b o r e  with score 5.419806003570557\n",
      "Predicted word is: e c t e n c l e s  with score 6.57691764831543\n",
      "\n",
      "Actual word is: a s s o c i a t e d  \n",
      "---\n",
      "Predicted word is: a s s o c i a t e d  with score 0.04028412327170372\n",
      "Predicted word is: a c i n a s t s  with score 6.346920490264893\n",
      "Predicted word is: a s s o c i a t e  with score 6.583090782165527\n",
      "Predicted word is: a c i n a s t e d  with score 6.760105609893799\n",
      "Predicted word is: a c i n a s t s t  with score 6.847634792327881\n",
      "\n",
      "Actual word is: r i g h t  \n",
      "---\n",
      "Predicted word is: b i g h e  with score 1.7798937559127808\n",
      "Predicted word is: r i g h t  with score 2.220038890838623\n",
      "Predicted word is: w i t t  with score 2.3412814140319824\n",
      "Predicted word is: b i t t  with score 2.5912160873413086\n",
      "Predicted word is: r i g h s  with score 2.641177177429199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual word is: p l u r a l i t y  \n",
      "---\n",
      "Predicted word is: p l u r a l i t y  with score 0.011723889037966728\n",
      "Predicted word is: p o u r e r i a l  with score 6.384951114654541\n",
      "Predicted word is: p l u r a n i t y  with score 7.605269432067871\n",
      "Predicted word is: p l u r a l i c e l  with score 7.951265811920166\n",
      "Predicted word is: p u r r e n t e n c i v e  with score 8.99815845489502\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n",
      "\n",
      "Actual word is: d e t e c t i o n  \n",
      "---\n",
      "Predicted word is: d e t e c t i o n  with score 0.05333971232175827\n",
      "Predicted word is: d i t e  with score 5.0716352462768555\n",
      "Predicted word is: d y n e  with score 5.4352803230285645\n",
      "Predicted word is: d i t e c t i o  with score 5.566892623901367\n",
      "Predicted word is: d y b e c e l  with score 5.701301097869873\n",
      "\n",
      "Actual word is: v a l u e  \n",
      "---\n",
      "Predicted word is: v a l u e  with score 0.013172484003007412\n",
      "Predicted word is: i m l e r  with score 6.2149658203125\n",
      "Predicted word is: v o l u e n t  with score 6.408112525939941\n",
      "Predicted word is: v a l h e  with score 6.49057149887085\n",
      "Predicted word is: v a r e e  with score 6.807107925415039\n",
      "\n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: a n d  with score 0.004450682085007429\n",
      "Predicted word is: i n g  with score 6.869454860687256\n",
      "Predicted word is: m h a l l  with score 8.27072525024414\n",
      "Predicted word is: w h r o w i d  with score 8.4688138961792\n",
      "Predicted word is: w h r r u s  with score 9.325377464294434\n",
      "\n",
      "Actual word is: e a c h  \n",
      "---\n",
      "Predicted word is: e a c h  with score 0.017392924055457115\n",
      "Predicted word is: e a c t  with score 4.776221752166748\n",
      "Predicted word is: e e c t  with score 6.160528659820557\n",
      "Predicted word is: e a c k  with score 7.003701686859131\n",
      "Predicted word is: h a n d  with score 7.68504524230957\n",
      "\n",
      "Actual word is: a l l o c a t i n g  \n",
      "---\n",
      "Predicted word is: a l l o c a t i n g  with score 0.836067795753479\n",
      "Predicted word is: a l l o c a t i v i t y  with score 4.22007417678833\n",
      "Predicted word is: a p p a c i t a t i o n s  with score 4.442594528198242\n",
      "Predicted word is: a l l a c i n g  with score 4.703607082366943\n",
      "Predicted word is: a l l o c a t i v i s  with score 5.011361122131348\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: n o t  \n",
      "---\n",
      "Predicted word is: n o t  with score 0.06482250988483429\n",
      "Predicted word is: s o  with score 4.163504600524902\n",
      "Predicted word is: n a t  with score 4.714540004730225\n",
      "Predicted word is: n e t  with score 4.896421432495117\n",
      "Predicted word is: t h o  with score 6.191328048706055\n",
      "\n",
      "Actual word is: f r o m  \n",
      "---\n",
      "Predicted word is: f r o m  with score 0.0046320087276399136\n",
      "Predicted word is: f f o m  with score 7.078866481781006\n",
      "Predicted word is: f r o m a  with score 7.596268653869629\n",
      "Predicted word is: o r m a n  with score 7.691491603851318\n",
      "Predicted word is: f  with score 8.947467803955078\n",
      "\n",
      "Actual word is: f r i e n d  \n",
      "---\n",
      "Predicted word is: b r a l  with score 2.57047438621521\n",
      "Predicted word is: w a l  with score 3.1413493156433105\n",
      "Predicted word is: r e c e s s  with score 3.4297714233398438\n",
      "Predicted word is: b r a s h  with score 3.5946130752563477\n",
      "Predicted word is: r e c e p t  with score 3.8311283588409424\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: a c c u r a c y  \n",
      "---\n",
      "Predicted word is: c a t u r a t i o n s  with score 2.5320000648498535\n",
      "Predicted word is: c a t u r a t i n g  with score 3.1296563148498535\n",
      "Predicted word is: c o m a u n e d  with score 3.3522861003875732\n",
      "Predicted word is: c a t u r a t i o n  with score 3.442592144012451\n",
      "Predicted word is: c a t a r a b t h i n g  with score 4.366505146026611\n",
      "\n",
      "Actual word is: a n d  \n",
      "---\n",
      "Predicted word is: a n d  with score 0.004450682085007429\n",
      "Predicted word is: i n g  with score 6.869454860687256\n",
      "Predicted word is: m h a l l  with score 8.27072525024414\n",
      "Predicted word is: w h r o w i d  with score 8.4688138961792\n",
      "Predicted word is: w h r r u s  with score 9.325377464294434\n",
      "\n",
      "Actual word is: f r o m  \n",
      "---\n",
      "Predicted word is: f r o m  with score 0.0046320087276399136\n",
      "Predicted word is: f f o m  with score 7.078866481781006\n",
      "Predicted word is: f r o m a  with score 7.596268653869629\n",
      "Predicted word is: o r m a n  with score 7.691491603851318\n",
      "Predicted word is: f  with score 8.947467803955078\n",
      "\n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: t h e  with score 0.0004572269681375474\n",
      "Predicted word is: t h  with score 9.038466453552246\n",
      "Predicted word is: h h  with score 9.3350248336792\n",
      "Predicted word is: n h e  with score 9.627456665039062\n",
      "Predicted word is: h h m  with score 9.925104141235352\n",
      "\n",
      "Actual word is: p e r i o d  \n",
      "---\n",
      "Predicted word is: p e r i o d  with score 0.11181401461362839\n",
      "Predicted word is: p e r i o n  with score 3.6776936054229736\n",
      "Predicted word is: p e r i o n s  with score 4.800350666046143\n",
      "Predicted word is: r e g e s  with score 5.432041168212891\n",
      "Predicted word is: u n t e d  with score 5.470211029052734\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: a r e  \n",
      "---\n",
      "Predicted word is: a r e  with score 0.025802642107009888\n",
      "Predicted word is: a s e  with score 4.869124412536621\n",
      "Predicted word is: a n e  with score 5.943931579589844\n",
      "Predicted word is: b r e w s  with score 6.046737194061279\n",
      "Predicted word is: a r t  with score 6.761077404022217\n",
      "\n",
      "Actual word is: w h e t h e r  \n",
      "---\n",
      "Predicted word is: w h e t h e r  with score 0.07227934151887894\n",
      "Predicted word is: e x t h i n  with score 5.112682342529297\n",
      "Predicted word is: w h a t h e r  with score 5.431345462799072\n",
      "Predicted word is: w h o t h o r e  with score 6.302656650543213\n",
      "Predicted word is: w h o t h i r e  with score 6.326063632965088\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n",
      "\n",
      "Actual word is: t o u c h  \n",
      "---\n",
      "Predicted word is: t o u c h  with score 0.12479810416698456\n",
      "Predicted word is: s o u c h  with score 3.858344554901123\n",
      "Predicted word is: p o c k  with score 4.945833683013916\n",
      "Predicted word is: c o n t a n e  with score 5.416254043579102\n",
      "Predicted word is: s o r w e r  with score 5.484140396118164\n",
      "\n",
      "Actual word is: c l i e n t  \n",
      "---\n",
      "Predicted word is: c l i e n t  with score 0.03673402592539787\n",
      "Predicted word is: p l a c e m  with score 4.740700721740723\n",
      "Predicted word is: p l a c e s  with score 4.765988349914551\n",
      "Predicted word is: p l a n e s  with score 6.048770904541016\n",
      "Predicted word is: o l  with score 6.500560760498047\n",
      "\n",
      "Actual word is: m e a s u r e  \n",
      "---\n",
      "Predicted word is: m e a s u r e  with score 0.15625157952308655\n",
      "Predicted word is: m e x s u s  with score 3.6399784088134766\n",
      "Predicted word is: m e x t u r  with score 4.705031871795654\n",
      "Predicted word is: t e r s u i  with score 4.742267608642578\n",
      "Predicted word is: m e x s u s e  with score 5.833532333374023\n",
      "\n",
      "Actual word is: r a i s i n g  \n",
      "---\n",
      "Predicted word is: t a c k i n g  with score 2.9706337451934814\n",
      "Predicted word is: r e w a b l i n g  with score 4.220996856689453\n",
      "Predicted word is: r e w a b l e  with score 5.282994747161865\n",
      "Predicted word is: r e m i n a t i c a l  with score 5.706801414489746\n",
      "Predicted word is: t a c k n e d  with score 5.719086647033691\n",
      "\n",
      "Actual word is: a  \n",
      "---\n",
      "Predicted word is: a  with score 0.0006665560649707913\n",
      "Predicted word is: w  with score 8.560648918151855\n",
      "Predicted word is: b  with score 9.210902214050293\n",
      "Predicted word is: e  with score 9.33383560180664\n",
      "Predicted word is: l  with score 9.730419158935547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual word is: v o l u m e  \n",
      "---\n",
      "Predicted word is: v o l u m e  with score 0.1608767956495285\n",
      "Predicted word is: v o l u m  with score 4.200084209442139\n",
      "Predicted word is: c o m p e c t e  with score 4.228625774383545\n",
      "Predicted word is: v o l u b r e  with score 4.680570602416992\n",
      "Predicted word is: v o t u b  with score 4.801225662231445\n",
      "\n",
      "Actual word is: a c c e s s  \n",
      "---\n",
      "Predicted word is: a c c e s s  with score 0.06131990626454353\n",
      "Predicted word is: a c c u s  with score 4.87781286239624\n",
      "Predicted word is: p r o c e s s e  with score 5.347423553466797\n",
      "Predicted word is: r o c e s t  with score 5.917101860046387\n",
      "Predicted word is: r e t e r t  with score 6.2648210525512695\n",
      "\n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: t h e  with score 0.0004572269681375474\n",
      "Predicted word is: t h  with score 9.038466453552246\n",
      "Predicted word is: h h  with score 9.3350248336792\n",
      "Predicted word is: n h e  with score 9.627456665039062\n",
      "Predicted word is: h h m  with score 9.925104141235352\n",
      "\n",
      "Actual word is: t r a n s f o r m i n g  \n",
      "---\n",
      "Predicted word is: t r a n s f o r m i n g  with score 1.8073536157608032\n",
      "Predicted word is: e r p a s i n g  with score 3.347397804260254\n",
      "Predicted word is: c o m p r e s e d  with score 3.99662446975708\n",
      "Predicted word is: m u l l i c a t i o n  with score 4.45242166519165\n",
      "Predicted word is: t r a n s i t t e r  with score 4.942381858825684\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: r e l a t i o n s h i p  \n",
      "---\n",
      "Predicted word is: r e l a t i o n s h i p  with score 0.09323781728744507\n",
      "Predicted word is: r e l a t i o n s t i c s  with score 4.614684104919434\n",
      "Predicted word is: r e l a t i o n s  with score 4.7529826164245605\n",
      "Predicted word is: r e l a t i o n s h i s  with score 5.386353015899658\n",
      "Predicted word is: r e v a l t e r  with score 6.295024871826172\n",
      "\n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: t h e  with score 0.0004572269681375474\n",
      "Predicted word is: t h  with score 9.038466453552246\n",
      "Predicted word is: h h  with score 9.3350248336792\n",
      "Predicted word is: n h e  with score 9.627456665039062\n",
      "Predicted word is: h h m  with score 9.925104141235352\n",
      "\n",
      "Actual word is: o n e  \n",
      "---\n",
      "Predicted word is: o n e  with score 0.005226354114711285\n",
      "Predicted word is: t a m  with score 7.227426052093506\n",
      "Predicted word is: i n e  with score 7.575857639312744\n",
      "Predicted word is: h a m  with score 8.252423286437988\n",
      "Predicted word is: f i e w  with score 9.095211029052734\n",
      "\n",
      "Actual word is: p l a y e r  \n",
      "---\n",
      "Predicted word is: p l a y e r  with score 0.9009724855422974\n",
      "Predicted word is: p l a m e t  with score 3.3242406845092773\n",
      "Predicted word is: p l a i s  with score 3.404524564743042\n",
      "Predicted word is: p l a m o t  with score 3.7214293479919434\n",
      "Predicted word is: p l a s o r  with score 4.00201940536499\n",
      "\n",
      "Actual word is: a c c e s s  \n",
      "---\n",
      "Predicted word is: a c c e s s  with score 0.06131990626454353\n",
      "Predicted word is: a c c u s  with score 4.87781286239624\n",
      "Predicted word is: p r o c e s s e  with score 5.347423553466797\n",
      "Predicted word is: r o c e s t  with score 5.917101860046387\n",
      "Predicted word is: r e t e r t  with score 6.2648210525512695\n",
      "\n",
      "Actual word is: t o  \n",
      "---\n",
      "Predicted word is: t o  with score 0.002189942402765155\n",
      "Predicted word is: c o  with score 7.37384033203125\n",
      "Predicted word is: n o  with score 7.4571075439453125\n",
      "Predicted word is: d o  with score 8.928506851196289\n",
      "Predicted word is: t h  with score 8.998828887939453\n",
      "\n",
      "Actual word is: t o  \n",
      "---\n",
      "Predicted word is: t o  with score 0.002189942402765155\n",
      "Predicted word is: c o  with score 7.37384033203125\n",
      "Predicted word is: n o  with score 7.4571075439453125\n",
      "Predicted word is: d o  with score 8.928506851196289\n",
      "Predicted word is: t h  with score 8.998828887939453\n",
      "\n",
      "Actual word is: h a v i n g  \n",
      "---\n",
      "Predicted word is: h a v i n g  with score 0.0693490207195282\n",
      "Predicted word is: h i v i n g  with score 4.26294469833374\n",
      "Predicted word is: s t o w n  with score 5.411113739013672\n",
      "Predicted word is: r e a n  with score 5.427787780761719\n",
      "Predicted word is: s t o n n e  with score 5.5598273277282715\n",
      "\n",
      "Actual word is: p r o c e s s  \n",
      "---\n",
      "Predicted word is: p r o c e s s  with score 0.026246272027492523\n",
      "Predicted word is: p r o d e s  with score 5.870446681976318\n",
      "Predicted word is: t r a c h i n  with score 6.075150966644287\n",
      "Predicted word is: p r o t e s s  with score 6.090655326843262\n",
      "Predicted word is: p r o d e s s  with score 6.356378555297852\n",
      "\n",
      "Actual word is: t h e  \n",
      "---\n",
      "Predicted word is: t h e  with score 0.0004572269681375474\n",
      "Predicted word is: t h  with score 9.038466453552246\n",
      "Predicted word is: h h  with score 9.3350248336792\n",
      "Predicted word is: n h e  with score 9.627456665039062\n",
      "Predicted word is: h h m  with score 9.925104141235352\n",
      "\n",
      "Actual word is: n e t w o r k  \n",
      "---\n",
      "Predicted word is: n e t w o r k  with score 0.011007474735379219\n",
      "Predicted word is: n e t w r r c y  with score 7.053821563720703\n",
      "Predicted word is: n a t e r i z e  with score 7.402794361114502\n",
      "Predicted word is: n e t w r r c e  with score 7.552586555480957\n",
      "Predicted word is: n a t e r i t t  with score 7.774384021759033\n",
      "\n",
      "Actual word is: n e t w o r k  \n",
      "---\n",
      "Predicted word is: n e t w o r k  with score 0.011007474735379219\n",
      "Predicted word is: n e t w r r c y  with score 7.053821563720703\n",
      "Predicted word is: n a t e r i z e  with score 7.402794361114502\n",
      "Predicted word is: n e t w r r c e  with score 7.552586555480957\n",
      "Predicted word is: n a t e r i t t  with score 7.774384021759033\n",
      "\n",
      "Actual word is: s e c o n d  \n",
      "---\n",
      "Predicted word is: s e c o n d  with score 0.0204647034406662\n",
      "Predicted word is: t h i r d p  with score 5.297338485717773\n",
      "Predicted word is: t h i r d t  with score 6.231442451477051\n",
      "Predicted word is: s e c t  with score 7.1735334396362305\n",
      "Predicted word is: t h i r d f e e  with score 8.538747787475586\n",
      "\n",
      "Actual word is: c o v e r  \n",
      "---\n",
      "Predicted word is: c o v e r  with score 1.0320167541503906\n",
      "Predicted word is: s t h e t  with score 3.0232410430908203\n",
      "Predicted word is: c o v e r e r  with score 3.5693676471710205\n",
      "Predicted word is: c o v e r u e  with score 3.7495737075805664\n",
      "Predicted word is: s t o k  with score 3.9835586547851562\n",
      "\n",
      "Actual word is: t o  \n",
      "---\n",
      "Predicted word is: t o  with score 0.002189942402765155\n",
      "Predicted word is: c o  with score 7.37384033203125\n",
      "Predicted word is: n o  with score 7.4571075439453125\n",
      "Predicted word is: d o  with score 8.928506851196289\n",
      "Predicted word is: t h  with score 8.998828887939453\n",
      "\n",
      "Actual word is: a l l  \n",
      "---\n",
      "Predicted word is: a l l  with score 0.2348816841840744\n",
      "Predicted word is: o l l  with score 2.2009007930755615\n",
      "Predicted word is: a b l e  with score 4.321646690368652\n",
      "Predicted word is: a l h  with score 4.493587017059326\n",
      "Predicted word is: a l o  with score 4.7957329750061035\n",
      "\n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: o f  with score 0.0006058525759726763\n",
      "Predicted word is: o u c t  with score 8.679265975952148\n",
      "Predicted word is: o f c  with score 9.021984100341797\n",
      "Predicted word is: o o  with score 9.345407485961914\n",
      "Predicted word is: o u  with score 9.528753280639648\n",
      "\n",
      "Actual word is: f o r  \n",
      "---\n",
      "Predicted word is: f o r  with score 0.0034711030311882496\n",
      "Predicted word is: b o d e  with score 7.284790515899658\n",
      "Predicted word is: a o d  with score 7.509518146514893\n",
      "Predicted word is: f o r m  with score 7.69878625869751\n",
      "Predicted word is: t o m m  with score 8.304365158081055\n",
      "\n",
      "Actual word is: f r o m  \n",
      "---\n",
      "Predicted word is: f r o m  with score 0.0046320087276399136\n",
      "Predicted word is: f f o m  with score 7.078866481781006\n",
      "Predicted word is: f r o m a  with score 7.596268653869629\n",
      "Predicted word is: o r m a n  with score 7.691491603851318\n",
      "Predicted word is: f  with score 8.947467803955078\n",
      "\n",
      "Actual word is: o n  \n",
      "---\n",
      "Predicted word is: o n  with score 0.0030640929471701384\n",
      "Predicted word is: o f  with score 6.763558864593506\n",
      "Predicted word is: i n  with score 7.711165428161621\n",
      "Predicted word is: o b  with score 8.076739311218262\n",
      "Predicted word is: l a t r  with score 8.528383255004883\n"
     ]
    }
   ],
   "source": [
    "print_examples(char_s2s, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to watch out for floating point overflow!\n",
    "\n",
    "The right spelling seems to be in there - but the scores are a bit whiffy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predicted word is: a d d r e s s e s \n",
      "Actual word is: a d d r e s s e s  \n",
      "---\n",
      "Predicted word is: n a t i v e \n",
      "Actual word is: n a t i v e  \n",
      "---\n",
      "Predicted word is: o f \n",
      "Actual word is: o f  \n",
      "---\n",
      "Predicted word is: w h e r e i n \n",
      "Actual word is: w h e r e i n  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "char_s2s.example_output(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(28/29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[], 1.0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[list(), 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,4] + [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 4\n",
    "a -= 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(4778/2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2278"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4778 % 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.605170185988091"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.90775528 4.82831374 3.61191841]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.1, 0.2, 0.3])\n",
    "b = np.log(a)\n",
    "c = 3.0 * -b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use argsort where -c reverses array and :3 takes top 3 items to return top 2 indices\n",
    "np.argsort(-c)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0] [6 5]\n"
     ]
    }
   ],
   "source": [
    "# Use argsort where -c reverses array and :3 takes top 3 items to return top 2 indices\n",
    "a1 = np.array([5, 2, 6, -1, 3])\n",
    "d = np.argsort(-a1)[:2]\n",
    "print(d, a1[d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a1[d[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.zeros(shape=(4))\n",
    "e[1:4] = a\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((d, d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
