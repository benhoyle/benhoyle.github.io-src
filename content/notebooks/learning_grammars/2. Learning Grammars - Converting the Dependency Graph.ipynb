{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat\n",
    "\n",
    "The discussions below are primarily-based on English, as that is my first language. \n",
    "\n",
    "I know that some assumptions may not apply to all languages. However, I am concentrating first on text generation in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we want to convert to a phrase-based grammar?\n",
    "\n",
    "Because this allows us to model the different options available for different chunks of speech. For example, we can have the following rules:\n",
    "\n",
    "* VP > VERB\n",
    "* VP > VERB ADV\n",
    "* VP > ADV VERB\n",
    "\n",
    "Or:\n",
    "* S > NP VP\n",
    "* S > NP VP NP\n",
    "* S > NP VP NP VP\n",
    "* S > NP VP CC S\n",
    "\n",
    "The dependency graph has single chains of words. The difficulty is knowing how to partition and group these chains. For example, we know that NOUN > DET is regularly repeated, but how do we know it should be treated as a modular unit NP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences and Things\n",
    "\n",
    "Thinking about language use, functional grammars seem useful to seed our sentences. For example, lanaguage is about \"things\" and \"happenings\", which at a simple level are mapped onto nouns and verbs. The reason the dependency graph takes a verb as its root is that a verb is seen to be a prequisite for a sentence.\n",
    "\n",
    "Looking at how langauge developed in my children, it is my belief that \"things\" have a slight primacy over \"happening\". A child's first words tend to be nouns: \"bus\", \"dog\", \"mama\". It's may be 6-12 months later than verbs come along: \"get drink\", \"go there\".\n",
    "\n",
    "When looking at text generation, we need to *condition* the generation on *something*. A sentence needs to be about something. In formal sentence, the subject of a sentence appears to have primacy over the object, as subject-verb make grammatical sense (\"The cat sat\") and the object appears to provide further information. Object-verb pairs tend to occur as commands or imperative phrases (\"Sit here\"). While common in speech, they are less common in formal writing. (We could test this by looking at distributions across our patent data.)\n",
    "\n",
    "Indeed, it appears that in many cases, the *things* in our sentence determine the verbs that are used. For example, if we have \"cat\" and \"mat\" this limits the relations between them (i.e. \"swam\" seems an unusual choice). The sequence of generation thus seems to be something along the lines of:\n",
    "\n",
    "* S > SUBJ\n",
    "* SUBJ > OBJ\n",
    "* SUBJ > VERB\n",
    "* SUBJ, OBJ > VERB\n",
    "* SUBJ, VERB > OBJ\n",
    "\n",
    "Thinging of *things* fits nicely with ideas of *coverage* and *attention*. In a piece of writing, e.g. a paragraph or document, we expect to cover a set of sentences about certain things. Each sentence may be about a different set of things, which is where attention comes in. However, at a top level, the set of things is reasonably limited. What we do though is we *zoom in* on things to add detail, i.e. we select one thing to be the centre of attention together with a context, and we generate another sentence. This is performed within the constraints of *coverage*:  we know that repetition is bad and should not occur. \n",
    "\n",
    "In phrase-based grammars subjects and objects are represented as noun phrases. Again, this needs investigation: we need to see which phrase-based groups subjects and objects belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "If we know that our sentence is about *things*, where do we get our inspiration from? Or more precisely, how do we know what *things* to write about?\n",
    "\n",
    "Let's have a look at some current applications of natural language generation:\n",
    "\n",
    "* machine translation: the output sentence is about the same things as the input sentence;\n",
    "* summarization: the output sentence is about the things in a larger input document or body of text; and\n",
    "* image captioning: the output sentence is about the things in the image.\n",
    "\n",
    "In each case, we have some form of input which provides the source of *things* that are present in the output sentence.\n",
    "\n",
    "In each case, there is some form of encoding of the input into a continuous multidimensional representation (generally, a vector of 100-300 elements). \n",
    "\n",
    "When training, the model learns a mapping from the input data to the continuous multidimensional representation and then a mapping from the continuous multidimensional representation to an output sentence.\n",
    "\n",
    "Even when attention is applied, this seems to modify the values in the continuous multidimensional representation but does not generally change the form of the representation.\n",
    "\n",
    "Hence, when thinking about lanaguage generation we can build a framework based on a general continuous multidimensional representation, a \"context tensor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will we use the result in generative models?\n",
    "\n",
    "One framework for language generation appears to be the following process:\n",
    "\n",
    "* Receive a context tensor;\n",
    "* Sample a subject representation based on the context tensor;\n",
    "* Given the context tensor and the subject representation, iteratively sample verb and object representations;\n",
    "* Somehow map the SVO sequence to a top-level phrase-based sequence;\n",
    "* Apply learnt rules hierarchically and modularly to provide language portions for each phrase.\n",
    "\n",
    "We also have the following ideas:\n",
    "\n",
    "* At least one level of attention is constant for each sentence, but varies between sentences.\n",
    "* Each sentence should say something different, i.e. we can add a constraint that forces the generation away from a sentence encoding that is close to a previous sentence encoding.\n",
    "* We should be able to take a phrase in the sentence and sample at progressive levels of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to learn rules in a deep learning framework?\n",
    "\n",
    "Grammar rules take an input and produce an output. The input is fixed but the output may vary in length (but can be constrained to generate a binary tree - i.e. input > O1, O2 (two outputs)).\n",
    "\n",
    "If a binary tree is assumed the rules may be modelled with a feed-forward numeral network, where each pair of outputs is a different output symbol.\n",
    "\n",
    "If we assume varying length outputs, then the rules may be modelled with a recurrent neural network. In this case, we supply a single input and multiple outputs are produced until we get a stop token. There are different ways we can apply such a recurrent neural network:\n",
    "\n",
    "* Supply the same input at each time step (this is a one-to-many implemenation); or\n",
    "* Supply the previous output as the input for the next time step (a many-to-many implemenation).\n",
    "\n",
    "We can use the same network to model the rules at the different levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are our tokens in our deep learning model?\n",
    "\n",
    "If we start with \"terminal tokens\", we have parts of speech. \n",
    "\n",
    "Spacy has [the following parts of speech tags](https://spacy.io/api/annotation#pos-tagging):\n",
    "\n",
    "* ADJ\tadjective\tbig, old, green, incomprehensible, first\n",
    "* ADP\tadposition\tin, to, during\n",
    "* ADV\tadverb\tvery, tomorrow, down, where, there\n",
    "* AUX\tauxiliary\tis, has (done), will (do), should (do)\n",
    "* CONJ\tconjunction\tand, or, but\n",
    "* CCONJ\tcoordinating conjunction\tand, or, but\n",
    "* DET\tdeterminer\ta, an, the\n",
    "* INTJ\tinterjection\tpsst, ouch, bravo, hello\n",
    "* NOUN\tnoun\tgirl, cat, tree, air, beauty\n",
    "* NUM\tnumeral\t1, 2017, one, seventy-seven, IV, MMXIV\n",
    "* PART\tparticle\t's, not,\n",
    "* PRON\tpronoun\tI, you, he, she, myself, themselves, somebody\n",
    "* PROPN\tproper noun\tMary, John, Londin, NATO, HBO\n",
    "* PUNCT\tpunctuation\t., (, ), ?\n",
    "* SCONJ\tsubordinating conjunction\tif, while, that\n",
    "* SYM\tsymbol\t```$, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù```\n",
    "* VERB\tverb\trun, runs, running, eat, ate, eating\n",
    "* X\tother\tsfpksdpsxmsa\n",
    "* SPACE\tspace\n",
    "\n",
    "There are thus 19 in total.\n",
    "\n",
    "If we work with patent data, then we will generally have fewer proper nouns and interjections.\n",
    "\n",
    "One of our first tasks is thus to work out the scale of our non-terminal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
