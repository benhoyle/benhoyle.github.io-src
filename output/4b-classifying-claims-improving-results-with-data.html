<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  4B. Classifying Claims - Improving Results with Data | Machine Learning Projects
</title>
  <link rel="canonical" href="./4b-classifying-claims-improving-results-with-data.html">


  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/theme.css">

  
  <meta name="description" content="This post follows on from part 4A and looks at how we can improve our initial results.">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="./">Machine Learning Projects</a></h1>
      <p class="text-muted">A selection of machine learning projects</p>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
  <article class="article">
    <div class="content">
      <h1>4B. Classifying Claims - Improving Results with Data</h1>
<p>This is an additional notebook to implement the "Improving Results with Data" section of part 4A.</p>
<h2>Changing Input Dimensionality</h2>
<p>Let's see if increasing or decreasing the dimensionality of our input data affects our classification results. We can do this by changing the number of words cap that is passed to our tokeniser.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;raw_data.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading data&quot;</span><span class="p">)</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} claims and classifications loaded&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre><span></span>Loading data
11238 claims and classifications loaded
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Create our Y vector as before</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">Y_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>

<span class="c1"># encode class values as integers</span>
<span class="n">label_e</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_e</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y_class</span><span class="p">)</span>
<span class="n">encoded_Y</span> <span class="o">=</span> <span class="n">label_e</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Y_class</span><span class="p">)</span>
<span class="c1"># convert integers to dummy variables (i.e. one hot encoded)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">encoded_Y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our classes are now a matrix of {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Original label: {0}; Converted label: {1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">Y_class</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nt">Our</span> <span class="nt">classes</span> <span class="nt">are</span> <span class="nt">now</span> <span class="nt">a</span> <span class="nt">matrix</span> <span class="nt">of</span> <span class="o">(</span><span class="nt">11238</span><span class="o">,</span> <span class="nt">8</span><span class="o">)</span>
<span class="nt">Original</span> <span class="nt">label</span><span class="o">:</span> <span class="nt">A</span><span class="o">;</span> <span class="nt">Converted</span> <span class="nt">label</span><span class="o">:</span> <span class="cp">[</span><span class="mi">1</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span> <span class="mi">0</span><span class="nx">.</span><span class="cp">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Let&#39;s start with decreasing our dimensionality</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>

<span class="c1"># create the tokenizer</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">2500</span><span class="p">)</span>
<span class="c1"># fit the tokenizer on the documents</span>
<span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our data has the following dimensionality: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Our data has the following dimensionality:  (11238, 2500)
</pre></div>


<p>We are going to leave our SVC classifier for these experiments as it took too long to train.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">MultinomialNB</span><span class="p">(),</span>
    <span class="n">MLPClassifier</span><span class="p">()</span>
<span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># Convert one hot to target integer</span>
<span class="n">Y_integers</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">name</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
    <span class="p">))</span>

    <span class="k">print</span><span class="p">(</span>
        <span class="s2">&quot;Classifier {0} has an average classification accuracy of {1:.2f} ({2:.2f})&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
        <span class="p">)</span>    
    <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Classifier MultinomialNB has an average classification accuracy of 57.04 (0.97)
Classifier MLPClassifier has an average classification accuracy of 60.17 (1.07)
</pre></div>


<p>Here are our results:</p>
<ul>
<li>Classifier MultinomialNB has an average classification accuracy of 57.04 (0.97)</li>
<li>Classifier MLPClassifier has an average classification accuracy of 60.17 (1.07)</li>
</ul>
<p>Comparing with our previous results:</p>
<ul>
<li>Classifier MultinomialNB has an average classification accuracy of 58.90 (1.12)</li>
<li>Classifier MLPClassifier has an average classification accuracy of 61.20 (0.54)</li>
</ul>
<p>We see a slight reduction in accuracy, although we are on or close to the bounds of variance. Reducing our dimensionality does not seem to be the way to proceed.</p>
<p>Even though our SVC classifier performed well in the spot-checks - it takes a very long time to train. For these more general experiments, we will thus limit to the Naive Bayes and the MLP classifier.</p>
<div class="highlight"><pre><span></span><span class="c1"># Now let&#39;s increase our dimensionality</span>

<span class="c1"># Delete our earlier data</span>
<span class="k">del</span> <span class="n">X</span><span class="p">,</span> <span class="n">t</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="c1"># create the tokenizer</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># fit the tokenizer on the documents</span>
<span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our data has the following dimensionality: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># SVC takes too long to train</span>
<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">MultinomialNB</span><span class="p">(),</span>
    <span class="n">MLPClassifier</span><span class="p">()</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">name</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
    <span class="p">))</span>

    <span class="k">print</span><span class="p">(</span>
        <span class="s2">&quot;Classifier {0} has an average classification accuracy of {1:.2f} ({2:.2f})&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
        <span class="p">)</span>    
    <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Our data has the following dimensionality:  (11238, 10000)
Classifier MultinomialNB has an average classification accuracy of 59.26 (1.16)
Classifier MLPClassifier has an average classification accuracy of 62.11 (0.36)
</pre></div>


<p>Results:</p>
<ul>
<li>Classifier MultinomialNB has an average classification accuracy of 59.26 (1.16)</li>
<li>Classifier MLPClassifier has an average classification accuracy of 62.11 (0.36)</li>
</ul>
<p>There does appear to be a small increase in performance (~2% - where variance is ~1%). So more words does help us. However, it does not help as much as we would expect it too (e.g. doubling our number of counted words does not double performance).</p>
<p>This indicates that much of our classification is being performed on a limited set of terms.  </p>
<p>Let's see what another doubling of our dimensionality does...</p>
<div class="highlight"><pre><span></span><span class="c1"># Delete our earlier data</span>
<span class="k">del</span> <span class="n">X</span><span class="p">,</span> <span class="n">t</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="c1"># create the tokenizer</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
<span class="c1"># fit the tokenizer on the documents</span>
<span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our data has the following dimensionality: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># SVC takes too long to train</span>
<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">MultinomialNB</span><span class="p">(),</span>
    <span class="n">MLPClassifier</span><span class="p">()</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">name</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
        <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
    <span class="p">))</span>

    <span class="k">print</span><span class="p">(</span>
        <span class="s2">&quot;Classifier {0} has an average classification accuracy of {1:.2f} ({2:.2f})&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
            <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
        <span class="p">)</span>    
    <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Our data has the following dimensionality:  (11238, 20000)
Classifier MultinomialNB has an average classification accuracy of 59.24 (1.33)
Classifier MLPClassifier has an average classification accuracy of 61.91 (0.43)
</pre></div>


<p>Results:</p>
<ul>
<li>Classifier MultinomialNB has an average classification accuracy of 59.24 (1.33)</li>
<li>Classifier MLPClassifier has an average classification accuracy of 61.91 (0.43)</li>
</ul>
<p>There seems to be a leveling off of performance with increased dimensionality. Here there will always be a trade-off between dimensionality and performance. In this case, as we are only obtaining a small percentage increase, it may be better to use a smaller dimensionality to allow faster classification.</p>
<p>Another aside on speed: the Naive Bayes classifier is much faster to train than the MLP classifier, and the difference in performance is only 1-2%. If we were looking at a production system, there may be a benefit in using the Naive Bayes classifier over the more fancy deep-learning approaches.</p>
<h2>Changing Data Conversion Methods</h2>
<p>The texts_to_matrix method for the text tokeniser has for different modes: "binary", "count", "tfidf", "freq". These are not explained but it is presumed that binary provides just an indication of presence for a word, and count/freq provide un-normalised count data. Looking at the <a href="https://github.com/keras-team/keras/blob/master/keras/preprocessing/text.py">source</a>, freq divides the count c by the length of the sequence. </p>
<div class="highlight"><pre><span></span><span class="n">modes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="s2">&quot;freq&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="n">modes</span><span class="p">:</span>
    <span class="c1"># create the tokenizer</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
    <span class="c1"># fit the tokenizer on the documents</span>
    <span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span>
            <span class="s2">&quot;Mode {0} - Classifier {1} has an average classification accuracy of {2:.2f} ({3:.2f})&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">mode</span><span class="p">,</span>
                <span class="n">name</span><span class="p">,</span> 
                <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
                <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
            <span class="p">)</span>    
        <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Mode binary - Classifier MultinomialNB has an average classification accuracy of 57.07 (0.82)
Mode binary - Classifier MLPClassifier has an average classification accuracy of 59.63 (0.71)
Mode count - Classifier MultinomialNB has an average classification accuracy of 57.53 (0.89)
Mode count - Classifier MLPClassifier has an average classification accuracy of 59.00 (0.63)
Mode freq - Classifier MultinomialNB has an average classification accuracy of 30.82 (0.05)
Mode freq - Classifier MLPClassifier has an average classification accuracy of 57.65 (1.38)
</pre></div>


<p>Results:</p>
<ul>
<li>Mode binary - Classifier MultinomialNB has an average classification accuracy of 57.07 (0.82)</li>
<li>Mode binary - Classifier MLPClassifier has an average classification accuracy of 59.63 (0.71)</li>
<li>Mode count - Classifier MultinomialNB has an average classification accuracy of 57.53 (0.89)</li>
<li>Mode count - Classifier MLPClassifier has an average classification accuracy of 59.00 (0.63)</li>
<li>Mode freq - Classifier MultinomialNB has an average classification accuracy of 30.82 (0.05)</li>
<li>Mode freq - Classifier MLPClassifier has an average classification accuracy of 57.65 (1.38)</li>
</ul>
<p>Some interestings results here. TD-IDF appears the best metric to use. Frequency counts appear to cause problems with both classifiers (the MLP does not converge to a solution after 200 iterations). Using TD-IDF over just count data appears to increase performance by 1-2%.</p>
<hr>
<h2>Increasing Performance with Ensembles</h2>
<p>How do each of our preferred machine learning algorithms build their models? If they each have different strengths and weaknesses we may be able to build an ensemble model that outperforms the individual performance.</p>
<h3>Looking at the Confusion Matrix for Insights</h3>
<p>Luckily scikit-learn provides a <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html">helpful example</a> that shows how to plot a confusion matrix. We will use this code below.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                          <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">,</span>
                          <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function prints and plots the confusion matrix.</span>
<span class="sd">    Normalization can be applied by setting `normalize=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Normalized confusion matrix&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix, without normalization&#39;</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># create the tokenizer</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="c1"># fit the tokenizer on the documents</span>
<span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;tfidf&quot;</span><span class="p">)</span>

<span class="n">NBclf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">MLPclf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">NB_y_pred</span> <span class="o">=</span> <span class="n">NBclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">MLP_y_pred</span> <span class="o">=</span> <span class="n">MLPclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute confusion matrix</span>
<span class="n">NB_cnf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">NB_y_pred</span><span class="p">)</span>
<span class="n">MLP_cnf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">MLP_y_pred</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">]</span>
<span class="c1"># Plot non-normalized confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">NB_cnf_matrix</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix for Naive Bayes, with normalization&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">MLP_cnf_matrix</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix for MLP Classifier, with normalization&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>Normalized confusion matrix
[[0.34 0.14 0.22 0.01 0.11 0.07 0.1  0.01]
 [0.02 0.52 0.11 0.03 0.09 0.13 0.07 0.03]
 [0.08 0.1  0.73 0.   0.01 0.02 0.02 0.04]
 [0.   0.   0.25 0.38 0.25 0.12 0.   0.  ]
 [0.   0.14 0.07 0.02 0.62 0.09 0.05 0.02]
 [0.01 0.12 0.02 0.01 0.08 0.69 0.05 0.03]
 [0.01 0.06 0.04 0.   0.01 0.03 0.7  0.16]
 [0.   0.07 0.05 0.01 0.02 0.05 0.23 0.58]]
Normalized confusion matrix
[[0.63 0.1  0.1  0.   0.   0.03 0.07 0.05]
 [0.09 0.53 0.07 0.01 0.02 0.07 0.11 0.11]
 [0.25 0.12 0.48 0.   0.01 0.01 0.04 0.08]
 [0.25 0.25 0.25 0.   0.12 0.   0.   0.12]
 [0.16 0.19 0.03 0.   0.38 0.12 0.07 0.05]
 [0.12 0.17 0.01 0.   0.01 0.48 0.11 0.11]
 [0.04 0.03 0.02 0.   0.   0.02 0.67 0.21]
 [0.02 0.04 0.02 0.   0.   0.02 0.24 0.66]]
</pre></div>


<p><img alt="png" src="./images/4a_output_19_1.png"></p>
<p><img alt="png" src="./images/4a_output_19_2.png"></p>
<h4>Observations on the Confusion Matrix</h4>
<p>The Naive Bayes classifier performs better than the MLP classifier for Sections C, D, E, F and G. The MLP classifier performs better for Sections A, B, and H. </p>
<p>This suggests that an ensemble of both classifiers may work at improving performance. scikit-learn has a method to use both hard and soft voting <a href="http://scikit-learn.org/stable/modules/ensemble.html">here</a>.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="n">estimators</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">estimators</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;NB&#39;</span><span class="p">,</span> <span class="n">NBclf</span><span class="p">))</span>
<span class="n">estimators</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;MLP&#39;</span><span class="p">,</span> <span class="n">MLPclf</span><span class="p">))</span>
<span class="c1"># create the ensemble model</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">scores</span>
</pre></div>


<div class="highlight"><pre><span></span>array([0.61, 0.59, 0.6 , 0.6 , 0.6 ])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
</pre></div>


<div class="highlight"><pre><span></span>59.89522060313159
</pre></div>


<div class="highlight"><pre><span></span><span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span>
</pre></div>


<div class="highlight"><pre><span></span>0.7295624868552363
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Trying soft voting</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="p">,</span> <span class="n">voting</span> <span class="o">=</span> <span class="s2">&quot;soft&quot;</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ensemble</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_integers</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">scores</span>
</pre></div>


<div class="highlight"><pre><span></span>array([0.61, 0.59, 0.6 , 0.6 , 0.6 ])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">EN_y_pred</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute confusion matrix</span>
<span class="n">EN_cnf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">EN_y_pred</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">]</span>
<span class="c1"># Plot non-normalized confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">EN_cnf_matrix</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix for Ensemble Classifier, with normalization&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty.
  if diff:


Normalized confusion matrix
[[0.37 0.14 0.21 0.01 0.09 0.07 0.1  0.02]
 [0.02 0.52 0.11 0.02 0.09 0.12 0.07 0.04]
 [0.08 0.1  0.73 0.   0.01 0.01 0.02 0.04]
 [0.   0.   0.25 0.38 0.25 0.12 0.   0.  ]
 [0.   0.16 0.07 0.02 0.6  0.09 0.05 0.02]
 [0.02 0.12 0.02 0.01 0.07 0.68 0.06 0.04]
 [0.01 0.06 0.04 0.   0.01 0.02 0.7  0.16]
 [0.   0.06 0.04 0.   0.01 0.05 0.23 0.6 ]]
</pre></div>


<p><img alt="png" src="./images/4a_output_27_2.png"></p>
    </div>
    <hr/>
    <footer>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-03-22T20:03:21.426229+00:00">
          <i class="fa fa-clock-o"></i>
          Thu 22 March 2018
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="./category/claim-classification.html">Claim Classification</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="./author/ben-hoyle.html">Ben Hoyle</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="./tag/improving_results.html">#improving_results</a>          </li>
      </ul>
    </footer>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <p class="col-sm-6 text-sm-left">
    <a href="https://www.linkedin.com/in/benhoyle/" class="text-muted" target="_blank">Ben Hoyle</a>
  </p>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" class="text-muted" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" class="text-muted" target="_blank">Adapted from &#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>
</html>