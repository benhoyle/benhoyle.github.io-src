{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "It is widely agreed that a good starting point for a sequence-to-sequence model is a recurrent neural network and attention. In this post we'll look at applying attention to our current models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Github repository](https://github.com/philipperemy/keras-attention-mechanism) by Philippe RÃ©my contains a great walk through for applying attention in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/philipperemy/keras-attention-mechanism/master/assets/graph_multi_attention.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, as applied by Philippe, is fairly straightforward to implement in Keras. \n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* An input is provided in with the dimensions: BS (batch_size), TS (time_steps), and I (input_dimension). I represents the length of each time step, e.g. if we had a one-hot vector with a length equal to a vocabulary size of 2500, I would be 2500. Time steps are the same as our sequence elements, e.g. if a sequence has a length of 22, time steps = 22. Batch size is the size used to group samples, e.g. for training.\n",
    "* This is followed by a [Permute](https://keras.io/layers/core/#permute) operation. This swaps around the first and second dimensions (excluding the batch size I think). So you have (batch_size, input_dim, time_steps). For example, if we had (None, 22, 2500) this would be swapped to (None, 2500, 22). \n",
    "* As stated in the code the Reshape line is just for housekeeping - the array is aleady switched to being input_dim, time_steps.\n",
    "* We then generate a dense layer, which is just a matrix multiplication with an output of size TS. This gives us a vector the same size as the number of our timesteps. Softmax is applied on the output to normalise as a probability (e.g. values are from 0-1 and sum to 1).\n",
    "* If a single attention vector is required the mean over all dimensions is taken - a single dimension vector may be useful as a simplification when visualising.\n",
    "* Some more housekeeping is then performed to again switch around the first and second dimensions (i.e. switch back to the original form).\n",
    "* Finally we perform a matrix multiplication using our obtained attention vector (a_probs) on the inputs. We then output the result of the multiplication, which is the application of the attention.\n",
    "\n",
    "Our attention weights are thus stored as parameters of the Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge, Permute, Reshape, Dense, Flatten\n",
    "#from keras.layers.core import *\n",
    "#from keras.layers.recurrent import LSTM\n",
    "#from keras.models import *\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention weights can be applied before or after our recurrent neural network (in our initial models an LSTM). When applying our weights after the LSTM we need to flatten the output to get a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philippe also provides a handy little utility to view the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "30000 samples loaded\n",
      "\n",
      "\n",
      "Adding start and stop tokens to output\n",
      "\n",
      "\n",
      "An example title: startseq System and method for session restoration at geo-redundant gateways stopseq\n",
      "----\n",
      "An example claim: \n",
      "1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:\n",
      "periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and\n",
      "in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "PIK = \"claim_and_title.data\"\n",
    "\n",
    "if not os.path.isfile(PIK):\n",
    "    # Download file\n",
    "    !wget https://benhoyle.github.io/notebooks/title_generation/claim_and_title.data\n",
    "\n",
    "with open(PIK, \"rb\") as f:\n",
    "    print(\"Loading data\")\n",
    "    data = pickle.load(f)\n",
    "    print(\"{0} samples loaded\".format(len(data)))\n",
    "    \n",
    "print(\"\\n\\nAdding start and stop tokens to output\")\n",
    "data = [(c, \"startseq {0} stopseq\".format(t)) for c, t in data]\n",
    "                                      \n",
    "print(\"\\n\\nAn example title:\", data[0][1])\n",
    "print(\"----\")\n",
    "print(\"An example claim:\", data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to Ludwig Model with Shared Embedding\n",
    "\n",
    "Our best version of the Ludwig model is the version with shared Glove embeddings. We will apply our attention mechanism to this after the application of the LSTMs. This effectively weights the ?. We'll create a new class and overwrite the build_model method. \n",
    "\n",
    "We need to remember to amend the definition of the LSTM such that sequences are returned. This then enables the correct dimensionality of input for our attention layer. \n",
    "\n",
    "This [post](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/) explains the general functioning of the LSTM cell in Keras and the return of sequences and hidden states.\n",
    "\n",
    "This [post](https://medium.com/datalogue/attention-in-keras-1892773a4f22) explains attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "\n",
    "* Some posts and text books refer to the output of an LSTM as `y`, others as `h`. The hidden state is also sometimes referred to as `h` (e.g. when used with `y`) and at other times `c` (e.g. with `h`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ludwig_model import LudwigModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this Attention Mechanism Works\n",
    "\n",
    "Let's start by considering one sample. We can effectively ignore the first batch size (BS) dimension (0) in our code.\n",
    "\n",
    "The attention mechanism set out above starts with an input from an encoding LSTM. As return sequences is true, the encoding LSTM outputs an array of length time steps (TS), where each time step has an associated encoding vector (I or h). In our case, we have an LSTM with a latent dimension of 128, which outputs encoding vectors of length 128 for each time step. Our Keras LSTM output for each sample is thus a matrix of size (300, 128), i.e. 300 timesteps each having a vector of length 128.\n",
    "\n",
    "A normal Dense layer takes an input vector and maps it to an output vector, e.g. X > Y where X is length L1 and Y is length L2. If we pass a matrix to a Dense layer, e.g. 500 samples of X - of size (500, L1) we would get a matrix of (500, L2) out. This is the same if we are doing things in batches - then dense layer is still applied to the last dimension (e.g. still L1>L2 even if we had batches of 64, e.g. input - (64, 500, L1).\n",
    "\n",
    "When we permute, we swap around our sample. So we have a sample that is the timestep data for each feature (e.g. as how each hidden dimension changes over the timesteps). E.g. `[ft1, ft2, ft3 ... fT]` where T = 22. The output of the dense layer is a (pseudo-) probability across the 22 time steps. I.e. a weighting for each time step for the feature. This is repeated for each feature so we have F_1=`[w1, w2, ... wT]`, F_2=..., F_latent_dim = `[...]` - i.e. 128 features with weights for 22 time steps (128, 22). We then swap the matrix around again, so our weights are arranged in timestep order, e.g. T_1=`[w_f1, w_f2, ..., w_f_latent_dim]` where each column / vector will sum to 1. This gives us a matrix of (22, 128).\n",
    "\n",
    "The later Mulitply operation multiplies these weights against the original LSTM encoding.\n",
    "\n",
    "In descriptions of attention we have a context vector that, for a output timestamp, is the weighted sum of the attention weights and the original encoding. Each encoding (h) has a weight (a).\n",
    "\n",
    "Our issue is that the output of our Multiply operation is a matrix of (128, 22) (as we are applying weights to each individual dimension of the encoding). We need to reduce this down to a vector. The matrix has weighted terms for each latent dimension and each timestamp. We could simplify this by only having one weight term per timestamp in the earlier calculations. ***Or to sum our weights across the time dimensions.*** For this can I just use K.sum on the backend?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, concatenate, merge, Permute, Reshape, \\\n",
    "Flatten, multiply, AveragePooling1D, RepeatVector, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "class LWAtt(LudwigModel):\n",
    "    \"\"\" Version of our Ludwig model with attention.\"\"\"\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\" Build the model. \"\"\"\n",
    "        print(\"Building model\")\n",
    "        self._load_shared_embedding()\n",
    "        # Define inputs\n",
    "        inputs_encoder = Input(shape=(self.encoder_seq_length,))\n",
    "        inputs_decoder = Input(shape=(self.decoder_seq_length,))\n",
    "        \n",
    "        # Define Shared Embedding Layer\n",
    "        Shared_Embedding = Embedding(\n",
    "            output_dim=self.word_embedding_size,\n",
    "            input_dim=self.num_encoder_tokens,\n",
    "            weights=[self.embedding_matrix]   \n",
    "        )\n",
    "        # Ah our problem is that our shared embedding has encoder length but we are also using on decoder\n",
    "        \n",
    "        embedded_inputs_encoder = Shared_Embedding(inputs_encoder)\n",
    "        embedded_inputs_decoder = Shared_Embedding(inputs_decoder)\n",
    "        \n",
    "        # Define LSTMs - these return output state h for each timestep (as we have r_s=True)\n",
    "        encoder_LSTM = LSTM(self.latent_dim, return_sequences=True)\n",
    "        decoder_LSTM = LSTM(self.latent_dim, return_sequences=True)\n",
    "        \n",
    "        # So output of this is, e.g. BS, 300, 128, i.e. an h vector for each TS\n",
    "        encoder_context = encoder_LSTM(embedded_inputs_encoder)\n",
    "        \n",
    "        # Add attention to encoder encodings - here we are swapping the dims to BS, 128, 300\n",
    "        a = Permute((2, 1))(encoder_context)\n",
    "        #a = Reshape((self.latent_dim, self.encoder_seq_length))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "        # Here we apply a dense layer to each a matrix\n",
    "        a = Dense(self.encoder_seq_length, activation='softmax', use_bias=False)(a)\n",
    "        # Single attention vector\n",
    "        # a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        # a = RepeatVector(self.latent_dim)(a)\n",
    "        a_probs = Permute((2, 1), name='attention_vec_a')(a)\n",
    "        att_mul_1 = multiply([encoder_context, a_probs])\n",
    "        \n",
    "        # Sum over time dimension\n",
    "        att_mul_1 = Lambda(lambda x: K.sum(x, axis=1), name=\"sum_over_time_att_1\")(att_mul_1)\n",
    "        # Take the average over our 300 time steps and flatten to 1D\n",
    "        #flatt_mul_1 = Flatten()(AveragePooling1D(pool_size=self.encoder_seq_length, strides=None, padding='valid')(att_mul_1))\n",
    "        # summary input model\n",
    "        decoder_context = decoder_LSTM(embedded_inputs_decoder)\n",
    "         # Add attention to answer encodings\n",
    "        b = Permute((2, 1))(decoder_context)\n",
    "        #b = Reshape((self.latent_dim, self.decoder_seq_length))(b) # this line is not useful. It's just to know which dimension is what.\n",
    "        b = Dense(self.decoder_seq_length, activation='softmax')(b)\n",
    "        # Single attention vector\n",
    "        # b = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(b)\n",
    "        print(b.shape) # - the above is of shape ?, 22\n",
    "        #b = RepeatVector(self.latent_dim)(b) # This repeats the averaged vector across the latent dim to create a matrix of 128,22\n",
    "        b_probs = Permute((2, 1), name='attention_vec_b')(b)\n",
    "        att_mul_2 = multiply([decoder_context, b_probs])\n",
    "        att_mul_2 = Lambda(lambda x: K.sum(x, axis=1), name=\"sum_over_time_att_2\")(att_mul_2)\n",
    "        # Take the average over our 22 time steps and flatten to 1D\n",
    "        #flatt_mul_2 = Flatten()(AveragePooling1D(pool_size=self.decoder_seq_length, strides=None, padding='valid')(att_mul_2))\n",
    "        # Now flatten our attention matrices to get a vector for both sequences\n",
    "        # att_mul_1 = Flatten()(att_mul_1)\n",
    "        # att_mul_2 = Flatten()(att_mul_2)\n",
    "        # decoder output model\n",
    "        decoder1 = concatenate([att_mul_1, att_mul_2], axis=1)\n",
    "        # decoder1 = concatenate([att_mul_1, att_mul_2])\n",
    "        # We do need to flatten our concatenated array to a vector here\n",
    "        # We can either use flatten(), which gives high dimensionality\n",
    "        # Or use an average in the time dimension\n",
    "        #cat_dim = self.encoder_seq_length + self.decoder_seq_length\n",
    "        \n",
    "        #flattened = AveragePooling1D(pool_size=cat_dim, strides=None, padding='valid')(decoder1)\n",
    "        outputs = Dense(self.num_decoder_tokens, activation='softmax')(decoder1)\n",
    "        # tie it together [article, summary] [word]\n",
    "        self.model = Model(inputs=[inputs_encoder, inputs_decoder], outputs=outputs)\n",
    "        self.infdec = self.model\n",
    "        print(\"Compiling model\")\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizers\n",
      "Our input data has shape (30000, 300) and our output data has shape (30000, 22)\n",
      "Generating training and test data\n",
      "Building model\n",
      "Loading GloVe 100d embeddings from file\n",
      "Found 400000 word vectors.\n",
      "Building embedding matrix\n",
      "(?, 128, 22)\n",
      "Compiling model\n",
      "No existing weights found\n"
     ]
    }
   ],
   "source": [
    "machine = LWAtt(\n",
    "    encoder_texts=[d[0] for d in data],\n",
    "    decoder_texts=[d[1] for d in data],\n",
    "    encoder_seq_length=300,\n",
    "    decoder_seq_length=22,\n",
    "    num_encoder_tokens=2500,\n",
    "    num_decoder_tokens=2500,\n",
    "    latent_dim=128,\n",
    "    weights_file=\"class_LWattmodel_sin_att.hdf5\",\n",
    "    training_set_size=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model:\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         multiple             250000      input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 300, 128)     117248      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 22, 128)      117248      embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_12 (Permute)            (None, 128, 300)     0           lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 128, 22)      0           lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128, 300)     90000       permute_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128, 22)      506         permute_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec_a (Permute)       (None, 300, 128)     0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec_b (Permute)       (None, 22, 128)      0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 300, 128)     0           lstm_13[0][0]                    \n",
      "                                                                 attention_vec_a[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 22, 128)      0           lstm_14[0][0]                    \n",
      "                                                                 attention_vec_b[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sum_over_time_att_1 (Lambda)    (None, 128)          0           multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sum_over_time_att_2 (Lambda)    (None, 128)          0           multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           sum_over_time_att_1[0][0]        \n",
      "                                                                 sum_over_time_att_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2500)         642500      concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,217,502\n",
      "Trainable params: 1,217,502\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "machine.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same number of parameters. However, this time our 128 LSTM outputs have been subject to the attention weighting.\n",
    "\n",
    "cf:\n",
    "```\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            (None, 300)          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_2 (InputLayer)            (None, 22)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_1 (Embedding)         (None, 300, 100)     250000      input_1[0][0]                    \n",
    "                                                                 input_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "lstm_1 (LSTM)                   (None, 128)          117248      embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "lstm_2 (LSTM)                   (None, 128)          117248      embedding_1[1][0]                \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, 256)          0           lstm_1[0][0]                     \n",
    "                                                                 lstm_2[0][0]                     \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 2500)         642500      concatenate_1[0][0]              \n",
    "==================================================================================================\n",
    "Total params: 1,126,996\n",
    "Trainable params: 1,126,996\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(machine.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image\n",
    "Image(model_to_dot(machine.model, show_shapes=True).create_png(prog='dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 38s 14ms/step - loss: 3.7534 - val_loss: 3.4038\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 36s 14ms/step - loss: 3.3330 - val_loss: 3.2739\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 3.3971 - val_loss: 3.3970\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 39s 15ms/step - loss: 3.3393 - val_loss: 3.1085\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 39s 16ms/step - loss: 3.2977 - val_loss: 3.5505\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 3.2598 - val_loss: 3.3541\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 38s 16ms/step - loss: 3.1801 - val_loss: 3.1463\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 40s 15ms/step - loss: 3.3285 - val_loss: 3.3301\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 38s 15ms/step - loss: 3.1715 - val_loss: 3.1427\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 44s 17ms/step - loss: 3.1403 - val_loss: 3.1518\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 41s 17ms/step - loss: 3.0927 - val_loss: 2.9610\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 42s 17ms/step - loss: 3.0823 - val_loss: 2.8885\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 40s 16ms/step - loss: 2.9154 - val_loss: 3.0092\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 39s 16ms/step - loss: 2.9821 - val_loss: 2.9993\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 40s 16ms/step - loss: 2.9369 - val_loss: 2.8782\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 39s 15ms/step - loss: 2.9769 - val_loss: 2.8922\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 2.9006 - val_loss: 2.8765\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 2.8783 - val_loss: 2.7867\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 2.8735 - val_loss: 2.9209\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.8660 - val_loss: 3.0078\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 39s 16ms/step - loss: 2.9306 - val_loss: 2.9922\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 39s 15ms/step - loss: 2.8293 - val_loss: 2.9170\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 38s 16ms/step - loss: 2.8519 - val_loss: 3.0365\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 40s 16ms/step - loss: 2.7265 - val_loss: 3.0915\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 41s 16ms/step - loss: 2.8533 - val_loss: 2.8047\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 39s 15ms/step - loss: 2.8442 - val_loss: 2.6748\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 39s 16ms/step - loss: 2.8391 - val_loss: 2.9218\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 41s 16ms/step - loss: 2.9080 - val_loss: 2.9467\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 42s 16ms/step - loss: 2.8851 - val_loss: 2.9306\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 39s 15ms/step - loss: 2.8868 - val_loss: 2.6075\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 2.7658 - val_loss: 2.9623\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 2.7461 - val_loss: 2.8857\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 39s 15ms/step - loss: 2.7981 - val_loss: 2.9200\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 2.7860 - val_loss: 2.6967\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 36s 15ms/step - loss: 2.7836 - val_loss: 2.7377\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 2.8316 - val_loss: 2.5625\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 2.7961 - val_loss: 2.6989\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 38s 15ms/step - loss: 2.7402 - val_loss: 2.6618\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 38s 15ms/step - loss: 2.7475 - val_loss: 2.7302\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 2.6838 - val_loss: 2.5104\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 2.7248 - val_loss: 2.6774\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 42s 16ms/step - loss: 2.7152 - val_loss: 2.7921\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 38s 15ms/step - loss: 2.6605 - val_loss: 2.8077\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465/2465 [==============================] - 38s 15ms/step - loss: 2.7356 - val_loss: 2.8176\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 36s 15ms/step - loss: 2.6346 - val_loss: 2.8013\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 2.7271 - val_loss: 2.6684\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 2.5915 - val_loss: 2.7757\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 39s 15ms/step - loss: 2.6833 - val_loss: 2.5717\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 36s 15ms/step - loss: 2.5971 - val_loss: 2.8874\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 38s 15ms/step - loss: 2.6325 - val_loss: 2.9706\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 2.6057 - val_loss: 2.6794\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 2.6739 - val_loss: 2.5132\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 2.5723 - val_loss: 2.5320\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 2.5742 - val_loss: 2.7109\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 2.6172 - val_loss: 2.6248\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 2.5930 - val_loss: 2.5833\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 2.6509 - val_loss: 2.7309\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 38s 15ms/step - loss: 2.7043 - val_loss: 2.5435\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 2.5634 - val_loss: 2.6111\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 2.6345 - val_loss: 2.7271\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 2.6088 - val_loss: 2.6624\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 37s 15ms/step - loss: 2.6469 - val_loss: 2.6294\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 38s 15ms/step - loss: 2.6623 - val_loss: 2.5261\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 36s 15ms/step - loss: 2.6092 - val_loss: 2.4698\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 2.5395 - val_loss: 2.5805\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 38s 15ms/step - loss: 2.6011 - val_loss: 2.5636\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 38s 15ms/step - loss: 2.5950 - val_loss: 2.6469\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 2.5909 - val_loss: 2.6206\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 2.6502 - val_loss: 2.7335\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 2.5626 - val_loss: 2.6966\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 2.6774 - val_loss: 2.4654\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 38s 15ms/step - loss: 2.5507 - val_loss: 2.7247\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 37s 15ms/step - loss: 2.5262 - val_loss: 2.7083\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 2.4720 - val_loss: 2.6559\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 2.4679 - val_loss: 2.5671\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 2.4786 - val_loss: 2.7057\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 2.6487 - val_loss: 2.8573\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 2.5289 - val_loss: 2.7940\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 39s 15ms/step - loss: 2.5725 - val_loss: 2.6111\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 2.6167 - val_loss: 2.5953\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 2.5245 - val_loss: 2.7498\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 2.5318 - val_loss: 2.3388\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.4623 - val_loss: 2.6604\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 2.4479 - val_loss: 2.4580\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 2.5413 - val_loss: 2.3676\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 41s 15ms/step - loss: 2.4327 - val_loss: 2.6044\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 38s 15ms/step - loss: 2.4993 - val_loss: 2.4743\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 2.6064 - val_loss: 2.7787\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 2.3296 - val_loss: 2.4972\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 37s 15ms/step - loss: 2.4877 - val_loss: 2.6850\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 2.5702 - val_loss: 2.3874\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 2.4809 - val_loss: 2.4518\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 2.4806 - val_loss: 2.5076\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 2.5932 - val_loss: 2.5643\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 2.4631 - val_loss: 2.4784\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 39s 15ms/step - loss: 2.4358 - val_loss: 2.5184\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 an enhanced tracking system over a network comprising a a computing device coupled to the network and including a browser for displaying to a user b a client website coupled to the network having on\n",
      "\n",
      "Predicted title is: method and system for providing a network  \n",
      "Actual title is: enhanced website tracking system and method  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method for components of a motor control system configured to be powered by each of a first and second power supply the method comprising receiving a first line voltage parame\n",
      "\n",
      "Predicted title is: system and method for controlling a vehicle  \n",
      "Actual title is: method and apparatus for a drive unit for multiple applications with varying voltage requirements  \n",
      "---\n",
      "Sample of claim text: 1 a portable electronic device comprising a processor a memory coupled with said processor a display coupled with said processor a housing for said processor said memory and said display wherein said \n",
      "\n",
      "Predicted title is: method and apparatus for controlling a portable device  \n",
      "Actual title is: method and system for changing the power state of a portable electronic device  \n",
      "---\n",
      "Sample of claim text: 1 an automated computerized method for processing an image comprising the steps of providing an image file an array of pixels forming an image in a computer memory forming a spaced corresponding to th\n",
      "\n",
      "Predicted title is: image processing apparatus and method  \n",
      "Actual title is: color scale spaced arrangement for use in an image  \n",
      "---\n",
      "Sample of claim text: 1 a method comprising accessing a backup chain comprising a base backup and a plurality of incremental controlling a length of the backup chain wherein controlling the length comprises specifying at l\n",
      "\n",
      "Predicted title is: method and system for generating a plurality of a plurality of a plurality of a plurality of a plurality of a  \n",
      "Actual title is: method and apparatus for optimizing a backup chain using  \n",
      "---\n",
      "Training for epoch 1\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 39s 15ms/step - loss: 2.5507 - val_loss: 2.4306\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 2.3365 - val_loss: 2.4644\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 37s 14ms/step - loss: 2.4905 - val_loss: 2.6029\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 39s 15ms/step - loss: 2.5051 - val_loss: 2.5743\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 2.3797 - val_loss: 2.8507\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 2.5468 - val_loss: 2.5150\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 36s 15ms/step - loss: 2.4254 - val_loss: 2.3094\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 2.5696 - val_loss: 2.4015\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 2.5000 - val_loss: 2.5638\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 2.5173 - val_loss: 2.4926\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 36s 15ms/step - loss: 2.3844 - val_loss: 2.4345\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 2.5466 - val_loss: 2.2712\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 2.3099 - val_loss: 2.4907\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 37s 15ms/step - loss: 2.3087 - val_loss: 2.4401\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 2.4226 - val_loss: 2.4552\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 38s 15ms/step - loss: 2.4540 - val_loss: 2.3299\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 36s 15ms/step - loss: 2.3675 - val_loss: 2.4014\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 2.3782 - val_loss: 2.5047\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 2.4501 - val_loss: 2.6300\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.3545 - val_loss: 2.6688\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 2.4605 - val_loss: 2.5351\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 37s 15ms/step - loss: 2.3818 - val_loss: 2.6080\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 37s 15ms/step - loss: 2.4347 - val_loss: 2.5376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.3335 - val_loss: 2.7420\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 2.3364 - val_loss: 2.3899\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 38s 15ms/step - loss: 2.5500 - val_loss: 2.3414\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 37s 15ms/step - loss: 2.5098 - val_loss: 2.6371\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 39s 15ms/step - loss: 2.4664 - val_loss: 2.5697\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 39s 15ms/step - loss: 2.4899 - val_loss: 2.5146\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 37s 15ms/step - loss: 2.3831 - val_loss: 2.3174\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 2.4900 - val_loss: 2.4776\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 2.4401 - val_loss: 2.5017\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 39s 15ms/step - loss: 2.3780 - val_loss: 2.5671\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 39s 15ms/step - loss: 2.4198 - val_loss: 2.4338\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 37s 15ms/step - loss: 2.4770 - val_loss: 2.5038\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 39s 15ms/step - loss: 2.4510 - val_loss: 2.2859\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 2.4054 - val_loss: 2.3918\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 38s 15ms/step - loss: 2.3609 - val_loss: 2.3441\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 2.2953 - val_loss: 2.2968\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 36s 15ms/step - loss: 2.3953 - val_loss: 2.1437\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 39s 15ms/step - loss: 2.4358 - val_loss: 2.4468\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 40s 15ms/step - loss: 2.3216 - val_loss: 2.3866\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 37s 15ms/step - loss: 2.4420 - val_loss: 2.3561\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 38s 15ms/step - loss: 2.3632 - val_loss: 2.5016\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 37s 16ms/step - loss: 2.3779 - val_loss: 2.6862\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 39s 16ms/step - loss: 2.3754 - val_loss: 2.4646\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 41s 17ms/step - loss: 2.2990 - val_loss: 2.5679\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 40s 16ms/step - loss: 2.3512 - val_loss: 2.3169\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 37s 15ms/step - loss: 2.2908 - val_loss: 2.6301\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 37s 15ms/step - loss: 2.3202 - val_loss: 2.5426\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 2.3285 - val_loss: 2.5368\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 2.3835 - val_loss: 2.2132\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 2.3112 - val_loss: 2.5062\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 2.4123 - val_loss: 2.4350\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 2.3552 - val_loss: 2.4121\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 2.2583 - val_loss: 2.4385\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 2.3515 - val_loss: 2.4468\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 39s 15ms/step - loss: 2.3559 - val_loss: 2.2495\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 2.3562 - val_loss: 2.4496\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 2.3573 - val_loss: 2.5024\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 2.3233 - val_loss: 2.5334\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 37s 15ms/step - loss: 2.3976 - val_loss: 2.2850\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 36s 15ms/step - loss: 2.2870 - val_loss: 2.3434\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 2.3076 - val_loss: 2.3678\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 2.2265 - val_loss: 2.4369\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2471/2471 [==============================] - 36s 15ms/step - loss: 2.2770 - val_loss: 2.3475\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 38s 15ms/step - loss: 2.2670 - val_loss: 2.4601\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 2.2533 - val_loss: 2.3826\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 2.4341 - val_loss: 2.4093\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 2.2674 - val_loss: 2.4955\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 2.3823 - val_loss: 2.3413\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 36s 15ms/step - loss: 2.2704 - val_loss: 2.4204\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 2.3061 - val_loss: 2.6294\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 2.3009 - val_loss: 2.4694\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 2.2966 - val_loss: 2.4272\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 2.1845 - val_loss: 2.5196\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 2.3484 - val_loss: 2.3586\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 2.2694 - val_loss: 2.5216\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 2.2307 - val_loss: 2.5378\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 2.3325 - val_loss: 2.4213\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 39s 15ms/step - loss: 2.3422 - val_loss: 2.4468\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 2.2778 - val_loss: 2.1174\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.2352 - val_loss: 2.4241\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 2.2222 - val_loss: 2.3228\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 2.3216 - val_loss: 2.2977\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 41s 15ms/step - loss: 2.2679 - val_loss: 2.5154\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 2.2561 - val_loss: 2.2823\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 37s 15ms/step - loss: 2.3281 - val_loss: 2.5390\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 38s 15ms/step - loss: 2.1230 - val_loss: 2.4513\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 2.2808 - val_loss: 2.3400\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 2.4418 - val_loss: 2.2159\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 2.2374 - val_loss: 2.0817\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 2.2251 - val_loss: 2.3636\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 2.2544 - val_loss: 2.4037\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 2.1863 - val_loss: 2.3683\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 38s 15ms/step - loss: 2.2402 - val_loss: 2.4087\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a music selection system in a vehicle comprising at least one information collection unit configured to collect vehicle information vehicle environment information vehicle interior environment infor\n",
      "\n",
      "Predicted title is: method and apparatus for vehicle  \n",
      "Actual title is: music selection system and method in a vehicle  \n",
      "---\n",
      "Sample of claim text: computer comprising hardware and software executing on the hardware establishing an account for non provided by an entity to an account holding user wherein the entity is a unit that has a and separat\n",
      "\n",
      "Predicted title is: system and method for providing a trading trading  \n",
      "Actual title is: conversion transfer of non to in game funds for in game  \n",
      "---\n",
      "Sample of claim text: 1 a maximum power point tracking device for a electric generation system that includes a battery and a dc dc converter the maximum power point tracking device comprising a sampling module configured t\n",
      "\n",
      "Predicted title is: power supply control system and method  \n",
      "Actual title is: maximum power point tracking device for a electric generation system and a tracking method for the same  \n",
      "---\n",
      "Sample of claim text: 1 a system for a vehicle having an on a component of the vehicle the system comprising a configured to a parameter array to whether a procedure is to be performed wherein the parameter array defines a\n",
      "\n",
      "Predicted title is: method and apparatus for determining a vehicle  \n",
      "Actual title is: analysis system  \n",
      "---\n",
      "Sample of claim text: text wherein the recognition results include a plurality of characters of varying size converted from user entered electronic ink displaying the recognition results wherein the plurality of characters\n",
      "\n",
      "Predicted title is: method and apparatus for displaying a display of a display screen  \n",
      "Actual title is: of space between text segments  \n",
      "---\n",
      "Training for epoch 2\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 2.3224 - val_loss: 2.2259\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 2.1123 - val_loss: 2.3352\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2586/2586 [==============================] - 38s 15ms/step - loss: 2.2952 - val_loss: 2.7047\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 40s 15ms/step - loss: 2.2876 - val_loss: 2.3689\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 2.1495 - val_loss: 2.6779\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 2.2951 - val_loss: 2.3304\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 36s 15ms/step - loss: 2.2643 - val_loss: 2.3082\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 2.2473 - val_loss: 2.3206\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 2.1681 - val_loss: 2.4671\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 2.2976 - val_loss: 2.4769\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 2.1904 - val_loss: 2.0565\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 2.2680 - val_loss: 2.2872\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 2.1438 - val_loss: 2.3115\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 38s 15ms/step - loss: 2.2659 - val_loss: 2.4127\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 2.2282 - val_loss: 2.2602\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 38s 15ms/step - loss: 2.2011 - val_loss: 2.3189\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 2.1691 - val_loss: 2.4198\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 2.2431 - val_loss: 2.5137\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 2.1624 - val_loss: 2.5344\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.2383 - val_loss: 2.3209\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 39s 16ms/step - loss: 2.2408 - val_loss: 2.3208\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.1757 - val_loss: 2.4987\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 37s 15ms/step - loss: 2.2414 - val_loss: 2.3635\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 2.1357 - val_loss: 2.4385\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 39s 15ms/step - loss: 2.1800 - val_loss: 2.4446\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 39s 15ms/step - loss: 2.3148 - val_loss: 2.2191\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 37s 15ms/step - loss: 2.1498 - val_loss: 2.5149\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 2.2203 - val_loss: 2.3597\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 39s 15ms/step - loss: 2.3293 - val_loss: 2.3120\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 2.3039 - val_loss: 2.2124\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 2.2049 - val_loss: 2.5353\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 2.2246 - val_loss: 2.4116\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 38s 15ms/step - loss: 2.1411 - val_loss: 2.3843\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 2.2330 - val_loss: 2.2838\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 36s 15ms/step - loss: 2.2157 - val_loss: 2.3743\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 2.2495 - val_loss: 2.1653\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 2.2396 - val_loss: 2.4586\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 38s 15ms/step - loss: 2.2279 - val_loss: 2.2854\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 38s 15ms/step - loss: 2.1900 - val_loss: 2.1868\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 2.1149 - val_loss: 2.0468\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 39s 15ms/step - loss: 2.1765 - val_loss: 2.2705\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 41s 15ms/step - loss: 2.2036 - val_loss: 2.2115\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 37s 15ms/step - loss: 2.2313 - val_loss: 2.3079\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 2.2385 - val_loss: 2.3275\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 36s 15ms/step - loss: 2.2123 - val_loss: 2.4344\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.1684 - val_loss: 2.3204\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 2.1230 - val_loss: 2.3439\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 39s 15ms/step - loss: 2.1668 - val_loss: 2.2115\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 36s 15ms/step - loss: 2.1125 - val_loss: 2.4422\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 37s 15ms/step - loss: 2.1455 - val_loss: 2.3798\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 2.1343 - val_loss: 2.3763\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 2.1686 - val_loss: 2.2200\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 2.1787 - val_loss: 2.3595\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 2.1132 - val_loss: 2.3705\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 2.1359 - val_loss: 2.4605\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 2.1375 - val_loss: 2.4562\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 2.1672 - val_loss: 2.3750\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 40s 15ms/step - loss: 2.2925 - val_loss: 2.2707\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 2.1204 - val_loss: 2.2993\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 2.1832 - val_loss: 2.3722\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 2.1339 - val_loss: 2.3298\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 36s 15ms/step - loss: 2.1832 - val_loss: 2.2244\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 37s 15ms/step - loss: 2.1507 - val_loss: 2.2289\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 2.1584 - val_loss: 2.2793\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 2.0495 - val_loss: 2.3039\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 38s 15ms/step - loss: 2.1420 - val_loss: 2.2934\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 2.0990 - val_loss: 2.4865\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 2.1133 - val_loss: 2.4091\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 2.3036 - val_loss: 2.2914\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 36s 15ms/step - loss: 2.1491 - val_loss: 2.5247\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 2.2430 - val_loss: 2.1539\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 38s 15ms/step - loss: 2.0997 - val_loss: 2.4001\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 2.1060 - val_loss: 2.5504\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 2.1189 - val_loss: 2.3619\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 1.9957 - val_loss: 2.2302\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 2.0909 - val_loss: 2.4495\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 38s 15ms/step - loss: 2.1846 - val_loss: 2.3884\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 38s 15ms/step - loss: 2.0495 - val_loss: 2.5869\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 39s 15ms/step - loss: 2.1271 - val_loss: 2.4243\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 39s 15ms/step - loss: 2.2126 - val_loss: 2.2696\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 39s 15ms/step - loss: 2.1227 - val_loss: 2.4964\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 2.1081 - val_loss: 2.0621\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 2.0689 - val_loss: 2.5277\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 2.0806 - val_loss: 2.4213\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 2.1065 - val_loss: 2.2648\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 2.1027 - val_loss: 2.3062\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 38s 15ms/step - loss: 2.1742 - val_loss: 2.2568\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 2.1384 - val_loss: 2.5099\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2586/2586 [==============================] - 39s 15ms/step - loss: 2.0001 - val_loss: 2.5303\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 38s 15ms/step - loss: 2.1153 - val_loss: 2.3722\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 37s 15ms/step - loss: 2.1752 - val_loss: 2.1959\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 39s 15ms/step - loss: 2.0757 - val_loss: 2.0612\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 2.0869 - val_loss: 2.3929\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 2.1445 - val_loss: 2.2322\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 2.0986 - val_loss: 2.4280\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 40s 16ms/step - loss: 2.0887 - val_loss: 2.4034\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method of storing data regarding parameter values comprising the steps of making a series of measurements of parameter values at times separated by predetermined time intervals processing via a pr\n",
      "\n",
      "Predicted title is: method and apparatus for performing a signal  \n",
      "Actual title is: data storage and transfer  \n",
      "---\n",
      "Sample of claim text: 1 a method to measure connectivity between two way active measurement protocol entities comprising a session server and session in a communication network the method comprising dividing in the session\n",
      "\n",
      "Predicted title is: method and system for message delivery  \n",
      "Actual title is: method for measuring of connectivity between two way active measurement protocol entities  \n",
      "---\n",
      "Sample of claim text: implement an application program which uses one or more of the plurality of functions through one or more of a plurality of application programming interfaces a plurality of service modules executed o\n",
      "\n",
      "Predicted title is: system and method for providing service providers  \n",
      "Actual title is: image forming device information processing method and information processing program  \n",
      "---\n",
      "Sample of claim text: 1 a method of automatically replacing pixels in an image said method comprising identifying a local of pixels around an pixel identifying secondary of pixels calculating feature values of the pixels c\n",
      "\n",
      "Predicted title is: method and apparatus for detecting and using a  \n",
      "Actual title is: automatic replacement of pixels in an image  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method to implement a vector operation on a parallel computer comprising a plurality of compute nodes operatively connected via a first network having a network processing ele\n",
      "\n",
      "Predicted title is: method and apparatus for performing a network  \n",
      "Actual title is: performing a vector operation on a parallel computer having a plurality of compute nodes  \n",
      "---\n",
      "Training for epoch 3\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 41s 15ms/step - loss: 2.1384 - val_loss: 2.1509\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 2.0266 - val_loss: 2.2161\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 2.0594 - val_loss: 2.3233\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 40s 15ms/step - loss: 2.1632 - val_loss: 2.2951\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 39s 15ms/step - loss: 2.0328 - val_loss: 2.6923\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 2.1111 - val_loss: 2.2714\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 36s 15ms/step - loss: 2.0264 - val_loss: 2.2667\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 2.1481 - val_loss: 2.3651\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 2.0953 - val_loss: 2.3616\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 2.1433 - val_loss: 2.4304\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 2.0890 - val_loss: 2.1906\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 2.1234 - val_loss: 2.1358\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 2.0440 - val_loss: 2.3099\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 38s 15ms/step - loss: 2.1262 - val_loss: 2.0871\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 2.0680 - val_loss: 2.1954\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 37s 15ms/step - loss: 2.1012 - val_loss: 2.2045\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 2.0442 - val_loss: 2.1534\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 2.0492 - val_loss: 2.3013\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 40s 15ms/step - loss: 2.0665 - val_loss: 2.3427\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.0700 - val_loss: 2.2480\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 2.1344 - val_loss: 2.3126\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 2.0949 - val_loss: 2.3656\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 36s 15ms/step - loss: 2.1184 - val_loss: 2.4059\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.9649 - val_loss: 2.3955\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 39s 15ms/step - loss: 2.0418 - val_loss: 2.2843\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2574/2574 [==============================] - 38s 15ms/step - loss: 2.1539 - val_loss: 2.0839\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 37s 15ms/step - loss: 2.0141 - val_loss: 2.4947\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 2.0927 - val_loss: 2.4430\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 40s 15ms/step - loss: 2.1461 - val_loss: 2.2826\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 2.0771 - val_loss: 2.2364\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 2.1214 - val_loss: 2.3599\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 40s 15ms/step - loss: 2.0878 - val_loss: 2.3943\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 39s 15ms/step - loss: 2.1533 - val_loss: 2.4428\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 39s 15ms/step - loss: 2.1144 - val_loss: 2.2834\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 37s 15ms/step - loss: 2.1401 - val_loss: 2.5269\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 2.0756 - val_loss: 2.2199\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 39s 15ms/step - loss: 2.0897 - val_loss: 2.2055\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 36s 15ms/step - loss: 2.0275 - val_loss: 2.2946\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 2.0149 - val_loss: 2.2368\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 2.0870 - val_loss: 1.8929\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 2.1149 - val_loss: 2.1480\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 40s 15ms/step - loss: 2.0585 - val_loss: 2.3073\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 38s 15ms/step - loss: 2.0819 - val_loss: 2.3896\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 38s 15ms/step - loss: 2.0949 - val_loss: 2.4306\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 37s 15ms/step - loss: 2.0930 - val_loss: 2.3270\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.0325 - val_loss: 2.2919\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 2.0312 - val_loss: 2.3014\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 38s 15ms/step - loss: 2.0115 - val_loss: 2.1230\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 36s 15ms/step - loss: 1.9675 - val_loss: 2.4645\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 38s 15ms/step - loss: 1.9485 - val_loss: 2.6314\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 39s 15ms/step - loss: 2.0021 - val_loss: 2.3558\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 2.0196 - val_loss: 2.1854\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 36s 15ms/step - loss: 1.9464 - val_loss: 2.2431\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 37s 15ms/step - loss: 2.0643 - val_loss: 2.3670\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 2.0178 - val_loss: 2.4485\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.9856 - val_loss: 2.5334\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 2.0792 - val_loss: 2.3154\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 38s 15ms/step - loss: 2.1119 - val_loss: 2.2223\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 2.0361 - val_loss: 2.2870\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 2.0507 - val_loss: 2.3725\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.9968 - val_loss: 2.2420\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 38s 15ms/step - loss: 2.1007 - val_loss: 2.2279\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 36s 15ms/step - loss: 2.0628 - val_loss: 2.1842\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 2.0031 - val_loss: 2.1056\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.9526 - val_loss: 2.2901\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 38s 15ms/step - loss: 1.9744 - val_loss: 2.1603\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 2.0324 - val_loss: 2.2349\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 38s 15ms/step - loss: 2.0024 - val_loss: 2.3477\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 40s 15ms/step - loss: 2.1706 - val_loss: 2.2973\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 2.0034 - val_loss: 2.3894\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 2.0412 - val_loss: 2.2152\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 38s 15ms/step - loss: 2.0022 - val_loss: 2.3838\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.9503 - val_loss: 2.4582\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.9280 - val_loss: 2.2395\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 1.9618 - val_loss: 2.3629\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 1.9610 - val_loss: 2.4700\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 2.0646 - val_loss: 2.3093\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.9156 - val_loss: 2.4095\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 41s 15ms/step - loss: 2.0089 - val_loss: 2.4067\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 2.0554 - val_loss: 2.2804\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 39s 15ms/step - loss: 2.0557 - val_loss: 2.4691\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.9763 - val_loss: 2.1078\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 2.0080 - val_loss: 2.3975\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 2.0065 - val_loss: 2.2906\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 2.0604 - val_loss: 2.2454\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 2.0434 - val_loss: 2.3670\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 1.9706 - val_loss: 2.2651\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 2.0602 - val_loss: 2.4791\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.9085 - val_loss: 2.3878\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 2.0024 - val_loss: 2.2646\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 2.1404 - val_loss: 2.2582\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.9685 - val_loss: 2.1064\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 35s 15ms/step - loss: 1.9521 - val_loss: 2.2174\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.9817 - val_loss: 2.2527\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 37s 15ms/step - loss: 1.9473 - val_loss: 2.2394\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 38s 15ms/step - loss: 1.9432 - val_loss: 2.4082\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a game including a with a plurality of potential selectable elements associated with the and a game generated portal having a message mode for providing information concerning game play \n",
      "\n",
      "Predicted title is: method and system for providing a user to a user  \n",
      "Actual title is: apparatus for providing  \n",
      "---\n",
      "Sample of claim text: 1 a method for providing access to dynamic content the method comprising a receiving by a server a page or portion of a page of content comprising an executable script and at least one static element \n",
      "\n",
      "Predicted title is: method and system for content delivery  \n",
      "Actual title is: method and systems for providing access to dynamic content via static pages  \n",
      "---\n",
      "Sample of claim text: loaded a plurality of second loading portions provided for each sorting destination and a robot configured to the plurality of loaded onto the first loading portion in accordance with the sorting dest\n",
      "\n",
      "Predicted title is: method and system for monitoring a work  \n",
      "Actual title is: robot system robot and sorted article manufacturing method  \n",
      "---\n",
      "Sample of claim text: 1 a network system for managing a print job comprising a printing system comprising a memory for storing computer executable instructions wherein the memory comprises a non transitory computer readabl\n",
      "\n",
      "Predicted title is: printing system and method for printing a print job  \n",
      "Actual title is: systems and methods for managing a print job  \n",
      "---\n",
      "Sample of claim text: 1 a method comprising receiving a indication corresponding to a memory device that includes an array of memory cells wherein each memory cell in the array of memory cells is written to a first state u\n",
      "\n",
      "Predicted title is: memory device and method for memory device  \n",
      "Actual title is: response to detection in a memory device  \n",
      "---\n",
      "Training for epoch 4\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 1.9826 - val_loss: 2.0880\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 1.8235 - val_loss: 2.2142\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.9748 - val_loss: 2.5879\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 40s 15ms/step - loss: 2.0263 - val_loss: 2.2267\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 1.9014 - val_loss: 2.4638\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481/2481 [==============================] - 38s 15ms/step - loss: 2.0390 - val_loss: 2.3195\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 37s 15ms/step - loss: 1.9995 - val_loss: 2.2595\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 2.0446 - val_loss: 2.2788\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.9518 - val_loss: 2.3616\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 2.0656 - val_loss: 2.3839\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 1.9906 - val_loss: 2.1422\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 37s 15ms/step - loss: 2.0030 - val_loss: 2.1362\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 1.8256 - val_loss: 2.1769\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 38s 15ms/step - loss: 1.9371 - val_loss: 2.2376\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 37s 15ms/step - loss: 1.9206 - val_loss: 2.2068\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 38s 15ms/step - loss: 2.0276 - val_loss: 2.1887\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 1.9421 - val_loss: 2.2152\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.9281 - val_loss: 2.4507\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 2.0029 - val_loss: 2.4880\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.8705 - val_loss: 2.3326\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 2.0009 - val_loss: 2.2629\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.9189 - val_loss: 2.2592\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 37s 15ms/step - loss: 2.0101 - val_loss: 2.3137\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.8543 - val_loss: 2.2907\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 39s 15ms/step - loss: 1.9258 - val_loss: 2.3521\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 39s 15ms/step - loss: 2.0746 - val_loss: 2.0908\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 37s 15ms/step - loss: 1.9550 - val_loss: 2.2947\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 1.9527 - val_loss: 2.4083\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 40s 15ms/step - loss: 2.0451 - val_loss: 2.2845\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.9528 - val_loss: 2.3056\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 2.0014 - val_loss: 2.4071\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 1.9861 - val_loss: 2.3587\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 39s 15ms/step - loss: 1.9846 - val_loss: 2.5035\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 1.9973 - val_loss: 2.2899\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 37s 15ms/step - loss: 1.9196 - val_loss: 2.4273\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 1.9353 - val_loss: 2.1094\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 1.9826 - val_loss: 2.3486\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 38s 15ms/step - loss: 1.9832 - val_loss: 2.2654\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 2.0029 - val_loss: 2.1501\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 1.9354 - val_loss: 1.9249\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 1.9498 - val_loss: 2.1838\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 41s 15ms/step - loss: 2.0076 - val_loss: 2.2705\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 37s 15ms/step - loss: 1.9850 - val_loss: 2.3776\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 2.0386 - val_loss: 2.3421\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 37s 15ms/step - loss: 1.9892 - val_loss: 2.3301\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.9504 - val_loss: 2.3594\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.8805 - val_loss: 2.2562\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 39s 15ms/step - loss: 1.9224 - val_loss: 2.1608\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2395/2395 [==============================] - 36s 15ms/step - loss: 1.9589 - val_loss: 2.2240\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 38s 15ms/step - loss: 1.8790 - val_loss: 2.4197\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 1.8884 - val_loss: 2.3789\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.9196 - val_loss: 2.1776\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 1.8642 - val_loss: 2.3242\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 37s 15ms/step - loss: 1.9214 - val_loss: 2.2620\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.9321 - val_loss: 2.4006\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.8832 - val_loss: 2.3440\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 1.9549 - val_loss: 2.2681\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 39s 15ms/step - loss: 2.0162 - val_loss: 2.1438\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 1.8779 - val_loss: 2.1442\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.9351 - val_loss: 2.1413\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.8645 - val_loss: 2.3058\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 37s 15ms/step - loss: 1.9857 - val_loss: 2.1520\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 37s 15ms/step - loss: 1.9566 - val_loss: 2.2214\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 2.0134 - val_loss: 2.1818\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 1.8373 - val_loss: 2.3462\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 37s 15ms/step - loss: 1.9896 - val_loss: 2.2862\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 1.9307 - val_loss: 2.3815\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 38s 15ms/step - loss: 1.8544 - val_loss: 2.4151\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 2.0663 - val_loss: 2.3371\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 36s 15ms/step - loss: 1.9119 - val_loss: 2.3225\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.8868 - val_loss: 2.1738\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 38s 15ms/step - loss: 1.9139 - val_loss: 2.3884\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.9318 - val_loss: 2.5512\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.9374 - val_loss: 2.2339\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 1.8606 - val_loss: 2.3940\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.8576 - val_loss: 2.5207\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 1.9744 - val_loss: 2.3577\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.8074 - val_loss: 2.4678\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 1.9327 - val_loss: 2.4197\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 39s 15ms/step - loss: 1.9364 - val_loss: 2.3591\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 39s 15ms/step - loss: 1.9096 - val_loss: 2.4678\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.9614 - val_loss: 1.9462\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.9293 - val_loss: 2.3026\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 1.8590 - val_loss: 2.3365\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 1.8823 - val_loss: 2.0704\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 1.9420 - val_loss: 2.4200\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 1.8561 - val_loss: 2.2343\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 39s 15ms/step - loss: 1.9406 - val_loss: 2.5068\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 38s 15ms/step - loss: 1.8076 - val_loss: 2.1583\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 1.9070 - val_loss: 2.2575\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 1.9861 - val_loss: 2.2651\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2557/2557 [==============================] - 39s 15ms/step - loss: 1.8234 - val_loss: 2.1103\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 35s 15ms/step - loss: 1.8862 - val_loss: 2.2971\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.9173 - val_loss: 2.3723\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 1.8432 - val_loss: 2.2497\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 38s 15ms/step - loss: 1.8190 - val_loss: 2.3745\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method for search phrases comprising obtaining one or more subject terms and one or more terms relating to the one or more subject terms from title information of information by combining at least\n",
      "\n",
      "Predicted title is: method and system for searching search results  \n",
      "Actual title is: method and system for search phrases  \n",
      "---\n",
      "Sample of claim text: 1 a tactile presentation apparatus comprising a plurality of tactile cells wherein the number of the plurality of tactile cells is equal to the number of pixels of a depth image and each of the plural\n",
      "\n",
      "Predicted title is: tactile feedback for electronic devices  \n",
      "Actual title is: tactile presentation apparatus tactile cell and method for controlling tactile presentation apparatus  \n",
      "---\n",
      "Sample of claim text: 1 an image processing apparatus that generates print data based on image data included in a print job for a driving recording head that can ink with a plurality of colors including black and that can \n",
      "\n",
      "Predicted title is: image processing apparatus and method for processing image  \n",
      "Actual title is: image processing apparatus and apparatus  \n",
      "---\n",
      "Sample of claim text: and an coupled to each of the set of to each of the set of to the rack each coupled to the rack and to a respective one of the to provide movement of the respective one of the relative to the rack to \n",
      "\n",
      "Predicted title is: method and system for determining a portion of an aircraft in a display system  \n",
      "Actual title is: adaptive information handling system rack  \n",
      "---\n",
      "Sample of claim text: 1 an apparatus for creating a digital work the apparatus comprising one or more processors and one or more memories operatively coupled to at least one of the one or more processors and containing ins\n",
      "\n",
      "Predicted title is: method and apparatus for providing a digital content  \n",
      "Actual title is: digital rights management of content when content is a future live event  \n",
      "---\n",
      "Training for epoch 5\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 1.9717 - val_loss: 2.0705\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 1.8104 - val_loss: 2.1182\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.9023 - val_loss: 2.4456\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 39s 15ms/step - loss: 1.9389 - val_loss: 2.0408\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 1.8337 - val_loss: 2.5772\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 1.9652 - val_loss: 2.3291\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 37s 15ms/step - loss: 1.8499 - val_loss: 2.2603\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.9637 - val_loss: 2.3072\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.8478 - val_loss: 2.4371\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.8868 - val_loss: 2.4877\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 1.8173 - val_loss: 2.2856\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.8799 - val_loss: 2.1870\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 1.8493 - val_loss: 2.2727\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 38s 15ms/step - loss: 1.8803 - val_loss: 2.2977\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.9014 - val_loss: 2.3425\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 37s 15ms/step - loss: 1.9016 - val_loss: 2.2783\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 1.8597 - val_loss: 2.1587\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 1.8405 - val_loss: 2.3789\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 1.8508 - val_loss: 2.5719\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.8013 - val_loss: 2.4116\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 1.8265 - val_loss: 2.1648\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.7942 - val_loss: 2.3735\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 38s 15ms/step - loss: 1.9187 - val_loss: 2.4220\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.7793 - val_loss: 2.3902\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.8374 - val_loss: 2.3905\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 38s 15ms/step - loss: 1.9723 - val_loss: 2.1885\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 38s 15ms/step - loss: 1.9327 - val_loss: 2.4957\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 39s 15ms/step - loss: 1.8968 - val_loss: 2.4759\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2603/2603 [==============================] - 39s 15ms/step - loss: 1.9637 - val_loss: 2.3198\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 39s 15ms/step - loss: 1.8730 - val_loss: 2.1403\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 1.8717 - val_loss: 2.4447\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 1.8586 - val_loss: 2.3745\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 39s 15ms/step - loss: 1.8834 - val_loss: 2.4168\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 39s 15ms/step - loss: 1.9126 - val_loss: 2.1871\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 37s 15ms/step - loss: 1.8707 - val_loss: 2.4435\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 39s 15ms/step - loss: 1.8569 - val_loss: 2.1931\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 37s 15ms/step - loss: 1.8724 - val_loss: 2.3497\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 38s 15ms/step - loss: 1.8844 - val_loss: 2.3154\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 1.8137 - val_loss: 1.9671\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 36s 15ms/step - loss: 1.8316 - val_loss: 1.9749\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 1.8391 - val_loss: 1.9716\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 41s 15ms/step - loss: 1.8224 - val_loss: 2.3174\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 36s 15ms/step - loss: 1.8983 - val_loss: 2.2552\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 38s 15ms/step - loss: 1.9016 - val_loss: 2.4475\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 35s 15ms/step - loss: 1.8513 - val_loss: 2.4015\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.8006 - val_loss: 2.4037\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.8627 - val_loss: 2.2188\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 38s 15ms/step - loss: 1.8061 - val_loss: 2.1496\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 35s 15ms/step - loss: 1.8555 - val_loss: 2.5129\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 38s 15ms/step - loss: 1.7612 - val_loss: 2.2483\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 1.7788 - val_loss: 2.3576\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.8692 - val_loss: 2.1236\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 1.8396 - val_loss: 2.3676\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 37s 15ms/step - loss: 1.8227 - val_loss: 2.3233\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.8251 - val_loss: 2.3431\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 38s 15ms/step - loss: 1.7643 - val_loss: 2.4132\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 1.8331 - val_loss: 2.2869\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 39s 15ms/step - loss: 1.9406 - val_loss: 2.3065\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 1.8049 - val_loss: 2.2379\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.8187 - val_loss: 2.4006\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.8010 - val_loss: 2.3555\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 36s 15ms/step - loss: 1.8606 - val_loss: 2.2860\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 37s 15ms/step - loss: 1.8548 - val_loss: 2.1614\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 1.8185 - val_loss: 2.0303\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.7202 - val_loss: 2.2905\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 38s 15ms/step - loss: 1.7974 - val_loss: 2.2512\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 1.8250 - val_loss: 2.3661\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 1.8437 - val_loss: 2.1976\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.9304 - val_loss: 2.3056\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.8399 - val_loss: 2.2212\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.9236 - val_loss: 2.2249\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2452/2452 [==============================] - 37s 15ms/step - loss: 1.8360 - val_loss: 2.4494\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.8007 - val_loss: 2.4485\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 38s 15ms/step - loss: 1.7686 - val_loss: 2.2811\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 1.7539 - val_loss: 2.4462\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.7884 - val_loss: 2.5240\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 1.8687 - val_loss: 2.2355\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.7728 - val_loss: 2.7163\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 1.7767 - val_loss: 2.4583\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 1.8452 - val_loss: 2.2255\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.9018 - val_loss: 2.4369\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.8575 - val_loss: 2.0059\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.7469 - val_loss: 2.4070\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 38s 15ms/step - loss: 1.7712 - val_loss: 2.4440\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 36s 15ms/step - loss: 1.7917 - val_loss: 2.1926\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 41s 15ms/step - loss: 1.8007 - val_loss: 2.4109\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 1.9069 - val_loss: 2.2532\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 1.8526 - val_loss: 2.4431\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.7999 - val_loss: 2.3986\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 1.7727 - val_loss: 2.2358\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 1.8753 - val_loss: 2.1525\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.7581 - val_loss: 2.0152\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 1.7061 - val_loss: 2.1085\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.8163 - val_loss: 2.2249\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 35s 15ms/step - loss: 1.7957 - val_loss: 2.2910\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 39s 15ms/step - loss: 1.7599 - val_loss: 2.4290\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method for performing multi in a mobile terminal having a touch screen the method comprising displaying a currently operating application window on the touch screen and simultaneously displaying o\n",
      "\n",
      "Predicted title is: method and apparatus for mobile terminal  \n",
      "Actual title is: apparatus and method for performing multi  \n",
      "---\n",
      "Sample of claim text: 1 a multi processor system comprising a plurality of processor elements and a network that is constructed in a mesh topology and the plurality of processor elements wherein the network includes a plur\n",
      "\n",
      "Predicted title is: method and apparatus for performing a remote of a network  \n",
      "Actual title is: processor system with mesh topology comprising local cache storing for each data information indicating redundancy in router cache for cache management  \n",
      "---\n",
      "Sample of claim text: 1 an energy force for a safety device as part of a vehicle system comprising a first component and a second component that is relative to the first component the force including an arrangement that pr\n",
      "\n",
      "Predicted title is: system and method for determining a pressure control device to a motor vehicle  \n",
      "Actual title is: force  \n",
      "---\n",
      "Sample of claim text: 1 a method comprising receiving data indicative of a point on a two dimensional image wherein the point is associated with a desired object accessing using a processor a three dimensional location com\n",
      "\n",
      "Predicted title is: method and apparatus for image segmentation  \n",
      "Actual title is: alternate viewpoint image enhancement  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method of generating a template map and a similarity metric used to determine a degree of visual similarity of two digital objects comprising storing a set of raw object templ\n",
      "\n",
      "Predicted title is: method and system for creating and displaying objects  \n",
      "Actual title is: learning of template map and similarity metric for object identification  \n",
      "---\n",
      "Training for epoch 6\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 1.8758 - val_loss: 2.2168\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 38s 15ms/step - loss: 1.6847 - val_loss: 2.3146\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.7580 - val_loss: 2.5128\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 39s 15ms/step - loss: 1.8855 - val_loss: 2.0950\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 1.6997 - val_loss: 2.6752\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 39s 16ms/step - loss: 1.7502 - val_loss: 2.2916\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 37s 15ms/step - loss: 1.7681 - val_loss: 2.2841\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.8297 - val_loss: 2.2973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.7645 - val_loss: 2.4645\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 1.8502 - val_loss: 2.5789\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 1.7554 - val_loss: 2.3899\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.7861 - val_loss: 2.2814\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.6978 - val_loss: 2.3737\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 38s 15ms/step - loss: 1.7324 - val_loss: 1.9690\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 1.8299 - val_loss: 2.2320\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 37s 15ms/step - loss: 1.7631 - val_loss: 2.1439\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 1.8263 - val_loss: 2.1962\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 1.7143 - val_loss: 2.3300\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 40s 15ms/step - loss: 1.8049 - val_loss: 2.4700\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.7258 - val_loss: 2.2563\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 39s 15ms/step - loss: 1.7778 - val_loss: 2.2915\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.7734 - val_loss: 2.3864\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 37s 15ms/step - loss: 1.8487 - val_loss: 2.1526\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.6978 - val_loss: 2.4116\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 1.7360 - val_loss: 2.1995\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 39s 15ms/step - loss: 1.9275 - val_loss: 2.1722\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 38s 15ms/step - loss: 1.8014 - val_loss: 2.3638\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 39s 15ms/step - loss: 1.7577 - val_loss: 2.5044\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 39s 15ms/step - loss: 1.8578 - val_loss: 2.3997\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 39s 15ms/step - loss: 1.7679 - val_loss: 2.1965\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.8086 - val_loss: 2.3568\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 1.7913 - val_loss: 2.3245\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 38s 15ms/step - loss: 1.7368 - val_loss: 2.4562\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 38s 15ms/step - loss: 1.8556 - val_loss: 2.3746\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 36s 15ms/step - loss: 1.8308 - val_loss: 2.4849\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 1.7484 - val_loss: 2.2070\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 1.7821 - val_loss: 2.3358\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 37s 15ms/step - loss: 1.8059 - val_loss: 2.3409\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 1.7580 - val_loss: 2.2359\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 38s 16ms/step - loss: 1.7767 - val_loss: 1.9712\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 1.7978 - val_loss: 1.9543\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 42s 15ms/step - loss: 1.7630 - val_loss: 2.3621\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 39s 16ms/step - loss: 1.8026 - val_loss: 2.3677\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 1.8000 - val_loss: 2.3639\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 37s 15ms/step - loss: 1.7918 - val_loss: 2.3544\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.7383 - val_loss: 2.2558\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.7495 - val_loss: 2.3530\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 39s 15ms/step - loss: 1.7494 - val_loss: 1.9819\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 36s 15ms/step - loss: 1.7301 - val_loss: 2.3742\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 38s 15ms/step - loss: 1.7353 - val_loss: 2.4982\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 39s 15ms/step - loss: 1.6265 - val_loss: 2.3484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.7128 - val_loss: 2.0988\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 1.6980 - val_loss: 2.3369\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 37s 15ms/step - loss: 1.6912 - val_loss: 2.4283\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.7422 - val_loss: 2.1557\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.6861 - val_loss: 2.4559\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 1.7762 - val_loss: 2.4225\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 38s 15ms/step - loss: 1.8417 - val_loss: 2.2724\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 1.7253 - val_loss: 2.2712\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.7395 - val_loss: 2.3661\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.7395 - val_loss: 2.3254\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 38s 15ms/step - loss: 1.6968 - val_loss: 2.0561\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 36s 15ms/step - loss: 1.7522 - val_loss: 2.3112\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.7740 - val_loss: 2.1548\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 1.6415 - val_loss: 2.3686\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 37s 15ms/step - loss: 1.7229 - val_loss: 2.4126\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 38s 15ms/step - loss: 1.7460 - val_loss: 2.3269\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 1.7603 - val_loss: 2.2933\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.7910 - val_loss: 2.2718\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 36s 15ms/step - loss: 1.6870 - val_loss: 2.4974\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.8182 - val_loss: 2.2136\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 38s 15ms/step - loss: 1.7207 - val_loss: 2.5059\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.7443 - val_loss: 2.4534\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.7219 - val_loss: 2.3221\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 37s 15ms/step - loss: 1.6816 - val_loss: 2.2814\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 1.7040 - val_loss: 2.4455\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 1.7234 - val_loss: 2.3221\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.6420 - val_loss: 2.5667\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 1.6862 - val_loss: 2.3530\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.7618 - val_loss: 2.4154\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 39s 15ms/step - loss: 1.7907 - val_loss: 2.4620\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 1.7853 - val_loss: 1.9530\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.6793 - val_loss: 2.4805\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 38s 15ms/step - loss: 1.7593 - val_loss: 2.3760\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 1.7437 - val_loss: 2.2768\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 1.7128 - val_loss: 2.3942\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 1.7380 - val_loss: 2.3449\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 1.7878 - val_loss: 2.4737\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.6870 - val_loss: 2.5028\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 1.7412 - val_loss: 2.2199\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 37s 15ms/step - loss: 1.7961 - val_loss: 2.2084\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.6864 - val_loss: 2.0569\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 1.7356 - val_loss: 2.3231\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.7100 - val_loss: 2.3093\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 1.6939 - val_loss: 2.3479\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 39s 15ms/step - loss: 1.6179 - val_loss: 2.3963\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method for operating a working machine provided with a hydraulic system for moving an implement on the working machine and or for steering the working machine and an operator control means which i\n",
      "\n",
      "Predicted title is: method and system for controlling a machine machine  \n",
      "Actual title is: method and a system for operating a working machine  \n",
      "---\n",
      "Sample of claim text: 1 a method for training a learning based classifier comprising generating a plurality of positive training samples and a plurality of negative training samples based on training images extracting a pl\n",
      "\n",
      "Predicted title is: method and apparatus for classifying and decoding  \n",
      "Actual title is: method and system for learning based object detection in medical images  \n",
      "---\n",
      "Sample of claim text: 1 a method implemented by a computing device the method comprising identifying an image that includes material determined to be to a child the image included in one of a plurality of frames that form \n",
      "\n",
      "Predicted title is: method and system for managing content  \n",
      "Actual title is: image recognition of content  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method comprising under the control of one or more computer systems configured with executable instructions receiving from a requesting entity a data store request to store a \n",
      "\n",
      "Predicted title is: data storage system and method  \n",
      "Actual title is: data identification  \n",
      "---\n",
      "Sample of claim text: 1 a subject tracking device comprising an input unit that sequentially inputs images obtained through imaging operation each as an input image an arithmetic operation unit that calculates first simila\n",
      "\n",
      "Predicted title is: image processing device method and program for processing target image  \n",
      "Actual title is: subject tracking device and camera  \n",
      "---\n",
      "Training for epoch 7\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 1.7623 - val_loss: 2.1489\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 37s 15ms/step - loss: 1.6743 - val_loss: 2.5087\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 38s 15ms/step - loss: 1.7141 - val_loss: 2.4825\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 40s 15ms/step - loss: 1.7578 - val_loss: 2.2416\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 39s 15ms/step - loss: 1.6548 - val_loss: 2.6867\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 38s 15ms/step - loss: 1.7525 - val_loss: 2.3288\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 36s 15ms/step - loss: 1.6849 - val_loss: 2.2159\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.7345 - val_loss: 2.3475\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.7237 - val_loss: 2.4847\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 1.7676 - val_loss: 2.4052\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 1.6731 - val_loss: 2.1655\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.7945 - val_loss: 2.3041\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 38s 15ms/step - loss: 1.6750 - val_loss: 2.2918\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 36s 15ms/step - loss: 1.6454 - val_loss: 2.1626\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.6553 - val_loss: 2.2597\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 38s 15ms/step - loss: 1.7218 - val_loss: 2.1144\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 1.7038 - val_loss: 2.1336\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.6666 - val_loss: 2.2339\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 40s 15ms/step - loss: 1.6888 - val_loss: 2.5842\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 37s 15ms/step - loss: 1.6558 - val_loss: 2.3014\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 1.6961 - val_loss: 2.2636\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 39s 16ms/step - loss: 1.6876 - val_loss: 2.4722\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 37s 15ms/step - loss: 1.7406 - val_loss: 2.4832\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.6381 - val_loss: 2.3153\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.6897 - val_loss: 2.2047\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 38s 15ms/step - loss: 1.7802 - val_loss: 2.1673\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 38s 15ms/step - loss: 1.7063 - val_loss: 2.3644\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 1.6877 - val_loss: 2.3979\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 39s 15ms/step - loss: 1.7650 - val_loss: 2.3899\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.6999 - val_loss: 2.2841\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2537/2537 [==============================] - 37s 15ms/step - loss: 1.7243 - val_loss: 2.4982\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 40s 15ms/step - loss: 1.6771 - val_loss: 2.3241\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 38s 15ms/step - loss: 1.7520 - val_loss: 2.5261\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 1.7072 - val_loss: 2.3001\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 36s 15ms/step - loss: 1.7177 - val_loss: 2.6276\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 40s 15ms/step - loss: 1.7446 - val_loss: 2.1708\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 1.7420 - val_loss: 2.3835\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 36s 15ms/step - loss: 1.6577 - val_loss: 2.3449\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 1.6813 - val_loss: 2.1676\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 1.6999 - val_loss: 2.0847\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 1.7061 - val_loss: 2.2874\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 39s 14ms/step - loss: 1.7479 - val_loss: 2.2885\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 38s 15ms/step - loss: 1.7836 - val_loss: 2.5124\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 1.7415 - val_loss: 2.5379\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 36s 15ms/step - loss: 1.6785 - val_loss: 2.4659\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.7164 - val_loss: 2.3545\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.6562 - val_loss: 2.4678\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 39s 15ms/step - loss: 1.6626 - val_loss: 2.1685\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 37s 15ms/step - loss: 1.6858 - val_loss: 2.2879\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 37s 15ms/step - loss: 1.6514 - val_loss: 2.3192\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 1.6425 - val_loss: 2.4414\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.6355 - val_loss: 2.2074\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 36s 15ms/step - loss: 1.6048 - val_loss: 2.3090\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 1.6576 - val_loss: 2.4620\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.6176 - val_loss: 2.3260\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 38s 15ms/step - loss: 1.6011 - val_loss: 2.4032\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 1.7301 - val_loss: 2.4233\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 39s 15ms/step - loss: 1.7641 - val_loss: 2.2323\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 38s 15ms/step - loss: 1.5786 - val_loss: 2.2424\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.6968 - val_loss: 2.3612\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.6584 - val_loss: 2.3706\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 38s 15ms/step - loss: 1.6950 - val_loss: 2.1865\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 36s 15ms/step - loss: 1.7166 - val_loss: 2.2581\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 38s 15ms/step - loss: 1.6438 - val_loss: 2.0596\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 1.6200 - val_loss: 2.3430\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 37s 15ms/step - loss: 1.6238 - val_loss: 2.3622\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 1.7212 - val_loss: 2.2661\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 1.6146 - val_loss: 2.4004\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.8091 - val_loss: 2.4118\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.6721 - val_loss: 2.3200\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.7071 - val_loss: 2.1545\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 36s 15ms/step - loss: 1.7304 - val_loss: 2.3235\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.6874 - val_loss: 2.5123\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.6292 - val_loss: 2.4093\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 38s 15ms/step - loss: 1.6186 - val_loss: 2.3403\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 36s 15ms/step - loss: 1.6435 - val_loss: 2.6449\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 36s 15ms/step - loss: 1.6946 - val_loss: 2.3160\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.5668 - val_loss: 2.4831\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 1.6311 - val_loss: 2.3738\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.7679 - val_loss: 2.2278\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.7051 - val_loss: 2.6715\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.6203 - val_loss: 2.0057\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 38s 15ms/step - loss: 1.6613 - val_loss: 2.4818\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 1.6206 - val_loss: 2.3723\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 1.6884 - val_loss: 2.3279\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 1.6577 - val_loss: 2.3622\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 38s 15ms/step - loss: 1.6853 - val_loss: 2.2610\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 37s 15ms/step - loss: 1.6901 - val_loss: 2.5848\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.5434 - val_loss: 2.4517\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 39s 15ms/step - loss: 1.6224 - val_loss: 2.2297\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 38s 15ms/step - loss: 1.7262 - val_loss: 2.1799\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.6101 - val_loss: 2.0985\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 35s 15ms/step - loss: 1.6524 - val_loss: 2.3384\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.5795 - val_loss: 2.3668\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 36s 15ms/step - loss: 1.6447 - val_loss: 2.3062\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 39s 15ms/step - loss: 1.6030 - val_loss: 2.3259\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method for providing an user interface at a display device the method comprising a plurality of software according to tasks to be performed by a software application the tasks being identified by \n",
      "\n",
      "Predicted title is: user interface for supporting a user interface  \n",
      "Actual title is: command user interface for displaying selectable software functionality controls  \n",
      "---\n",
      "Sample of claim text: 1 a method of decoding a two dimensional color barcode comprising capturing an image of the two dimensional color barcode having a plurality of layers of encoded data with a three channel image captur\n",
      "\n",
      "Predicted title is: method and system for decoding digital  \n",
      "Actual title is: color for mobile applications a per channel framework  \n",
      "---\n",
      "Sample of claim text: 1 a computing system implemented method for managing user data extraction templates using weighted ranking score analysis comprising the following which when executed individually or by any set of one\n",
      "\n",
      "Predicted title is: system and method for generating a statistical document for use in a data set  \n",
      "Actual title is: method and system for managing user data extraction templates using weighted ranking score analysis  \n",
      "---\n",
      "Sample of claim text: 1 a method for generating a transformed connection space for spectral data the method comprising accessing a first connection space for spectral data in a full spectral space accessing a first map whi\n",
      "\n",
      "Predicted title is: method and apparatus for generating a component differential space  \n",
      "Actual title is: generating a transformed connection space for spectral data  \n",
      "---\n",
      "Sample of claim text: 1 a method for running a probe based system at high speed on a digital computer the method comprising providing a computer programmed to perform the steps of converting probe information of a probe in\n",
      "\n",
      "Predicted title is: method and system for managing and maintaining a surface in a data center  \n",
      "Actual title is: method for fast multi dimensional pattern recognition  \n",
      "---\n",
      "Training for epoch 8\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 39s 15ms/step - loss: 1.6933 - val_loss: 2.1090\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 37s 15ms/step - loss: 1.5834 - val_loss: 2.2704\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.6228 - val_loss: 2.5954\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 38s 15ms/step - loss: 1.7387 - val_loss: 2.3588\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 1.6002 - val_loss: 2.6293\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 37s 15ms/step - loss: 1.7166 - val_loss: 2.3293\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 37s 15ms/step - loss: 1.6265 - val_loss: 2.3537\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.6891 - val_loss: 2.3615\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.6665 - val_loss: 2.3330\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.6804 - val_loss: 2.5025\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 37s 15ms/step - loss: 1.5742 - val_loss: 2.3063\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.6900 - val_loss: 2.3100\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.5628 - val_loss: 2.1538\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 37s 15ms/step - loss: 1.6507 - val_loss: 2.1759\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 39s 15ms/step - loss: 1.5936 - val_loss: 2.2428\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 38s 15ms/step - loss: 1.6371 - val_loss: 2.2777\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 36s 15ms/step - loss: 1.6252 - val_loss: 2.2153\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.6023 - val_loss: 2.2667\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 1.6477 - val_loss: 2.5266\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.5563 - val_loss: 2.2754\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 1.6124 - val_loss: 2.3727\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.6543 - val_loss: 2.4348\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 36s 15ms/step - loss: 1.6892 - val_loss: 2.3752\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.5200 - val_loss: 2.3574\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.6232 - val_loss: 2.3847\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 37s 14ms/step - loss: 1.6850 - val_loss: 2.1370\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 38s 15ms/step - loss: 1.6323 - val_loss: 2.3459\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 1.6344 - val_loss: 2.5058\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 40s 15ms/step - loss: 1.7312 - val_loss: 2.3951\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 37s 15ms/step - loss: 1.6143 - val_loss: 2.1728\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 1.6510 - val_loss: 2.3296\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 40s 15ms/step - loss: 1.6397 - val_loss: 2.3373\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 38s 15ms/step - loss: 1.6301 - val_loss: 2.5085\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 1.6599 - val_loss: 2.3017\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 36s 15ms/step - loss: 1.6638 - val_loss: 2.4917\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 38s 15ms/step - loss: 1.6518 - val_loss: 2.1747\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 1.6916 - val_loss: 2.3927\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 37s 15ms/step - loss: 1.6404 - val_loss: 2.2954\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 38s 15ms/step - loss: 1.5470 - val_loss: 2.1554\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 36s 15ms/step - loss: 1.5908 - val_loss: 2.1046\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 39s 15ms/step - loss: 1.6472 - val_loss: 2.1552\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 41s 15ms/step - loss: 1.5808 - val_loss: 2.3603\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 37s 15ms/step - loss: 1.6675 - val_loss: 2.2172\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 1.6674 - val_loss: 2.5647\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 37s 15ms/step - loss: 1.5962 - val_loss: 2.4170\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 36s 15ms/step - loss: 1.5828 - val_loss: 2.2939\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.6297 - val_loss: 2.3048\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 38s 15ms/step - loss: 1.6503 - val_loss: 2.0337\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 35s 15ms/step - loss: 1.6736 - val_loss: 2.4979\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 37s 15ms/step - loss: 1.6106 - val_loss: 2.2901\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 38s 15ms/step - loss: 1.5836 - val_loss: 2.3326\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.6104 - val_loss: 2.3215\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445/2445 [==============================] - 36s 15ms/step - loss: 1.6197 - val_loss: 2.3233\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 1.5903 - val_loss: 2.3548\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.5610 - val_loss: 2.3323\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 38s 15ms/step - loss: 1.5515 - val_loss: 2.5894\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 39s 15ms/step - loss: 1.6150 - val_loss: 2.6002\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 38s 15ms/step - loss: 1.7237 - val_loss: 2.1936\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 37s 15ms/step - loss: 1.5857 - val_loss: 2.2672\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 39s 15ms/step - loss: 1.5789 - val_loss: 2.5203\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.5936 - val_loss: 2.3227\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 37s 15ms/step - loss: 1.5925 - val_loss: 2.1847\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 36s 15ms/step - loss: 1.6642 - val_loss: 2.2353\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.6048 - val_loss: 2.3008\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.5376 - val_loss: 2.4997\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 37s 15ms/step - loss: 1.5285 - val_loss: 2.5373\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 38s 15ms/step - loss: 1.6389 - val_loss: 2.4217\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 1.5693 - val_loss: 2.4669\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.7154 - val_loss: 2.4166\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.6271 - val_loss: 2.4192\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.5934 - val_loss: 2.2256\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 37s 15ms/step - loss: 1.5257 - val_loss: 2.4717\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.6008 - val_loss: 2.5424\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.6168 - val_loss: 2.2509\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 37s 15ms/step - loss: 1.5661 - val_loss: 2.4415\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.6021 - val_loss: 2.5825\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 37s 15ms/step - loss: 1.6107 - val_loss: 2.3468\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.4691 - val_loss: 2.5685\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 40s 15ms/step - loss: 1.6119 - val_loss: 2.5009\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 1.6791 - val_loss: 2.5458\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.6577 - val_loss: 2.4812\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.5925 - val_loss: 1.9885\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.5786 - val_loss: 2.4467\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 39s 15ms/step - loss: 1.5448 - val_loss: 2.5458\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 37s 15ms/step - loss: 1.6037 - val_loss: 2.2512\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 40s 15ms/step - loss: 1.6192 - val_loss: 2.4523\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 37s 15ms/step - loss: 1.6167 - val_loss: 2.3637\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 1.6764 - val_loss: 2.4761\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 38s 15ms/step - loss: 1.5701 - val_loss: 2.3702\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 38s 15ms/step - loss: 1.6070 - val_loss: 2.3514\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 36s 15ms/step - loss: 1.6497 - val_loss: 2.2814\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.5321 - val_loss: 2.1720\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 35s 15ms/step - loss: 1.5672 - val_loss: 2.3392\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.5926 - val_loss: 2.2710\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 35s 15ms/step - loss: 1.5726 - val_loss: 2.3934\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593/2593 [==============================] - 39s 15ms/step - loss: 1.6046 - val_loss: 2.4217\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method comprising obtaining one or more historical contextual values from one or more the one or more historical contextual values corresponding to historical instances where a device action was p\n",
      "\n",
      "Predicted title is: system and method for providing an integrated user to an integrated development environment  \n",
      "Actual title is: context based device action prediction  \n",
      "---\n",
      "Sample of claim text: 1 a method of operating a touch screen device having a touch screen having a first area and a second area the method comprising displaying a plurality of menu images corresponding to a plurality of in\n",
      "\n",
      "Predicted title is: method and system for displaying a screen screen  \n",
      "Actual title is: touch screen device and operating method thereof  \n",
      "---\n",
      "Sample of claim text: loaded at the first web browser to the second web browser at the second via the instant messaging connection loading the web page at the second web browser in response to receipt of the web page addre\n",
      "\n",
      "Predicted title is: web page book generation  \n",
      "Actual title is: instant messaging with browser collaboration  \n",
      "---\n",
      "Sample of claim text: 1 method for measuring geometric to an anatomical system based on two two dimensional images of the anatomical system of a patient comprising a receiving registration data comprising at least one set \n",
      "\n",
      "Predicted title is: method and system for reconstructing an image using segmentation  \n",
      "Actual title is: measurement of geometric to an anatomical system  \n",
      "---\n",
      "Sample of claim text: 1 a method for a computer system to an input from a user and generate a response comprising receiving a user input converting the user input into an input array comprising rows and columns having a pl\n",
      "\n",
      "Predicted title is: method and system for classifying and classifying  \n",
      "Actual title is: system  \n",
      "---\n",
      "Training for epoch 9\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 1.6407 - val_loss: 2.2607\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 36s 15ms/step - loss: 1.4798 - val_loss: 2.4271\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 38s 15ms/step - loss: 1.6386 - val_loss: 2.6348\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 39s 15ms/step - loss: 1.6124 - val_loss: 2.3390\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 38s 15ms/step - loss: 1.5163 - val_loss: 2.7570\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 37s 15ms/step - loss: 1.6591 - val_loss: 2.2724\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 37s 15ms/step - loss: 1.5644 - val_loss: 2.4202\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.6670 - val_loss: 2.3852\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 37s 15ms/step - loss: 1.5233 - val_loss: 2.3980\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.6584 - val_loss: 2.4891\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 36s 15ms/step - loss: 1.4994 - val_loss: 2.2040\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.5581 - val_loss: 2.1700\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.4942 - val_loss: 2.2827\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 37s 15ms/step - loss: 1.5643 - val_loss: 2.2639\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 38s 15ms/step - loss: 1.5849 - val_loss: 2.2585\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 39s 15ms/step - loss: 1.6028 - val_loss: 2.3511\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 37s 15ms/step - loss: 1.5470 - val_loss: 2.2805\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.5524 - val_loss: 2.4634\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 41s 15ms/step - loss: 1.5219 - val_loss: 2.6574\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 38s 15ms/step - loss: 1.5384 - val_loss: 2.2717\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 38s 15ms/step - loss: 1.5352 - val_loss: 2.4764\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 39s 15ms/step - loss: 1.5748 - val_loss: 2.5811\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 36s 15ms/step - loss: 1.6528 - val_loss: 2.4987\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.4910 - val_loss: 2.5019\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 38s 15ms/step - loss: 1.5546 - val_loss: 2.3204\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 37s 14ms/step - loss: 1.6302 - val_loss: 2.1359\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 36s 15ms/step - loss: 1.5728 - val_loss: 2.4389\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 38s 15ms/step - loss: 1.5999 - val_loss: 2.4245\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 40s 15ms/step - loss: 1.6647 - val_loss: 2.2694\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.5732 - val_loss: 2.3557\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 38s 15ms/step - loss: 1.5516 - val_loss: 2.4993\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 38s 15ms/step - loss: 1.5931 - val_loss: 2.5030\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 39s 15ms/step - loss: 1.6167 - val_loss: 2.5450\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 40s 15ms/step - loss: 1.6069 - val_loss: 2.3126\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 35s 15ms/step - loss: 1.6490 - val_loss: 2.5232\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 39s 15ms/step - loss: 1.5631 - val_loss: 2.1616\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 38s 15ms/step - loss: 1.5223 - val_loss: 2.4680\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 36s 15ms/step - loss: 1.5633 - val_loss: 2.4423\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 39s 15ms/step - loss: 1.5677 - val_loss: 2.1718\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 37s 15ms/step - loss: 1.5810 - val_loss: 1.9503\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 40s 15ms/step - loss: 1.5458 - val_loss: 2.0941\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 40s 15ms/step - loss: 1.5327 - val_loss: 2.4799\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 37s 15ms/step - loss: 1.5569 - val_loss: 2.5313\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 37s 15ms/step - loss: 1.5913 - val_loss: 2.4108\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 36s 15ms/step - loss: 1.5427 - val_loss: 2.5290\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.5477 - val_loss: 2.3694\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 37s 15ms/step - loss: 1.5279 - val_loss: 2.4939\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 38s 15ms/step - loss: 1.5394 - val_loss: 2.3048\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 36s 15ms/step - loss: 1.5978 - val_loss: 2.3676\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 37s 15ms/step - loss: 1.4989 - val_loss: 2.4319\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 39s 15ms/step - loss: 1.5202 - val_loss: 2.3390\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 36s 15ms/step - loss: 1.5376 - val_loss: 2.1785\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 37s 15ms/step - loss: 1.4974 - val_loss: 2.3824\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 36s 15ms/step - loss: 1.5372 - val_loss: 2.5151\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 37s 15ms/step - loss: 1.5158 - val_loss: 2.3685\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 38s 15ms/step - loss: 1.5045 - val_loss: 2.5006\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 38s 15ms/step - loss: 1.5796 - val_loss: 2.4884\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 39s 15ms/step - loss: 1.6287 - val_loss: 2.4571\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 37s 15ms/step - loss: 1.5191 - val_loss: 2.3538\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 37s 15ms/step - loss: 1.5417 - val_loss: 2.4042\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 38s 15ms/step - loss: 1.5330 - val_loss: 2.5215\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 37s 15ms/step - loss: 1.6168 - val_loss: 2.1212\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 37s 15ms/step - loss: 1.5909 - val_loss: 2.2348\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 37s 15ms/step - loss: 1.5413 - val_loss: 2.2849\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 37s 15ms/step - loss: 1.4693 - val_loss: 2.3539\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 37s 15ms/step - loss: 1.5569 - val_loss: 2.4280\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 39s 15ms/step - loss: 1.5898 - val_loss: 2.4348\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 37s 15ms/step - loss: 1.4884 - val_loss: 2.4200\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.6121 - val_loss: 2.4813\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 37s 15ms/step - loss: 1.4840 - val_loss: 2.4573\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 39s 15ms/step - loss: 1.5730 - val_loss: 2.2977\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 37s 15ms/step - loss: 1.5786 - val_loss: 2.4116\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 38s 15ms/step - loss: 1.5596 - val_loss: 2.5450\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 39s 15ms/step - loss: 1.5528 - val_loss: 2.3129\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 37s 15ms/step - loss: 1.5371 - val_loss: 2.3979\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.5206 - val_loss: 2.6160\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 38s 15ms/step - loss: 1.5692 - val_loss: 2.3697\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 39s 15ms/step - loss: 1.4512 - val_loss: 2.6953\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 39s 15ms/step - loss: 1.6049 - val_loss: 2.4932\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 37s 15ms/step - loss: 1.5835 - val_loss: 2.4037\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 38s 15ms/step - loss: 1.5629 - val_loss: 2.5385\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 38s 15ms/step - loss: 1.5305 - val_loss: 1.9995\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 37s 15ms/step - loss: 1.4903 - val_loss: 2.5906\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 38s 15ms/step - loss: 1.5176 - val_loss: 2.4541\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 38s 15ms/step - loss: 1.5112 - val_loss: 2.2509\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 39s 15ms/step - loss: 1.5118 - val_loss: 2.3104\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 39s 15ms/step - loss: 1.5442 - val_loss: 2.5546\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 38s 15ms/step - loss: 1.6228 - val_loss: 2.7167\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 39s 15ms/step - loss: 1.5160 - val_loss: 2.5101\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 38s 15ms/step - loss: 1.4732 - val_loss: 2.4145\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 37s 15ms/step - loss: 1.5770 - val_loss: 2.2675\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 38s 15ms/step - loss: 1.4905 - val_loss: 2.2147\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 36s 15ms/step - loss: 1.5089 - val_loss: 2.4507\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 38s 15ms/step - loss: 1.5520 - val_loss: 2.3397\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 35s 15ms/step - loss: 1.5238 - val_loss: 2.4096\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 38s 15ms/step - loss: 1.5299 - val_loss: 2.5543\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a computer implemented method of determining whether applications are similar comprising receiving by a computer source code for a plurality of applications associating for each application semantic\n",
      "\n",
      "Predicted title is: method and apparatus for generating a web page  \n",
      "Actual title is: systems and methods for finding project related information by clustering applications into related concept categories  \n",
      "---\n",
      "Sample of claim text: 1 a method comprising receiving an email recipient input name from an email sender as an input to an email client application to identify an intended email recipient determining if the email recipient\n",
      "\n",
      "Predicted title is: email proxy email service  \n",
      "Actual title is: name resolution in email  \n",
      "---\n",
      "Sample of claim text: 1 a network management system which causes a user device to receive alternate content different from was requested by the user device the user device being connected to the network management system t\n",
      "\n",
      "Predicted title is: network management system and method  \n",
      "Actual title is: systems and methods for providing content and services on a network system  \n",
      "---\n",
      "Sample of claim text: 1 a method for transferring usage information from a third party operated transport to a transport existing user account associated with a first authority comprising transferring from a computer assoc\n",
      "\n",
      "Predicted title is: method and system for managing authority relating to a plurality of items  \n",
      "Actual title is: transferring data from a third party operated transport to a user account  \n",
      "---\n",
      "Sample of claim text: 1 an audio system for an information handling system the audio system comprising an audio interface configured to provide a connection between a mobile information handling device and a port an audio \n",
      "\n",
      "Predicted title is: audio output system and method for transmitting audio and music and audio output output  \n",
      "Actual title is: audio system for an information handling system  \n",
      "---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXd4FNX6x7/vlvSQQBJqgNC7oBQpgqCCiL1xLVzLVbnXcm3Ye+9X78+KDdu1oagoRSyAgFIEpPdO6DUhCSm7e35/zMzuzOzM7Owmm91k38/z5MnszJnZs7sz5z1vPSSEAMMwDMOEwhHrDjAMwzB1AxYYDMMwjC1YYDAMwzC2YIHBMAzD2IIFBsMwDGMLFhgMwzCMLVhgMIwFRFRARIKIXDbaXkNE82qjXwwTC1hgMPUGItpGRJVElKvb/5c86BfEpmfhCR6GiVdYYDD1ja0ALldeEFEPAGmx6w7D1B9YYDD1jU8AXKV6fTWAj9UNiCiLiD4mogNEtJ2IHiIih3zMSUQvEdFBItoC4GyDc98noj1EtIuIniIiZ3U6TETJRPRfItot//2XiJLlY7lENIWIjhLRYSKaq+rrvXIfjhHReiI6vTr9YJhQsMBg6hsLADQgoi7yQH4ZgP/p2rwGIAtAWwCnQhIw18rHbgBwDoATAfQBcInu3A8BeAC0l9uMAHB9Nfv8IID+AHoB6AmgH4CH5GPjABQCyAPQBMADAAQRdQJwC4C+QohMAGcC2FbNfjCMJSwwmPqIomUMB7AWwC7lgEqI3C+EOCaE2AbgPwD+LjcZDeC/QoidQojDAJ5VndsEwCgAtwshSoUQ+wG8Il+vOlwJ4AkhxH4hxAEAj6v6UwWgGYDWQogqIcRcIRWA8wJIBtCViNxCiG1CiM3V7AfDWMICg6mPfALgCgDXQGeOApALwA1gu2rfdgAt5O3mAHbqjim0ls/dI5uIjgJ4G0Djava3uUF/msvbLwLYBOAnItpCRPcBgBBiE4DbATwGYD8RfUFEzcEwUYQFBlPvEEJsh+T8HgXgG93hg5Bm7a1V+1ohoIXsAdBSd0xhJ4AKALlCiGz5r4EQols1u7zboD+75c9yTAgxTgjRFsB5AO5UfBVCiM+EEKfI5woAz1ezHwxjCQsMpr5yHYDThBCl6p1CCC+AiQCeJqJMImoN4E4E/BwTAdxKRPlE1BDAfapz9wD4CcB/iKgBETmIqB0RnRpGv5KJKEX15wDwOYCHiChPDgl+ROkPEZ1DRO2JiAAUQTJF+YioExGdJjvHywEcB+AL8ztimLBggcHUS4QQm4UQi00O/xtAKYAtAOYB+AzABPnYuwBmAFgOYCmCNZSrACQBWAPgCICvIfkY7FICaXBX/k4D8BSAxQBWAFgpv+9TcvsOAH6Rz5sP4E0hxCxI/ovnIGlMeyGZxe4Pox8MEzbECygxDMMwdmANg2EYhrEFCwyGYRjGFiwwGIZhGFuwwGAYhmFsUecqZ+bm5oqCgoJYd4NhGKZOsWTJkoNCiLzqXKPOCYyCggIsXmwWLckwDMMYQUTbQ7eyhk1SDMMwjC1YYDAMwzC2YIHBMAzD2KLO+TCMqKqqQmFhIcrLy2PdlaiTkpKC/Px8uN3uWHeFYZgEo14IjMLCQmRmZqKgoABSjbb6iRAChw4dQmFhIdq0aRPr7jAMk2DUC5NUeXk5cnJy6rWwAAAiQk5OTkJoUgzDxB/1QmAAqPfCQiFRPifDMPFHvREYoSiv8mJvUTmqvLxkAMMwTCQklMDYf6wcXl/Nl3M/evQo3nzzzbDPGzVqFI4ePVrj/WEYhokGCSMwFENONFb/MBMYHo/H8rxp06YhOzs7Cj1iGIapeepFlJQtFNt/FCTGfffdh82bN6NXr15wu91ISUlBw4YNsW7dOmzYsAEXXHABdu7cifLyctx2220YO3YsgECZk5KSEpx11lk45ZRT8Mcff6BFixaYPHkyUlNTa76zDMMwEVLvBMbjP6zGmt3FQfu9PoHyKi9Sk5xwhOk47tq8AR49t5vp8eeeew6rVq3CsmXLMHv2bJx99tlYtWqVP/R1woQJaNSoEY4fP46+ffvi4osvRk5OjuYaGzduxOeff453330Xo0ePxqRJkzBmzJiw+skwDBNN6p3AiAf69eunyZN49dVX8e233wIAdu7ciY0bNwYJjDZt2qBXr14AgN69e2Pbtm211l+GYRg71DuBYaYJFB+vwrZDpWjfOANpSdH92Onp6f7t2bNn45dffsH8+fORlpaGoUOHGuZRJCcn+7edTieOHz8e1T4yDMOES+I4vRUXRhR8GJmZmTh27JjhsaKiIjRs2BBpaWlYt24dFixYUPMdYBiGqQXqnYYRC3JycjBo0CB0794dqampaNKkif/YyJEjMX78eHTp0gWdOnVC//79Y9hThmGYyCERjSl3FOnTp4/QL6C0du1adOnSxfK8kvIqbDlYinZ5GUhPrtty0s7nZRiGUUNES4QQfapzjYQxSSnULfHIMAwTPySOwIimE4NhGCYBSBiBEc1Mb4ZhmEQgYQQGwzAMUz1YYDAMwzC2SBiBwS4MhmGY6pEwAiOaRFreHAD++9//oqysrIZ7xDAMU/MkjMCIRXlzO7DAYBimrlC3M9jCguCAQNaRVYC3OZDRJPQpNlGXNx8+fDgaN26MiRMnoqKiAhdeeCEef/xxlJaWYvTo0SgsLITX68XDDz+Mffv2Yffu3Rg2bBhyc3Mxa9asGusTwzBMTVP/BMb0+4C9K4N2JwmBtpUegMoBOICk9OBzzWjaAzjrOdPD6vLmP/30E77++mssWrQIQgicd955mDNnDg4cOIDmzZtj6tSpAKQaU1lZWXj55Zcxa9Ys5ObmhvtJGYZhapWomaSIKIWIFhHRciJaTUSPG7S5hogOENEy+e/6qPUHtZOD8dNPP+Gnn37CiSeeiJNOOgnr1q3Dxo0b0aNHD/z888+49957MXfuXGRlZdVCbxiGYWqOaGoYFQBOE0KUEJEbwDwimi6E0Jdr/VIIcUuNvauJJlBZ5cWOfUfRxbEDcLgkrSEKCCFw//3345///GfQsaVLl2LatGl46KGHcPrpp+ORRx6JSh8YhmGiQdQ0DCFRIr90y38xC2oNb4298FCXNz/zzDMxYcIElJRIH33Xrl3Yv38/du/ejbS0NIwZMwZ33303li5dGnQuwzBMPBNVHwYROQEsAdAewBtCiIUGzS4moiEANgC4Qwix0+A6YwGMBYBWrVpVv2M1nIyhLm9+1lln4YorrsCAAQMAABkZGfjf//6HTZs24e6774bD4YDb7cZbb70FABg7dixGjhyJ5s2bs9ObYZi4plbKmxNRNoBvAfxbCLFKtT8HQIkQooKI/gngb0KI06yuFWl58wqPF1v2HkEXx06AnECzEyL9ODGHy5szDBMudaa8uRDiKIBZAEbq9h8SQlTIL98D0DtafSBE1yzFMAxT34lmlFSerFmAiFIBDAewTtemmerleQDWRqs/WnHB9UEYhmHCJZo+jGYAPpL9GA4AE4UQU4joCQCLhRDfA7iViM4D4AFwGMA1kb6ZEAJE5jqE5kgdlhd1bYVEhmHqD1ETGEKIFQBONNj/iGr7fgD3V/e9UlJScOjQIeTk5JgLDQLILynq5qArhMChQ4eQkpIS664wDJOA1ItM7/z8fBQWFuLAgQOmbbw+gYNFJfDREQAEFEXR+hVFUlJSkJ+fH+tuMAyTgNQLgeF2u9GmTRvLNke3r8TWLx7GUOcCgBzAo0dqqXcMwzD1g4SpVus+vB7nOuUkc+GLbWcYhmHqIAkjMOBKjnUPGIZh6jQJIzAcbnYUMwzDVIeEERjEGgbDMEy1YIHBMAzD2CJxBQYnwDEMw4RFwggMl86HsWrSszHqCcMwTN0kYQSGI0krMNzrJseoJwzDMHWThBEYcLIPg2EYpjokjsBgpzfDMEy1SByB4UyKdQ8YhmHqNIkjMFy6xD1eTYlhGCYsEkdgOF1AdutY94JhGKbOkjgCAwCSMmLdA4ZhmDpLYgkMSqyPyzAMU5Mk1ghKpi8YhmGYECSWwGAhwTAMEzGJJTDYJMUwDBMxiTWCUkDDEFx8kGEYJiwSS2CoTFIueGPYD4ZhmLpHggmMACwwGIZhwiOxBIbKh1Hg2cJrYjAMw4RBggkMbZSUr/J4jDrCMAxT90gsgSH7MEpSmgEAistYYDAMw9glsQSGrGEUNToBAHC0pDSWvWEYhqlTJJbAaNYTACAymgAAikpYw2AYhrFLYgmMEU8D1/0CNO4GADhaUibtFwKY8xJwcGMMO8cwDBPfJJbAcCUBLfsiNysdALBs+wFpf+kBYOaTwGejY9g5hmGY+CaxBIZMSrK0mNKRYtmHUV4cw94wDMPUDRJSYMDpAgD4vFXS6wpZYCRnxqhDDMMw8U9iCgyHGwAgvB7pdXmR9D+5QYw6xDAME/8kpsBwSgID3krpf/lR6T+vyMcwDGNK1AQGEaUQ0SIiWk5Eq4nocYM2yUT0JRFtIqKFRFQQrf5ocCgmKVnDqJLDa13JtfL2DMMwdZFoahgVAE4TQvQE0AvASCLqr2tzHYAjQoj2AF4B8HwU+xNA1jBGlk4GPBXAkg81+xmGYZhgoiYwhESJ/NIt/+mr/Z0P4CN5+2sApxNR9JfFk30YgyvmADMeAHYu1OxnGIZhgomqD4OInES0DMB+AD8LIRbqmrQAsBMAhBAeAEUAcgyuM5aIFhPR4gMHDlS/YypNQhzZFtjvcFb/2gzDMPWUqAoMIYRXCNELQD6AfkTUPcLrvCOE6COE6JOXl1f9jsk+DACYv2GPar+FwCjZD5Qeqv57MwzD1FFqJUpKCHEUwCwAI3WHdgFoCQBE5AKQBSD6o7JKw8hCSWC/EMDGX4Cdi4LPeakD8GLbqHeNYRgmXolmlFQeEWXL26kAhgNYp2v2PYCr5e1LAMwUtbHYtjMQDZVNKoFRdgj49GLg/eFR7wLDMExdwxW6ScQ0A/ARETkhCaaJQogpRPQEgMVCiO8BvA/gEyLaBOAwgMui2J8ASen+zYZqDWPdlFp5e4ZhmLpI1ASGEGIFgBMN9j+i2i4HcGm0+mBKciBBL40qav3tGYZh6iKJmentTg/dZvdf0e8HwzBMHSIxBYbDxsde+klg2+eNXl8YhmHqCIkpMHRs8LVAkdBpHRmNA9uVvJQrwzAMCwwATvhA+iT09NzAdhUv5cowDJO4AuOuTf7Ndo49gL4giSr0Fr6q2ukTwzBMHJO4AiMjD7hmqv9lUAErn0e1bcOH4fMC898AqsprpHsMwzDxRuIKDABIbejfDMoXVAsMYUNgrPxKKmT4m0nB3R/vB15oF0EnGYZh4oPEFhjuVNND2w4UBYSIzxf6WspiTCX7jI8veBMoOxhmBxmGYeKHBBcYgcgovUnqf39swacLd0jrZVSWICSuFOm/h01SDMPUT6JZGiT+UWkY+igpF7xYt7cYeG8MsHdF6Gspq/WxD4NhmHpKgmsYaaaHXPCCKkrtCQs1rGEwDFNPSWyB4QwoWD7dV+EiL5qXrrJ/LY/sw/BwbSqGYeoniS0wACApEwdP/De8uq/CCR+SK4/av47i9FZY8BYw58Ua6CDDMEx8kNg+DAB4oBDFB0rQYOlHmt0ueFFZEYZ5yStrFsqS5D/eJ/0fcncNdJJhGCb2sIYBIMnlCNIwmmY4UXa8zP5FvHI2+OEtwPEwNBOGYeoe750BTL831r2odVhgAEh2OYMERkbpDhw9ZiOcVkHxXRTvAiboV6JVoU8Q3P0X8Of79t+HYZjYU/gnsHB8rHtR67BJCoDbSajQCYzhzqUY4jCIkNq5CFgzGeh1BdCkG/DDbUB5EZDTPtDmwFrzNxM+gJyB1+8Mlf73vS7yD8AwDFMLsMAAkOJ2okg4grL3kskT3FhZ73v+68BjRcCSD8N7M58XcDhDt2MYPcW7JZNnwSmx7glTWxTvkUL7O54Z654AYJMUAElgtMzNrJ03s1OXqibZuwrYb6HxMHWHtwYBH54d614wtcmEM4HPRgebsmMECwwZh9Md/kneCMqe1/bqfeMHAW/2r933VCgqZGFVkxw/HOseMLXBltnAqknS9tHt0v84WfXTlsAgonZElCxvDyWiW4koO7pdq2UoAjNRxbHQbfav074OR8P44kpg0bvh9cmMWMxQXukWO2HF1B6TbgCmcfh4jfHx+cDX/9Du8xmYx2OAXQ1jEgAvEbUH8A6AlgA+i1qvYkF6TvjnVBSHbqP/4YWq8u2BDdbnrpsCTLsr/H4ZUVOCh4lPDm4CHm8EHNpc+++9ciKw6J3oXX/nImDd1NDt6jNxsoibXYHhE0J4AFwI4DUhxN0AmkWvWzHg4glAhxFhnfLJ7JWhG5HuK1aXSn+jb2A72hrA2u+je32m9jAqt7/iC0l7Xfl17fcn2rw/HPjiilj3wpqq48C2edG7fh3TMKqI6HIAVwOYIu+LwOgfx2TkAf1vMjx0SBg7xBf8uSj0dUkXemVmkooTGyVTBzAcPJT7LD6cozWCEMCupbHuhT2+v1UKSDiyLTrX99YtgXEtgAEAnhZCbCWiNgA+iV634osykWK4/42kV0OfHKRhyIKhQpcU+NfHEfSsniIEUG7D3JeoGAkM/cSkPrBqEvDusFj3IpjSQ8H79srWhsowqkOEQ13SMIQQa4QQtwohPieihgAyhRAma5HWYUweOl/wit9hXFP3FSsaRukB7f4pd+jeVGV2eMLEv+LzAY9lAQvqWcbpgreA51oCR3fGuifxiZWGESfhl0EU7ZKSVEv22z/nYAgf3/Y/pPu/tu+TTy4I3qc81/rnvaaoSz4MIppNRA2IqBGApQDeJaKXo9u1+EFUS2DoTVKyIKgsNXgj1cOuvkHMZhdKhdyfHoq8f/HImsnS/yIWGIYYmTUpzk1SC8dLZXCWfWr/nFDCb/EH0v/tv0fer0jYvyZ4n2I5qG5S7sFNxvvrkoYBIEsIUQzgIgAfCyFOBnBG9LoVI9Jypf+6WUK1NQy1+Um5sYzWzVAvvqQvl26EchMJG2uO1yWThfJ5ojVbq+tY+bviVcPwC7I4uw93LAD2rQ7vHKPnrabu2TkvGO+vYz4MFxE1AzAaAad3/aNpd+Da6cAZj2t2V0fD2HroOH6e+IbqYvKNZbQyn9pur08KNBoI/LOOGA4S+9fWfHXe6j58e1YAh7fWXH/ijbrg9I7GAOfzaZ+RlROrdz1vlZRJPX5weOcZCowaClpRP+eznwts1yWTFIAnAMwAsFkI8ScRtQWwMXrdiiGtBwIObYmtvAapJo1Dc7C0EpPWqhxhfg3jeHDj8qLAtl5gGGkk4WgY0eLN/sAHo2r2mn6BEaF6//Zg4NVeNdefcNi9DNj5Z3Tfw8rpveU3YMVX0X3/UKyYCDyZI9W9UlAGwrA0XZ3wm/mE5NsKSpiNcEKnmDyNBvtPLgQmXW//Wsrn+/CcakY8qgXGs4HtumSSEkJ8JYQ4QQhxo/x6ixDi4uh2LYboBuAG+V0jvlQH2oUmdMT/+utv5VmRkQBY/S2wRs6X0JukqgyiL8K6iaJoCtgfpkofCr8DMc7MF3Z451TgfZW1VnHM7g1jud9QWGkYhYuAb8IY6OwgBDD/DeDYXnvtV38r/d9nYOuvzn24Ui6XcfyIdTsrdiwIfA4rLWjzTGDlV/Yd6oqQOLY7OKClJqhLJikiyieib4lov/w3iYjyo925mNGkm/b1BW8Bg24PedqjVVcH7cumUjzuDqzmd8nuF4Gtc4xNUrOfASb+XdrWCwwjJ3k4taxqevD1VAJlUapt5BfYcWJeUSgvlirGhsPaH6T/W2bXXD+MZrDRlK0HNwIzHgC+usbmCQadiUjD0KE4lPUCM5xrTjgTeGug3CcbmsAPt9q7rnqSacf/aHodk3ve5wF+fhTYMCPya9cAdk1SHwD4HkBz+e8HeV/9pN0w4N+qhKGUBkDncwyb/ujrBwBY6OuMj7z2ShA///l0lIdazU8vDCoNFnNSPzhV5YGbrajQVj+qxRdXAC+0ic61lc9hlNEcS94aBLzcJbxzFD9MTZoNLX0YUUCxn4ftqxIG2yT9vpHkKygFQr2e6jn3y+Q8Cv33qFyzcElg3+aZksk1VDiwWvgon+3XJyWNxi7HjwCrTDL1fVXSkgrhXC8K2BUYeUKID4QQHvnvQwB5UexX7GnUVvvaqfJrNGwD4ZKS+Ro2Cv9rKC0txe6DIdRqvZNr61zp/4LxwOrv5DaqG/7pJsCyz6QZyCvdgPU/ml97/hvALJV99JMLgd9MojPM2PSz9vXhLUDpQWBtBDERpYd0IcXyw7dnGTDjwfiJ/CnaUY2Ta/Az1HpVgDAd6mSQE6JMgIiA3/8LPNMstIaq/90V36LPoxPAKmFZWSZpv3bQC4zHs6VlV987Tbt/+++BEF7Tvqr6U1Uqfd65L0kaDSAJnrkvW5uW5v7H/Ji3UuqvM8m6H1HGrsA4RERjiMgp/40BYJDuGICIWhLRLCJaQ0Srieg2gzZDiaiIiJbJf49E8iGigl7Ndcizm5Qs4LZloIYFAIA+HZqHfekUVMLplX0YZpFAerW2VJ7h/Hgv8JVs+tLf8NvmSn8AsGux7oKqzzPjAeA3OQKjvFi6mWc9bf8D/G6Q4f7qicCL7YEvr5Tq6tjl0Gbgxbba5S6Vh2/aXdKsysgcF0uWf2m/rV/DqKbAKNoV2I5Vpnd1PsOfSvFLAv54XdoM1xehmKS8leZC85lmgUE6FGoNdvMs6b/Zsquhlj9Q96eyVJo8KZQdliZlvz4ecGSXHZbef/p9gdwLq4mA8ky56obA+AekkNq9APYAuATANSHO8QAYJ4ToCqA/gJuJyMh7PFcI0Uv+e8Jmf2JHe3nFPfkGcrrDj6BKQRUcPlkgnPNKcAOfN9gktfGn4H361w1aAH+8Jh8zmWWpH/pDm6Wok3D5+WGTA/K1jRz6Zijhrxt/Ul1Gb76JkoYRqQ/m27GB7cLF1hVi/bPtECapj86VBhUzNqg0RiOB8WuIR6d4d2RmoMLF2ug9QL4/5T5s/Dn4nFBJhGXyYGo1ESjaJc3Q1Sgahrcy2P+gvq93Lw30ff86SaM2Qv09KtnbLuMyQHAlm/cV0P6+laVax7faf3VwPbDqG8mcu/wzYOFbwMSrgvujR/munCH6EWXsRkltF0KcJ4TIE0I0FkJcAMAySkoIsUcIsVTePgZgLYAW1e5xbXL6I8B1v0jbTboBF78PnC/PjlyyoIhEYFAlnD7J6S1cBufPfCp4wN+zPLji7PrpuhMNTAB61M72GQ/a63C42HHGH94q198xSOjSD66RLFQVihVfSQ9tdYvbvXc68NpJFg1Ug+euJeYht1vnSJqeGerfLRKT1MtdgE8vMT426xnjfm2fL30+/Sp/n/1NCpsFzK8JBAZx9bXVmpDVejLqCYSCouV7KrTfwTfXAy911Lb1VEp9f/NkSaM+uFErVH68H/AaTGxSTJb5CWUKUt+zMx7UVq5VWxH2rQG2/iZtT75Z/lzycSuBoSylUEdMUkbcabchERUAOBHAQoPDA4hoORFNJ6JuBsdBRGOJaDERLT5wIAoha2YMHge07Kt0AuhxSUBAhJpxWHCT63s0X/Yq4EyG1+gnMNImgECBM4XZz2hfqx8ivcBRHlS1uWiDXuDYQL8glBF2okRe7QWMP8U4ekY/e6wJm/2vT2gTobbIJohws3zt8tG50nel1jDePU0bchsO6t8t0pj87b9LvjDF/AJI3/9vzxv364ORchv5+/dWSv4mvf9KwW+qkz/zxp8lR7n+2sos2SiQw9/GYGBU/HreiuB7pFTnlNbfg6/30Q7qC940jjgyG5DNTFKPZUkapvoeTUrT3lfqvhzejKAABYdb0iCsKt0qWl4dMUkZYctoSkQZkBZgul0uL6JmKYDWQoieAF4D8J3RNYQQ7wgh+ggh+uTlxYmvXVFd5ZkfRWI2cafAZ+DDKPY4jAWGEtLpcBk79tQDif6B2TxTWsEvHP+CEZ+NDt1G/96Wtm8bGobRALlvtfSQlRyQNK3HsoAD683fZu5/dIlQ1az9462y/lxb5wC/PAbDxyQck53/HLWGYUNgCCGZdUoPavv50Tna4nnhCJ8jWyV/kxnfjpXs8oqQXPY/rZAGABDglp8dq6growF691/Sf09laBOfUWa0/pxQ5XnU/HCbeSDJuinaa2c00T4DZZbuXklIfXWttYapfFd1WMMIOUISkRuSsPhUCPFN0AWEKBZClMjb0wC4iSi3Gn2qPTrKjrVcSRUuEhno0qxBWJfwOpKxrDBYLd96qMJ4lq4IjORMYPOvBhdUnWMkcNZNsRYYBzcGmwk8FcCSDwM2VDvO1aCyJhYPt6GGobu19A//2ilSLP0zzYGX2geSHXfq1icp3mPxvtUsP/JkrhTtY0Vypsrprdqv9wnYIVwNw+cFXukKvNjO2qSnHiCFqL5zXp9gaqTpKmbYb8dK38WsZ4PvSysns7cidMi1UTSSXlM1Eip6TUXN538z3u9M1mo83kqtuevH+3Tvq+ub021sglOj3DPxLDCI6BgRFRv8HYOUj2F1LgF4H8BaIYRhZVsiaiq3AxH1k/sTQhzHCX2vB8ZtAHpdCYx8Du1u+BAf/aNvqLM0HC4tx7vztgftr/TBIMoJgQio5AbA55cFH9+zPLBtZhYyyhhXeL0P8NF52n3b/5BmVz/IQW5mTkE1QRqGxcPtF1BWAkP1gB3bK0ViqXG6gtsBki8IMB4wa6JY3LLPddfU9Ts509jprRbK3qrQK7UV75aixRTsmOjUg6GViVCtqT6eHVoIhqKyFJrfUldmByDtoDf/DSliTx9557AQGB4Dk5QeQw1Dd47d8NtQuJJ1iXseayGt//2cbiA9xDx5qZz8G2OBof81NQhhstScPQYB+DuAlUS0TN73AIBW8rXHQ4q2upGIPACOA7hMiHgJug8BEZDZRNrufyMsFHVTklFlWAm3O20Ffv8/8xPNVPmdARfRzNW70OPMiuBkGTOVW2H3UumG378G+Pg8oPc10n6lLpCdGzYcgWFUxkKvxagfMKPyFA4TgaEMEEbO1ZoQGHp78oK3tK+TM2GYw6A4ya+ZKvk6NFMOAAAgAElEQVQUftObbXTor/upKt7k7s1AmsF6KeoBy6pwnf5++O1F4JQ7jNvaoarU2h9FpDUD/vZ84Dw1VgLBWxVaaBo9I/pzqpORrUYvMHb8AbjTzNsHaRhJ9suJVMN3WhNYCozqIISYhxB+DiHE6wBet2pTvyFDp7ehI1xNRWiThtdTiV/W7sPl+gNWGobCjAeARW9L2/N0Yb92NIztv0s2fIWXuwIXjgc6DDc/h0iqfDthZPAArx78jOzOymxUPyAoD7HR4KIcW/OdFMwQCfoQx02/aF+70wIDn1HOwcqv7M1yKyxWH9y1NBBGqkbjzwpDYOgH7nDRh8oaTRaM/Eb6WluWM3RPaA3jvdOD9wVF30UgMIxMYUaTKKvnTF9ld4NFkm3Qe8V2ZWxecKAWuL5yHG6qDK5JIwD4DH6CTaL60cdJ8MABg5vbKgNcwfAGlmW/nRnOTw8BvzwaeF120N4iTwveBMqPGkRJqQY/Iw3Jr2HoBhlFUBjZ/JXBQ6n1pPDfHsHhxl6PlKWrJ8hsqM9MdgbexywhLCVL+7pkvxTOqsZKCyov0jrzFdQD7u5lwcf97UwGzUijx/QmKf0gTWQsRDb/CmyTF0LyVARHBKrxeYwnAerB3GjArolw7V8fD95Xm2aiGOdhRE3DYAL84uuNFghWOQUoSGCUC3doDcMGeVSEU6f1DD6w8K3gfQY9MyXSh0OdrzLtboMGZL6OdyiBoZhAvFXAe6oQTuU8vcB4sb25CeDoDslf0PV8oKVUJwzLPzceKEIhRGgHtV5gvHu6VILkHz9J708ES0W93MQ8qRaen11qfr6ZiVIp0GeF0Wy7okRnktK3IfPvRBnkP7kI2G7h1zHTMEL6NfQmqQii1ZTEWO0bh3+dSIlnpzdTc1QZyGZhYJJKoSr0dlR/qZGujmBnum2s7v8gJ6ZNFJuuzwsseif4OJF5Ipd6gFGbuhQUh7DPCxSqksT8JindAGXHXvz+8ED9rmMW0VZW+KqsZ7FCSIUt1Sj1qiaMAFbJ5bytItPMzFV2Z8+G4dk2CyWuM6gbph+El35s0MZEYCiDoZWwAGQNw6CPofwa+t/d6DvKbm19DSOhVBN5QnZ9ackZ1X+vasACo5Yw0ho8cKJKVHMN4NpCGXQjFhiyhqGusaOBzO2+yntXlmqjhYLa6cNvvwc+v8LeA71tXvDgeUQuW2I3DFYvmOw4Z61iPBSzjNVgYja427XPG2kYdms8zXzSoD8eWGpEU+801zDsripnpmGE0uam6nKNjb6jSHwENSEwUhvZa5dcnTik6sMCI4r0Ln8Lyy6TSiVXIVgwVMKFI4jtDWCMwSC2Z5lU88YR4S2jCIySfcbHzWzbAPDBWdLgGaoWktGAsX5q6IFk/zqp/MWMB7T7FQe7nUS7x7KCtR+fx3oQJLI+Xl4k5ZZYfm4TgfNGP4tzIH2mP983jjozcqIbYST8fd7QuTpmv8fkW4JzaYwo2mWsaYb6nfWC31BgRGDyqYnlWVMb2muXFFsNg30YUWKdqzMOIQveVCnk0WsgMKqEC0dFbG+AsFgzGeh6Qeh2RrjTpf9W5iCr8NslHwGDggoeazGduar2GxUcVJbL9VdUVfZHYOPWv28o05BVueujOyTTmOX5EThukzKkZMzp9xgft6oPpea4wXdpZ7ZtVhLk2J7QnxcI/p0UQmV/79etAGi0CmIkGnRNaBhpjexloLGGUT+5J1uqbe90SLMtj4HA8MCJo6i+wJjW7hFsTz+h2tfxY2UmiXQ2ldlEuq6R3VvB6sFzuoHFE6zfw2zwVAsMo0Wf9JFSCp5yqU9mA1QoSg8Ce1eYH1/yobWGYZV1rGC0Nnwoqo4DhzaFf54dQpmkgJrLf9ATroDXf/fkiKxUzPLPQ7cBgH5jta8bqBYt1efS5HQwvkakpWxqCBYYUcInD7oOpeafgTLXrllDdG1hbru8qOIx02OHnbk4ImsnN63ujNeODoi8s+EQaUlwn1fyKZgN+kTWwsjhAuYZFgwIYFb2JNQM0GzhGm+lselj4L+tr6ewcmKg/pEZViW+7ZQjr7Ao4GeGMAk8qAl8HnvlYxpGYbXGUEmpoXC4tf6iZJulfpQKDCHRfS+3Lg2EyZpVyY0zWGDUNLctB677BUM6SDnWeZnJmHbrYDx+fo+gptTlPFw7qABX5RjPUKzCa1/Nvg+DKl5Fr3Ipwc4ravCntEo6MrJ528HnlRLzTCHrGaId27LZgBFpdVdPhbETlGpwlmelgdhxtsfb4lJ2NAwgomUBQlJdE6LDBU3fzzNYKKw66CPIXMlSZVvAXvRTX4OqCLUM+zBqmoYFQMMCjGshMKZ/azTLSkWzrFR0bd4AUFVT/lfl7Rg/5B5c5HDgopPygafTg7JsrQTGtlIXypCCMqSEbGuIM8ncNGA1UFllHVux6G2g1xXmx4msBz87AsNUw4hQYFQcMw71jTRSzAgjDUbBlsCIQMOIJna/65rKJzj3/6Rkx1lPA0cjCCVvkA8UFxofq+mcB8OsfllA2XFmN+leo92JBNYwooTTQWierZtFqSIhenRsp404unM1cKd2Bn5J39Y4rUK36pjM+qPaWa5RxrjCkWyDZUYePoDjMMkatTIN2a15E3RNH/DX/8yPH9lurdnYsd2aahgRLsC0+H3jIo81KTCssNNvq0WIapLGhkvVBDPtLthKZLNbE+m8EJWDslsHBlv9AmN2yOukeiG05rSa/p2V2nNqFAGr1jBumBXcLhr9iQAWGLXJvduAVlIG7c1D22mPpTYEGmgLAJ91Qj62COOiwEVI17w2KmKocPf+kXio6tqg/XFV5XHPMut1Axa+HfoaZhpGTVUlVYix41FDbZmkwtEsLU2PMnYFRqv+1scdTuAEeY0Wqwq3RqQ31kZWOdzQVtqt5u/ccaT2dYcRwCjdBFD5/ZIbANmtgIG3Ai1MVnBkgZGAhFrjue1Q/2aTrHRM+fcphs3KdNqBlUmqEi7DKC0hbK2BFcQaX4hs2Gph0ic7EUNmg2ckJSCsiCeBETSQR/abhqRxV/ttjbRQ/fm5nYLbGBFKsJAz4A9Z8oG9aypcNwOaaVNSmk7DqGahP71JixxAu9O0+xRtPjkTuH0lMMIgGVLRoFhgJCBtTpX+NzDWHHDV5MA2OdG9RVZQk4Lyz6AfGBST1AJfF3zs0caye+A01ECEzcHFQ9oH513PKFvnRUTzE+237anziZhlKBs5Q8dMArpdZP+9CgYHtmvS6V1d9H6Oe7cCjdoZt42U3tcCF6k0vO4Xm7cFgkup9BsLXK9b8Cu/j733DlVsz+GM3NfQqK124uZO1UZJRZL1rb43ctoHHzO7pj5xTy24lHpjcTBRYYFR2wy5W4qkyrHxUNvIqr6ktxTLrQz+x0QqNunMWF7h1AiHrdkD5XPssTDrLPyzMrBGgl67sWTwOPttASApPXQbhfze2teHNxu3M/JtuNOg+QZCCQF1Wfc4mOn50a/7kJQRKJoYCc16BbYVv0V+HyBZNXHpen5410xKD9YU7JqkQrVzuoN/j78brvRsjHoy4U6H9eJPKkY8Ddy/S7vvro3AaXJV5rwuQL8btMfJYS7crEqDKIsrVWftlhoi9j1INBwOKZLKDrpBzOtK1yT7XNa3JS6VBYZ6TfFKaGcxXhB8ctjt795uGLH3Xxjw7K+2NQwfHFjuCywRZXrWpR8G7zv9EVvv4cdq4Rk96gRDvb0YAAbcIv1XigiOmRQ45nBrzw/1vuqBK5TAcKfVnhZilHxmd2AxGqSaqsK/1avAqWe8dj5bo3aBfBV9fgNgva6KOqFN/b1fMD5Ye3ImBed92C2zAWgDLfQmqexWge1TVcus3l8IDLhZO7m5ZhqQ0TigQbQ/Pfg+cVgIjDQLgaHkrFgtr1xLsMCIZ3QqqPOBncDtgbj9py7ojmS3/uElVArtjeqF0+/jOIIMVMGFPUXltjUMAaAcgRudzM7sdmHoi6XlAFdOMj9uV8PoeFZgO7kB0DNoqaiAYF7xhfxalSzmcEKjYYTKC1A/6BmNrdvaLSSnoB6YAKDLueGdr4Yc1StVoR7k1OYTCtMZ7HAGruVwBQ/qVmYkdQVftUnK6Q4WNEZ+BqvBV496EFbfA1f/oE3cG3Z/YFtZelf9mZrKIa/+Ndx9wUKSHPZNUgrX/xoQ3LUVEWcBC4x4Rj+TczgBhxOX95MGGJfTgRR38E9YAe3DWChy/UOjWqswyj43opKS/PkeaNJdIzDE5V/YuoafrPyATdadLkWOqUmyqWFcoXrfHpcaz1j1D6d6oNMfc6daz5yVmW7jbvYEY6i6Rmr0g+elHwNDdYUQkxtIpsxQEAFFJnkFQRgIfvV3ZOb0tWOScyYHBkyjzG/973WuKklO/dvozbJ6E5XRAByOhqHOpnenB4R1duvwTEAuWdgo5/i8JgLDRFCaargUqB8VaQ5UDcICI55RHl7dA/DsRT2w9VnJ8ex2Sj9h0wbSg9S9RQNUyoJgm68JOpd/gAMInK/O17ii8kEsU5ma1OwXcqmCAbdgaqOrUAk3Zp4xFbhhJhzyQLMiaxgmFtuMz1fwVgFOecBJSg9+uN06DeOGmcCJf9fuy5R9NIpJiSh44Ljo3eCBzaWaQRqZpKzyT5QHvet5NkpfCIQVtOzSaTcOR7DG8695ksak/y6MsJtFbSTU1EJTiehp3EXbJldX58hIWKsFsJ1lWntfreqX7rtTC2j9+iFGA3A4FV31JqmT/wXcux1oGKbAUO4/tYah/4zkNBfCQb4a+R4jCmjTrQfZ70+UYIERzygD3u2rgmbiJA9aLrlYlVK7Kjczxe/DcMKHctlBrQzy6kdxg2iJCyqfwqiKZ/CZZ5jm+qdVvIRdl0wBznwaZZAGoKNpBYArOaBhkANzN5qtb2HC/jWBh0YZoC96L3Bcr2G06A20PFnVsYeBcfo4fzKYeSYFCwy1KcnhgtYkpRv09NqGcn2rwowKdtqoMapAqh+EG8qhzOrBUC9cFaxMRqfeG9g26qb63BPHSI7cFrrgAr0PLqMJ8JguWsuZFBjMjYIO1N/RlV/rDxq3JQqu76QXGDcvshbonc/RvlYLDHeqdG5qduD97KK0DWWSMgtkMXsvcgCtTgYeOQy0rqV6cRawwIhnlBsuOcNUzVaq4TrlG85BhApFYFBgxqwM8kYZ4WtEAYRq/yZfc5QgDbszumHn4TIUHZccq3dOXI4N+475r1Xh8WHKighWo1MGcmXmecKlASFi5MNIzwtsazQJ1cCiHzhcKcGDvvqhdLq0g5Z+lq83ESjXt2tqyu1orx1gIjBUn0cdaq2mnSLkdYONWR8H/luK0jtbKbZoIDHUg5zDFdpfY3YdhyPgSzAqWKlMFloPAjroSpqbClwy0DB0M/ZQ5b/1kwh1Jrm+AGAkUUn+e0wYm5QjvV4chNQCLDDiGxs3SYvsVNw8rB1uPk2K+SbA7/RukBQYSBxkLjAA7SPvgDTgLNp6GINfmIUFWwIP/N/enu8XGLuLtPkNQ14wKWmgR3nI1QObMoCoZ81XfCX9Tw08yL9vLYbPpxtQjExSrmTt93ebrsifQycw9DO8Zrr10BWhpddEzLj6B+CKidp9Y2cHts/5b2DbqPCc4uxt0l2TzOnnzGcDpiG9kDUbcEc8pQ1DDSX87IYPG70dOQMlu40y+JVS9Or3GPGUfD39BVWvQ2kYoQSG/j7p+Tfgwb1A/5uAU3Xrg0QkMEJoGArqvB4714sT4iignAnCRvgiEeHuMzsDazfJr+HXMFKcgQfNr2GYZHevFy3924r56sUZ64PaHSmrCjwTumM7DpcBuvH0vblbEFRjUx4khM/nH/ydyuClmKRSsoCOI+R9gQFx+tpD8G46iCEd87QDS5CGkRwYjJIyAiYdfx/c2k+gjiy6cb5UFFBZWzqvM9D/Rilmv//N+k9jgAAym0p/atRJiepZtaGGIQuMoGJ+KvOMIlyDBnaDEVztRPf7FkKYzuyurmjoC3EEhG6bIdL/tsOALfKkQqnhdNJVgXNa9DG/HiB95lBOb/132f9mYMBNwLunSas9GvkQ3KnAyGeN30/NhW8D5TrH8yl3AIWLVeeond6685Vj92wNI98oSpn7ERJf4ouRUB7oCNRQQsAkRSonbqfG0g0qQPjqXwNw/1mdNed94h2Ob7Mlx6MT1jPPXUIK87NTIuSpqWtR5VA7m10Q8uc6XlmJjg9Nx7CXZgeOKzZ69VrVqofrW+8pqPQox5QBj6wFhlGYq1Pn9PZVAdf+KNn4m3TVDkw57aXXQ++1p2Gor5vfT8oHuWmBtk2odRdMBYYKv79Hb/NXfXdKTsNQle/CL2AMBEZWfvC+kMjXufWvQOKfwwk0aiMNjsrCQVepEuoaNJf8Hj1Uq/v5B3/5eko0ncZ0aCOsVs3IZ6TP1OtK+T2qMUfueRlwsm4RpDMeA65RLQrmn00J6TdU5yEpx9Ia2Sh5ovOJxAnx1RtG4oS/Sf8jTP5SnN6kGmwu7d0CgFSksG9BIwzrHLBN33dWZwCEkqZSoTfFfGXGn6Izzq14Cu96zzZt80DVdRhe8QIA4Kg38FAXXrMY87ZIDlKH8MHrE5JmoqBE+KgjllSO3lKkBg9zRJi3VTfzcyYHrpVqsDiNPg9D+CSn4jB5Jq623V/wlsmntMH1PwNnPh0caaT+bZt0k2aqahSTlFVOhTLo6O8T9QB7w0zg2una436TlO6bPO81ILOZtJ3f1/x91fS9AbhSNh02aiuZdtR9Smtk33nsUGk+4zZItZWkHfJ/Aw1D0YJuXwXcuc782so9UNMly/XoI8PUlQ6q48OIE9gkFY+c95o0yIQ1Gwo8/BVCHqDlwaZtbjqyUqSb9eI+kumpVaM0dG6aiUfP7Yb+bRuhbW46+rg2A+sCPgwrVgrjcFyFyd6BKJWjq8ZV3YiPk54HAJzx7jokVxVjeYpJAqA/GslYw9CgGvA+X7oXmjKNrhTJodr5HCnaR48+rFZPp1HA1VOkYAO9ozUkFte9fmawf8Xhkmaq815R9d+GwFCEStBAJL//JR9IJbX1ZbUdKju7GrWmozenmXG2rvqqMkmJZIEkh8q3ZVQKnMg8Qzy7pfb1JROAw1sCr51hRLlVh44jpQTRQbcGH4szbSESWGDEI05XeNmqQCDCI7slMtLSAJ/KJEXwDw5JLuknT3E78ePtQ/ynj+jWFMd3ShFPLhMNIy3JibJKe1nE6uq5c3wBB3J5lQ9OuXKuw2BgPVLlQkOQ3wE6a91+5KS7YLxieWDm6fXpM4ndUqTRZZ8ad9Cp82HoBxIioI1Nx2RQtywGJaX+lTpySJlFdhoVKEppxyRlqmHIgsDMAWxlkqouSuHBvM7W7YzQm6QUNCYpmzWoQhVIjBbpOcBty4yPGVkMUrLrlCBhgVFfaDMYuPQjoONI/HpaFfAsQL4qtGqUhkfP7QYc3Sg3NFdxU5OlWWHDVBeW3zsCf3t7Pkb1aIb9x8oxbngnrNhVhKsnLLLVHas6VUqpdSNfyZRVB/H3xwIF9a798E8AwLYUoIx0moZSIrvZCajarq/bE+LWdrgk7WPjT/4eR0zPy4HlxsvsmqIeJJSB5HLVNcwEhlCZZxTzil7DUNqYmUBsOb0jNIX0+QdQtFOqtRQuZqYydZ9CVa81Q9ESw52I1SRGguHuzbC896KtEYUJC4z6RLcLpP/ewEAx5x45Vn+hPDhbzWbk2H8X+ZCV6tZoIACQkRy4buPMZOw/Zr7OhNX6HIrAMPKVfLl0DzLaF+LCE/PhVYXPXl75IPLbdgU2qRp3OENyJud1xrGFC3BpxSP4Iu15OL0ma3GrIZIidJxJwHf/sm4bigvHS6UtnlLyRWw85BqTlMHA7gylYYiAUHE4gYymQIluvXUzH5hZWG3zXsDuvyy7HZK0RtKyqdXBKtxXSeIc/Unw2hJW9BgtaXV9rwPmvFi9/kWK0bNnanaOL9+FQt3RhRj7KAOC4sAEAg+hlcBwGvgPVCS7pAGobW46vvynddapXmDsE9lY4uugObbA1yXoPB8c+GOTFLd/QCWQ5vu6YZtHmh2qBQkad5EGfyE547f1fUzar0/CkikZMx0HT5SrqJIqEay6M7lwHZrqwdxoYFcS9yx9GIqG4QJu/B34p1yVN9Rv7e+r6jM/VmS/inJ10CdIqlEnvZkdz20v9bXrecb5K2Y4XcDAWyLzrdQUYa2vEV+ahQJrGPURImkGpo77tyMw/GYQM4GhFJNTbZsgdALj5Io31R3E8IoXsFvkYEDbHMzfEkjs8sCJKq/0/p8t2qG5hqLReHw+LNl+BGlJTnRp1gClFR5UyufsbT8a7UZKkTprdhdj26FSjOoREJxX/OjDisIB2KYs6WAzOW3SkkIU5Kahd2sTk4b6e7UjfDQmKYPvUnHummoYpPVhpOcGqpqqS2kY4bBjkooS49YBXpMlc8nEGR9nZpmIset/iWNYYNRXup6nfa3MYq1mOUYZ2AY4iDQC44jIQEMqQXaaG/OquuEU5+qQ3dsopHj/d67qjZ/X7AO+l/Z74cCxcg+2HCjBq79u1JyjaBxVXh8ufusPAEDPltlYvjPg81CEDQCMelWacW97LhD+u6JQCun1+oRUVsVotm3AuK+WB10LAHCj1A8QAee/AUy+OeS1ANg3SVkWRAwRJWVm1rB0eke5FIVRiLOCkoiYp9c8Q3yeukJYS77G52dlk1Si0PtqqRLnkLvN2yg3tEnyVlaqdHxguxwkqQTGTY0/Bu7bAScRrq+6C1uunG+7W+lJLmSmBB6kRhmpKC6vgr76BwB/hNairYGlWNXCAgCqvNKJa3Zbl4IeN1GOZJEHT4/Xh6KyKoszTGiiqtbb4Uz752lMUkYaRohMbyAg4PVakl/DMCt0ZyEMOo2SHNcjnzdvA0gLBl34jnWbcMnIk+pmXfye8fE4y0kIG7uZ8xriS7tigZEouFOBs563zilISpNWzbvKeInLxg1S8Ou4U/HwOV39/gwAeP+GU4GULPiEQDmS4WykzQBPdTsxuk8+Lu/XEi9dqq3R5HAQMlMCA16HZtlYtvMoyqvMZ9af60xVajxeHyo9Pr92AQD7isux+6h2tbLvlu2WOyANuhv2laD/s7p1p8NF+W5PuTN0W0cok5SNsFqYCAa/+TGUhmGAKwk45xXjPAg1BYOkOkw1TduhwfdofTFJ1QOiZpIiopYAPgbQBNKd/Y4Q4v90bQjA/wEYBaAMwDVCiKXR6hNjgxCLA7XLkxyNQvUQpyXJtaHk18kuJ7o2a4A1e6RZ/hldm+CFSwKC4q6vtAsB5aQHsm87NWuIqg1FePyH0GYtIyq9PtzxpTYO/uRnJEGgNycJIUDy4EkQOK4SUlVeH1bvLkbP/Cz745UrObjMtx2MzD8OlxS0MPT+4GMKiq8pbJNUfFQ+DZ8a0jCu/gEoDbMsf8yIL60qmj4MD4BxQoilRJQJYAkR/SyEWKNqcxaADvLfyQDekv8zcQ4ZzF6VgTXJ5cBX/xqA0koPKqp8aNzA2tmnCCEAOPfElnj0tyL8ue2IxRnmeLwCU1cal1z/Zc0+zesKjw/JDqfhI/nCj+vw7tytOKt7U0xftdegRQ1iZCIikhzEVijlSzqM0O4PZZKqcwKjhjWMNkNCt4kb4ku7ippJSgixR9EWhBDHAKwF0ELX7HwAHwuJBQCyiagZmDpJ82wpZNHtJKQnu9A4MwUtG6VpzFdqfh13KgDJLKXQKMPmEq0mqJ3eeq7/eLHmdeeHf8TDPxgPyou3SwIr6sICCG8A73aR9L/tUKkcxh1rgFPv07YJZZKKsEZZzFBKuzQzzvevlygRb2E5yqNPrURJEVEBgBMBLNQdagFgp+p1obxPM0UkorEAxgJAq1atotVNJkwGtM3B6V0CRfo+urYv5m85pHFiW9E216BGlMHg+bc+LfHlYuk2aZeXjs0HSk2vOS3MAX7JjmJIixLKKxIKASJSVcQNzRuzNqFb8wYY2snOYkMGhFMaovUArdkrSz8HA+xHSdURupwbmamvLnPJBGDNZCAvjIW4aoGoO72JKAPAJAC3CyEiWsVcCPGOEKKPEKJPXl5e6BOYWuHzsf1x/eBAEcLGDVJwfi+jAcwYI7MWHC7/KoIAcHKbRnju4h7+11eebF1Sfc6GA7bfHwiUMFHesbxKEhRmAmNP0XGMm6j1wbw4Yz2u+eBPTF+5J8i5ruazhTuwr9hgudKanvGHzMNQCQxlvWjGmDGTgJv089xaIKMx0O+G2n/fEERVYBCRG5Kw+FQI8Y1Bk10A1GUm8+V9TKLicMLtDAx0L1xygkawnJCfpWl+bs/maJdndzGaYPQW4q+W7ITXJ/yJgHoembwak5YWGh678dOlGPjcTBTcNxUvztCaug6XVuKBb1fi5Gd+xapdutmyUnCwpug0SvqfaWLdVRclvOKLxJu9h0P7M4DGERRSrKdETWDIEVDvA1grhHjZpNn3AK4iif4AioQQESwSzdQlpt06GBOu6WN80CH5PhRIZ1Y5IV+b+HX7GR3w6fX9q90npdT6I5NX4/NFO1BRZSww9MvDBi0XK/PGrM2a1x5V9vxD362SNsb+Ji0w1KCG3XZD7gbu3mJ+XbulyxlGRzSNmYMA/B3ASiJS4hwfANAKAIQQ4wFMgxRSuwlSWO21UewPEyd0bd4AXZub5IM4XHji/G645gOpSm1WmuQP+e7mQdh2sFSTMAhIOR76fXY4u0czTF25x7Cq7qGSShSXGyfx/bpuPwDguJBCgbs8MM3W+6kFkN/k1ryXYdvbv/gLA9vlYnTflobHQ+JwSGW2zSACrvslsKKdXcZMMq3RxVhw5jMh8mnqDlETGEKIeQgRRCykYP4I6iAz9RZyYGinxtj67CiUV/mQmiTZ93u1zEavlsGDVVqSU+Pz+MegNpjw+1YAWme5mra56XjjypMw9b6p/n1ZKS5ALnE0fdWekFXWZDoAABlCSURBVOt+9Kx4N6yPpTZxqftrxHfLduO7Zbsxum9LTF+5Bzd+uhTTbxuMLs3CXchJ4o9NB7G8sAg3Dm0X2NnS5op6atqfEdH7JzyRlHqPUzjTm4kvZH8FEfmFhZ7/qLLFU9xOuJ2B2/jEVqFnwIpLxEGB9cmTTwuUTFm391jIa1TC7V8KNxQ+n8Dp//nN/7qorAp/bLKXOPbKLxsAAGf939wQLc254r2FeP7HEDkdDGODOhZfxzDAxb3zUZCbhklLdyHZ5fAHBfXMz9IID2GS9JQk54X8Om4ojpVXAflFSK70AN/NqNF+7jhUhhmr9waZltbvO4Yr3luITU+fhSNlVZixei+u6NcKDgdpMuiN2LS/BIVHyvDjqr3YdfQ4PrmO81yZ2oMFBlMn6d26kb/UOBHwzU0D0S43A0t2BJY+vfikfExcLEU0NUxz44hcXDBJjsJqo8oDSXXXfDLbYz+sxsx1+00z3Y+UVeGqCYuwdk8x+rVphI5NMlGhC+fVy48zXv4NRvh8Aj4h4HKy0YCJHiwwmPjg2h+BrXMiPv2kVg0BSKXXAWBwh1yc3Dbg+E1xOwFIAsMo89wwJ6SazJQd5IVHjHMz+j79i3/7UEkl0ASm0Vlm7Ck6jqxUN/729gKs3FUUXH5dhcfrY4HCVAsWGEx80HqA9BclslLd2FMkJc2ZRVXddnoH/J9uDQ41gzvkYu7G8IvW7bJI5lMoPFKGzxeVoiLMKr0Dnp2Jzk0z7fldWGAw1YTvHqZeotcY3r2qD3q0kMJIzQTGHcM7+mfow7s2wa2nd9AcL8iJLEFw477AYD68q3HZ8Lu/XoH7v1mJx34I1OYc+d85muiq+79ZaXiuHWEBAG/O2oyJf+7EfqNsc4axAWsYTELQslEaxv+9NwY9NxOXhchvWPbIcKQlueByEApy0nCnXArkrB5NkZbkxNtztsBBUpmSTxZsD/ne6sq7t5/RAfkNU7F851Es3XHU4iz7gkDN6PHzsWznUWx4Wir58fZvgQTC12dtAgC0b5yBX+48Nejc45VebNh3DD0NwpcZBmANg6lnWMUYtchOxbbnzsaIbtaZztlpSUhyOeBwEJplSRV4L+/XEgPb5eKGIVLtLJfDgScv6B6yPxnJ2jlZZrIbj57bDR/9o1/IcyNh0bbDqPT6sKJQWoTq2enB4bSb9pcYnnvX18tx/hu/41BJRVT6xtR9WGAw9QrF6Z3krBkndv+2jfDiJSfg4XO6AoA/bNfKR37XiECF0Yn/1PplMuTVBdOToqvcn/f67+j88I9hnbNE1oTK5Ugtn09g6Y7I1iVh6icsMJh6xaB2ObjulDZ45kKpwu1PdwzB5zdEXmuKiHBpn5b+VQWVwogOncToW9DQv33LaZLvIyc9Cc2zUzTtlOVoHSGyvdUM61TzFZqNtAiPXBfLK6+L/sEf23DRm3/gd5tJhjXF7qPH8fUS4wKPTGxhHwZTr3A5HX5tAAA6NsmUFgmuIZJkDeOfp0qmqbFD2uKdOVvw1b8G4tOF27FLDqFd9shwuJwOpOnyO9SJhePH9EaHJhmaLHAjBnfIw6z14ZVtD0Xvp6SQ3ifO74arBhQAALxygcQKjxSppTjrdx4uCzp/wZZDaJubjsYNAgLxWHkVjpRWoVVO9RbBGvP+Qmw5UIqR3ZsGmfSY2MIaBsOEgcvpwLbnzsbtZ0hmpwdGdfFHVl15cmvcM1IqhZ2dloSMZBccDkLPfOMifyO7N0W7vAx8cE1wXaeLTgqsK9KvTaOa/hh+HpkcWDtd0TCU5MEv/pTqcHmFQFFZFQY9NxOTl+3Cm7M34bJ3FuCS8fM117p0/HwMeXFWyPcsPFKG7YfMF8E6UCxpP4qmw8QPLDAYJsp8d/Mgy+PDOjfGhSdqF5568vyAQ11Z+jZaeBVTlPx/w75jmPhnoGijzyewft8x7Dp6HPd8vQIv/LgeALBDp3noo7r2HytHwX1Tg8xLpzw/C6e+ONu0P4q5zmxNEiZ2sL7HMFGGiDDj9iEorTQvca03+6SpCi+mJxuXLUl2OYJKiUTCsfIqZKcl+TWMO3UrCnp9wt8//ftNWlKI3q0bavYpy9xulZfSveur5RjYLsdS8L05exMGtctFz5bZUNw7isAouG8qbju9A+4YHl/LlSYirGEwTC3QqWmmv3yJEZf1k9aqf+fvvXHD4DaaxMMkld9jxWMj/Nuz7hpaI32bLq+DbrYYlE8A+44ZJ/uN+2o5hr40G0Nfmu3fpwgVtWN/4HMzsWT7Yc25SqHFCo8XL/y4Hpe+LZm4lICCSo/P3yerDHym9mANg2HigEt65+OS3vkAEJQnQkSY8u9TMHv9fjRIkUqq92ndEFmpweXVe7duiCXbg0Nh05OcaJqVgs3yrP+zG07GvuJy3PHlctz/zUp0b57l1zD0PDFlDW4Z1t72Z6nw+JDscuBF2XSlcPFb83FBr8BytCUVHmSmuLFf9lkk60KWKz0+S7PUxn3HsP9YBQa1z7XdN6Z6sIbBMHWA7i2y/OG68+4dhk+uO1ljtlJolpUStA8AOjTJxDWD2gAAfh13Kga2y9VoPOe+Ps/y/bceNHdS66mo8mLzgRIs2nY46Nh3y3b7t4uOS8UglRpfuZlSVV9Fu7rn6+VBJrDDpZX+MN/hr8zBle8tNO3Hyz9vwIPfGpdTUaj0+HDgGCcq2oUFBsPUMfIbpiE1yWlYYdcsDLVBqhtjTm6FtU+MRLu8DABA65x0LHrwdFvvOXXlHtv96/fMr1i8LXTC34zV+wAAxbLgKKv04Mkpa/wD+PLCIpRUaP0+Y95biCvfWwiPSvPYcsA4c/3VXzfi04XBBRvV3DlxGfo+/Yvf9DV95R7sLeJaW2awwGCYOGXyzYPwcZglRPQCY3AHyVyTneo2XMWwcaaxRlJdQg3UAPDkFKnQomJ22ldcgffnbdW02aOr9LtmT7HmHAC48M0/Iu7nlBV7/NfzeH248dOlGP32/BBnJS4sMBgmTunZMhtDOlpneT9+XjcMbBdY96OHKudj23Nn49I+UqHF9o0zotNJE1buKrLdVkkUNOKh71b5t9WRZGo/TdHxKqzaVYRK2XxVUuHBPlVFXiEEJi/bhXKL0vGVXh+q5LwPfbgwE4Cd3gxTh7l6YAEykl34Y/Mh9GvTCOf3aoGOTTL9g+c5PZqh0uPTOJutGNIxD4dLK7BqV7FpmySnI6IciSSXA5UeH/Iyk/1mp0cmr0K35g1Mz1Hndgx+IZAU+Pf3F2nanfPaPIzp3wpPXdADZ786F9sPBQb9eZsO4rYvlmHskLZ4YFQXw/ep8vggdNPnI6WV2HKwFL1bN0RphQcpbiecYZR0qY+whsEwdZzzezXHncM74sNrpYzxLs0a+EuUOxyES3rn21o46ZW/9cSrl/XC1XKpkAFtc/D9LYOClq+dc88wvHb5ibb7d/eZnQDALxhOVJVP/3j+dr9wqy7Ldkrl4tXCAoBfOO08XIbJy3Zh6oo96Pf0L36nOwAcr/IGaTr/+t8SXPzWHzhe6UW3R2fgnq9XaI5v3HfMtLLvsfIqw/11HRYYDFPHcTkduPX0Dv4CiZFyQa8WmgS+lo1ScUJ+NibdOBAAkJuRhG3PnY2mWSk4t2dztM3VLih1m27BKYWbh7XHzHGn+qOyerbMxumdG/uPvzV7s+F5NYWSiLh851Hc9sUy3PzZUuw/VoENqoWtTnl+Fq7/eLHmPCUEefVuybw2aWkgY33VriIMf2UOej/1i0bQlFV6sHDLIfR47Cf8tqFm63/FAywwGIYBEAhnVQSG0yENDylu5b95IcWc9CTcOLSd6bXb5mVgdJ+WyM1IwkUntcD5qlIou2soKmnVrmK8N3eL6XH9+3h1eScrCrV+l9yMJAABRzsAzFy3D16fwDmvBcKQ/7dAcvDvOFSGro/M8Auor5cU4toPFqHMIsO/rsE+DIZhNLRqJFWbVUxIyhoeaq0AANwuScB8ObY/erbMRorbiZNaZeOU9rm4/YyOaPvANE37Tk0zsfih4QCA83qmonlWSlABw+ry1NS1ttseK7c3kKvXZP/Hh4uDjisahqKxKO1/WC7lnMzZcAAjuzez3a94hgUGwzAaTu2Yhx9uOQXdW0gCo3FmCmbdNRQtG2prQSkahstJfu3jm5sChRY/+kc/WLmI+xRErwqvniYNkrGvWOtvMMqIN2LjPuM8DwWCdbHEVANTYaXHh44PTcddIzr6EzKN2HGoDIu3H8ZFJ+Xb6mu0YZMUwyQ4k24cgLeuPEmzr0d+liYxsE1uepDjXPFJKOVK9JzaMS9kWLCe01RaTIcaCgV+7Nyuhiscjv/N3HdScN9Uf4TWzHX7La+vfE1VJgLDJ4JLrhTLTvGXftqAc16bizW7i/Hx/G1B7S5883fcOXG5aZ2v2oY1DIZJcHq3jmymf99ZnXHOCc3QoUlmxO+dnebG0bJARNGbV56EGav3IjcjGT1bZqP7ozMAAHPuHoath0px9YRFZpcK4qKTWiA/OxVXDSjAO3PMfRvVZd2eYox5byHO7Ga8UtfxyoBTfGVhEXIzk1BRFRAuq3YVY9SrcwEAVw0owKz1+3HtB3/ij/tOw6HSSgBSfS590mUsYIHBMExEuJ0OnGhRgdcO024djLdmb8YnC7YDkBzr5/dqEdSuSVYy9hQdD9oPSA53ZWBV0yI7FXeOkEJ6y2sodNcIpT5WkwbGWfNllV7M2XAA78/bit82HECSy4Gv/zXAsK0QAlOWS9nns9YHNJvyKm9cCAw2STEMEzOaZ6fiyQu6h2yX7HLC7QoersaPOQnZacYmMfW661ZZ3jVFaYWxE/2ur5bjqgmL/GG2lR6fqcP9se9Xo22eFK685UCg4OMHv281bF/bsIbBMEzM+eCavth5xF5JjiSnAw+e3QW9WzdE9xZZ2HW03F+XSo3LESwwHjq7C7q3yEL7xhm4+6vlmLX+ABqmudGxSSYWbg2urhsOu44aa0BGmCX2fTR/u39bXSH41Zmb/NpSLGENg2GYmDOsc2NcJWeYq/nw2r54UC7noeRN9MjPwtUDC9C9hVQ36x+DCtAiOxVdmmlLjDidAYGh+Iwv7dMS/dvmIDcjGZ3l9qP7tDQ0g6m58uRWIT9DOPWzim2E9OprWs3beND29aMFCwyGYeKWoZ0a44YhbQEEBIZLV8+JiPD7fadh+m2DMfXWU/wVe9XtlOKLmapqvqPlwoyDO+QhU841aZBibHRpo8tqD8Xl/VqioYmpDJCc36E4ovPLjHnffO2P2oIFBsMwdYI+rRvi8n6t8NKlPU3bdGue5RcEah/G5zf0x5dj+2uWjW2Tm46tz47CKR1ykZMuZXUf0/khBrXPwa2ntceY/q3D6uuzF52Avx4ZYXpccfJbcaQs2JH//fLdBi1rj6gJDCKaQET7iWiVyfGhRFRERMvkv0ei1ReGYeo+LqcDz17UAy3lTHQzlLwHtYaRl5mMk9vmBLVVck36FDSC00G4S/YTNJZX/0txOXHniE5IcTvx5pUn4fmLe0TU9xuHtsPl/UKbtbSfI3jfqjDMXtEgmk7vDwG8DuBjizZzhRDnRLEPDMMkGLee3gHF5VUY3bel7XOSXA5sfmYUAOC8ns2xalcRbvx0qabNqB5SeY97J0nLvnZqkon1cjmQe0d2httJQaVJvrlpIEorPBjcQUpg/O6vXTiuiti6ZVh7vD5rk//18K5N8POafab9vOfM2Dq+o6ZhCCHmAKhe2AHDMEyYNEpPwsuje0VcvbdlozRNYUUzZtwxxL+dm5GE6we3DWpzUquGfmEBBAo5ApJJ7C6dADBbkx2QsuDtlKmPJrH2YQwgouVENJ2Iupk1IqKxRLSYiBYfOFD/SgYzDBNf2C3Eccuw9gCAQe1z/fsyTdZVB4CJ/xyAy/u1wqPndsXMcacGHXfo1mm/akDAdzKye1ObvYoesczDWAqgtRCihIhGAfgOgGEVLiHEOwDeAYA+ffrER1EVhmESFiUC6q4zO2m0hL8eHg6X07zkYocmmXj2ImM/SIMUF647pQ0+/GMbAOCnO4agTW46PpZzMxRnfiyJmcAQQhSrtqcR0ZtElCuEiH2wMcMwCU0nuT6W4rdQs+rxM+EkY6HQUI62CodUtxPtGqdjyr8Ha/Z3lPtw14iO/u1YEzOBQURNAewTQggi6gfJPHYoVv1hGIZRaJWThvVPjUSyK7h+U4aFySkSVj9+JkzkDwBYlj+vbaImMIjocwBDAeQSUSGARwG4AUAIMR7AJQBuJCIPgOMALhPCoA4wwzBMDDASFtHAoUtE/OS6fjhsUEwxHqC6Nkb36dNHLF4cvOoVwzAMYw4RLRFC9KnONWIdJcUwDMPUEVhgMAzDMLZggcEwDMPYggUGwzAMYwsWGAzDMIwtWGAwDMMwtmCBwTAMw9iCBQbDMAxjizqXuEdEBwCEXq7KmFwAiVyrKpE/fyJ/diCxP38if3Yg8PlbCyHyQjW2os4JjOpARIurm+lYl0nkz5/Inx1I7M+fyJ8dqNnPzyYphmGY/2/vXmOlqs4wjv+fgOKlDYJtDBWTo5FqqIlCqoJaQ1qL1hhtjIliE0kl8ZJa0NQYqB+M3zSaekmM0VRrYgiaeiV8kFa8a0TEIp4KyCEYRUWIVdSaNIivH9Y7sDko3ecwMOfMfn7J5Mxae8+evfa757yzL7OW1eKEYWZmtTQtYdzb6RXosCa3v8lth2a3v8lthza2v1HXMMzMbPCadoRhZmaD5IRhZma1NCZhSDpL0hpJfZLmdnp92k3SEZKelfS2pH9LmpP1YyX9U9La/Dsm6yXpztweKyVN7mwL9pykEZL+JWlRlo+UtDTb+LCk/bN+VJb7cnpPJ9e7HSQdIukRSaslrZI0tWGxvyb3+15JCyQd0K3xl3S/pE2Seit1A461pJk5/1pJM+u8dyMShqQRwF3Ab4CJwAxJEzu7Vm33NfCniJgITAH+kG2cCyyJiAnAkixD2RYT8nEZcPe+X+W2mwOsqpRvBm6LiKOBT4FZWT8L+DTrb8v5hrs7gKci4ljgeMp2aETsJR0OzAZ+HhHHASOAi+je+D8AnNWvbkCxljSWMmz2ycBJwA2tJLNbEdH1D2AqsLhSngfM6/R67eU2Pwn8GlgDjMu6ccCafH4PMKMy//b5huMDGJ8flF8CiwBRft06sv8+ACwGpubzkTmfOt2GPWj7aGB9/zY0KPaHA+8DYzOei4Azuzn+QA/QO9hYAzOAeyr1O833fY9GHGGwY4dq2ZB1XSkPsScBS4HDIuKjnLQROCyfd9s2uR24Dvgmy4cCn0XE11mutm9723P6lpx/uDoS2Az8LU/J/VXSwTQk9hHxAXAr8B7wESWey2lO/GHgsR7UPtCUhNEYkn4APApcHRGfV6dF+SrRdfdRSzoH2BQRyzu9Lh0yEpgM3B0Rk4D/suOUBNC9sQfIUynnURLnT4CD2fWUTWPszVg3JWF8ABxRKY/Puq4iaT9KspgfEY9l9ceSxuX0ccCmrO+mbXIqcK6kd4GHKKel7gAOkTQy56m2b3vbc/po4JN9ucJttgHYEBFLs/wIJYE0IfYAZwDrI2JzRGwFHqPsE02JPww81oPaB5qSMJYBE/Kuif0pF8QWdnid2kqSgPuAVRHxl8qkhUDrDoiZlGsbrfpL8i6KKcCWyiHtsBIR8yJifET0UGL7TET8DngWuCBn69/21ja5IOcftt++I2Ij8L6kY7LqV8DbNCD26T1giqSD8nPQan8j4p8GGuvFwHRJY/IIbXrW7V6nL97sw4tEZwPvAOuA6zu9PnuhfadRDkNXAivycTbl3OwSYC3wNDA25xflzrF1wFuUO0w63o42bIdpwKJ8fhTwGtAH/B0YlfUHZLkvpx/V6fVuQ7tPAF7P+D8BjGlS7IEbgdVAL/AgMKpb4w8soFyr2Uo5upw1mFgDl+Y26AN+X+e93TWImZnV0pRTUmZmtoecMMzMrBYnDDMzq8UJw8zManHCMDOzWpwwrLEkfZl/eyRd3OZl/7lf+ZV2Lt+sE5wwzEpHbgNKGJVfEH+fnRJGRJwywHUyG3KcMMzgJuAXklbkuAojJN0iaVmOIXA5gKRpkl6UtJDyS2IkPSFpeY7FcFnW3QQcmMubn3WtoxnlsnslvSXpwsqyn9OOMS3m56+WzYaM//ctyawJ5gLXRsQ5APmPf0tEnChpFPCypH/kvJOB4yJifZYvjYj/SDoQWCbp0YiYK+mqiDjhO97rfMqvso8HfpSveSGnTQJ+BnwIvEzpD+ml9jfXbHB8hGG2q+mU/ndWULqIP5QyAA3Aa5VkATBb0pvAq5TO3Cawe6cBCyJiW0R8DDwPnFhZ9oaI+IbStUtPW1pj1iY+wjDblYA/RsROnbFJmkbpOrxaPoMyGM9Xkp6j9FM0WP+rPN+GP582xPgIwwy+AH5YKS8Grszu4pH00xyQqL/RlKE+v5J0LGVo3Jatrdf38yJwYV4n+TFwOqUDPLMhz99gzEoPr9vy1NIDlLE0eoA38sLzZuC33/G6p4ArJK2iDH35amXavcBKSW9E6Wq95XHKcKFvUnoXvi4iNmbCMRvS3FutmZnV4lNSZmZWixOGmZnV4oRhZma1OGGYmVktThhmZlaLE4aZmdXihGFmZrV8C1Yffl42BQvIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f23517dcd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "machine.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Training\n",
    "\n",
    "The training and test loss start to diverge around 250 iterations. Then the model starts overfitting on the training data. We could probably do with some regularisation to prevent overfitting and keep the test loss in sync with the training loss.\n",
    "\n",
    "We also need to set up the save weights so it only saves in the test loss has not increased by a margin.\n",
    "\n",
    "Let's try with some regularisation using Dropout and L2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2 # L2-regularisation\n",
    "l2_lambda = 0.01\n",
    "\n",
    "class LWAtt_reg(LudwigModel):\n",
    "    \"\"\" Version of our Ludwig model with attention.\"\"\"\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\" Build the model. \"\"\"\n",
    "        print(\"Building model\")\n",
    "        self._load_shared_embedding()\n",
    "        # Define inputs\n",
    "        inputs_encoder = Input(shape=(self.encoder_seq_length,))\n",
    "        inputs_decoder = Input(shape=(self.decoder_seq_length,))\n",
    "        \n",
    "        # Define Shared Embedding Layer\n",
    "        Shared_Embedding = Embedding(\n",
    "            output_dim=self.word_embedding_size,\n",
    "            input_dim=self.num_encoder_tokens,\n",
    "            weights=[self.embedding_matrix]   \n",
    "        )\n",
    "        # Ah our problem is that our shared embedding has encoder length but we are also using on decoder\n",
    "        \n",
    "        embedded_inputs_encoder = Shared_Embedding(inputs_encoder)\n",
    "        embedded_inputs_decoder = Shared_Embedding(inputs_decoder)\n",
    "        \n",
    "        # Define LSTMs - these return output state h for each timestep (as we have r_s=True)\n",
    "        encoder_LSTM = LSTM(self.latent_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)\n",
    "        decoder_LSTM = LSTM(self.latent_dim, return_sequences=True,  dropout=0.2, recurrent_dropout=0.2)\n",
    "        \n",
    "        # So output of this is, e.g. BS, 300, 128, i.e. an h vector for each TS\n",
    "        encoder_context = encoder_LSTM(embedded_inputs_encoder)\n",
    "        \n",
    "        # Add attention to encoder encodings - here we are swapping the dims to BS, 128, 300\n",
    "        a = Permute((2, 1))(encoder_context)\n",
    "\n",
    "        a = Dense(self.encoder_seq_length, activation='softmax', use_bias=False, kernel_regularizer=l2(l2_lambda))(a)\n",
    "\n",
    "        a_probs = Permute((2, 1), name='attention_vec_a')(a)\n",
    "        att_mul_1 = multiply([encoder_context, a_probs])\n",
    "\n",
    "        # Sum over time dimension\n",
    "        att_mul_1 = Lambda(lambda x: K.sum(x, axis=1), name=\"sum_over_time_att_1\")(att_mul_1)\n",
    "\n",
    "        decoder_context = decoder_LSTM(embedded_inputs_decoder)\n",
    "         # Add attention to answer encodings\n",
    "        b = Permute((2, 1))(decoder_context)\n",
    "        b = Dense(self.decoder_seq_length, activation='softmax', use_bias=False, kernel_regularizer=l2(l2_lambda))(b)\n",
    "       \n",
    "        b_probs = Permute((2, 1), name='attention_vec_b')(b)\n",
    "        att_mul_2 = multiply([decoder_context, b_probs])\n",
    "        att_mul_2 = Lambda(lambda x: K.sum(x, axis=1), name=\"sum_over_time_att_2\")(att_mul_2)\n",
    "        \n",
    "        decoder1 = concatenate([att_mul_1, att_mul_2], axis=1)\n",
    "        \n",
    "        outputs = Dense(self.num_decoder_tokens, activation='softmax', kernel_regularizer=l2(l2_lambda))(decoder1)\n",
    "        # tie it together [article, summary] [word]\n",
    "        self.model = Model(inputs=[inputs_encoder, inputs_decoder], outputs=outputs)\n",
    "        self.infdec = self.model\n",
    "        print(\"Compiling model\")\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizers\n",
      "Our input data has shape (30000, 300) and our output data has shape (30000, 22)\n",
      "Generating training and test data\n",
      "Building model\n",
      "Loading GloVe 100d embeddings from file\n",
      "Found 400000 word vectors.\n",
      "Building embedding matrix\n",
      "Compiling model\n",
      "No existing weights found\n"
     ]
    }
   ],
   "source": [
    "reg_machine = LWAtt_reg(\n",
    "    encoder_texts=[d[0] for d in data],\n",
    "    decoder_texts=[d[1] for d in data],\n",
    "    encoder_seq_length=300,\n",
    "    decoder_seq_length=22,\n",
    "    num_encoder_tokens=2500,\n",
    "    num_decoder_tokens=2500,\n",
    "    latent_dim=128,\n",
    "    weights_file=\"class_LWattmodel_att_reg.hdf5\",\n",
    "    training_set_size=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 41s 16ms/step - loss: 5.8585 - val_loss: 3.7435\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 39s 16ms/step - loss: 3.5613 - val_loss: 3.4640\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 42s 16ms/step - loss: 3.5851 - val_loss: 3.6235\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 42s 16ms/step - loss: 3.5451 - val_loss: 3.3144\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 42s 17ms/step - loss: 3.4953 - val_loss: 3.7243\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 40s 16ms/step - loss: 3.4705 - val_loss: 3.5657\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 39s 16ms/step - loss: 3.4088 - val_loss: 3.3930\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 3.5786 - val_loss: 3.6329\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 39s 16ms/step - loss: 3.4682 - val_loss: 3.4289\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 41s 16ms/step - loss: 3.4713 - val_loss: 3.5387\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 40s 16ms/step - loss: 3.4984 - val_loss: 3.3935\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 41s 16ms/step - loss: 3.5042 - val_loss: 3.2808\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 40s 16ms/step - loss: 3.3541 - val_loss: 3.4928\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 41s 17ms/step - loss: 3.4211 - val_loss: 3.4991\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 42s 16ms/step - loss: 3.4083 - val_loss: 3.3365\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 41s 16ms/step - loss: 3.4602 - val_loss: 3.4191\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 40s 17ms/step - loss: 3.3773 - val_loss: 3.3890\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 41s 16ms/step - loss: 3.3801 - val_loss: 3.2769\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 44s 16ms/step - loss: 3.3632 - val_loss: 3.4281\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 39s 16ms/step - loss: 3.3568 - val_loss: 3.4528\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 40s 16ms/step - loss: 3.3688 - val_loss: 3.4102\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 41s 16ms/step - loss: 3.3036 - val_loss: 3.3795\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 38s 16ms/step - loss: 3.3080 - val_loss: 3.4724\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 39s 16ms/step - loss: 3.2087 - val_loss: 3.5041\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 41s 16ms/step - loss: 3.3172 - val_loss: 3.3066\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 40s 16ms/step - loss: 3.2845 - val_loss: 3.1063\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 40s 16ms/step - loss: 3.2907 - val_loss: 3.3953\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 40s 16ms/step - loss: 3.3645 - val_loss: 3.3565\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 42s 16ms/step - loss: 3.3580 - val_loss: 3.3912\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 40s 16ms/step - loss: 3.3609 - val_loss: 3.0767\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 40s 16ms/step - loss: 3.2428 - val_loss: 3.4914\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 41s 16ms/step - loss: 3.2531 - val_loss: 3.3661\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 41s 16ms/step - loss: 3.3147 - val_loss: 3.3799\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 41s 16ms/step - loss: 3.2773 - val_loss: 3.1823\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 39s 16ms/step - loss: 3.2502 - val_loss: 3.2283\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 42s 16ms/step - loss: 3.2995 - val_loss: 3.1113\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 40s 16ms/step - loss: 3.2758 - val_loss: 3.2555\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 40s 16ms/step - loss: 3.2518 - val_loss: 3.2636\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 41s 16ms/step - loss: 3.2812 - val_loss: 3.3723\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 39s 16ms/step - loss: 3.1772 - val_loss: 3.1369\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 43s 16ms/step - loss: 3.2390 - val_loss: 3.2860\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 44s 16ms/step - loss: 3.2690 - val_loss: 3.3590\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 39s 16ms/step - loss: 3.1603 - val_loss: 3.3254\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465/2465 [==============================] - 40s 16ms/step - loss: 3.3000 - val_loss: 3.2565\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 38s 16ms/step - loss: 3.1821 - val_loss: 3.2911\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 39s 16ms/step - loss: 3.2758 - val_loss: 3.2011\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 39s 16ms/step - loss: 3.1438 - val_loss: 3.2925\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 41s 16ms/step - loss: 3.2567 - val_loss: 3.1641\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 38s 16ms/step - loss: 3.1308 - val_loss: 3.4368\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 40s 16ms/step - loss: 3.2212 - val_loss: 3.5263\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 41s 16ms/step - loss: 3.1657 - val_loss: 3.2261\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 39s 16ms/step - loss: 3.2674 - val_loss: 3.1343\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 39s 16ms/step - loss: 3.1409 - val_loss: 3.1230\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 3.1051 - val_loss: 3.3345\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 39s 16ms/step - loss: 3.2254 - val_loss: 3.2473\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 41s 16ms/step - loss: 3.2083 - val_loss: 3.2115\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 40s 16ms/step - loss: 3.2409 - val_loss: 3.3282\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 41s 16ms/step - loss: 3.2507 - val_loss: 3.1066\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 41s 16ms/step - loss: 3.1977 - val_loss: 3.2406\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 41s 16ms/step - loss: 3.2428 - val_loss: 3.3955\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 41s 16ms/step - loss: 3.2187 - val_loss: 3.2829\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 39s 16ms/step - loss: 3.2786 - val_loss: 3.3392\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 39s 16ms/step - loss: 3.2855 - val_loss: 3.1489\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 40s 16ms/step - loss: 3.2259 - val_loss: 3.0721\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 40s 16ms/step - loss: 3.1724 - val_loss: 3.2453\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 40s 16ms/step - loss: 3.1946 - val_loss: 3.1928\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 42s 16ms/step - loss: 3.2400 - val_loss: 3.2534\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 41s 16ms/step - loss: 3.2364 - val_loss: 3.2306\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 43s 16ms/step - loss: 3.2311 - val_loss: 3.3466\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 40s 16ms/step - loss: 3.1963 - val_loss: 3.3143\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 3.2861 - val_loss: 3.1222\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 40s 16ms/step - loss: 3.2151 - val_loss: 3.3069\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 3.1403 - val_loss: 3.2566\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 42s 16ms/step - loss: 3.1372 - val_loss: 3.2774\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 41s 16ms/step - loss: 3.1044 - val_loss: 3.1843\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 3.1423 - val_loss: 3.3621\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 39s 16ms/step - loss: 3.2599 - val_loss: 3.4877\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 41s 16ms/step - loss: 3.1893 - val_loss: 3.4566\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 42s 16ms/step - loss: 3.2849 - val_loss: 3.2276\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 40s 16ms/step - loss: 3.2352 - val_loss: 3.2330\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 41s 16ms/step - loss: 3.2137 - val_loss: 3.3643\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 40s 16ms/step - loss: 3.1929 - val_loss: 3.0203\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 40s 16ms/step - loss: 3.1013 - val_loss: 3.2803\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 41s 16ms/step - loss: 3.1193 - val_loss: 3.1191\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 40s 16ms/step - loss: 3.1813 - val_loss: 3.0673\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 42s 16ms/step - loss: 3.0490 - val_loss: 3.2360\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 40s 16ms/step - loss: 3.1306 - val_loss: 3.0925\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 40s 16ms/step - loss: 3.2643 - val_loss: 3.4264\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 41s 16ms/step - loss: 3.0234 - val_loss: 3.0887\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 42s 16ms/step - loss: 3.1706 - val_loss: 3.3364\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 40s 16ms/step - loss: 3.2226 - val_loss: 3.1001\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 41s 16ms/step - loss: 3.1665 - val_loss: 3.1915\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 38s 16ms/step - loss: 3.1629 - val_loss: 3.1635\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 40s 16ms/step - loss: 3.2503 - val_loss: 3.2860\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 38s 16ms/step - loss: 3.1435 - val_loss: 3.0183\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 42s 16ms/step - loss: 3.1329 - val_loss: 3.1292\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a method for retrieving and viewing in a single web browser instance operating on a user's computer comprising the sequential steps of from said single web browser a search request to an internet se\n",
      "\n",
      "Predicted title is: method and method for a a a a a a  \n",
      "Actual title is: multi window internet search with webpage  \n",
      "---\n",
      "Sample of claim text: 1 a rotation amount detecting device for a rotation body comprising a first rotation body provided with a first surface portion being in contact with a sheet being the first rotation body rotating alo\n",
      "\n",
      "Predicted title is: method and method for a a a a a a a  \n",
      "Actual title is: rotation amount detecting device for rotation body length measuring device and image forming apparatus  \n",
      "---\n",
      "Sample of claim text: 1 a system comprising a control module configured to output a signal a voltage module directly coupled to the control module a complex programmable logic device coupled to the control module for recei\n",
      "\n",
      "Predicted title is: method and method for a a a a a a  \n",
      "Actual title is: system  \n",
      "---\n",
      "Sample of claim text: 1 a compressed domain compression apparatus comprising a partial decoder that partially an encoded piece of data to generate a partially decoded piece of data a least significant bit plane removal uni\n",
      "\n",
      "Predicted title is: method and method for a a a a a a  \n",
      "Actual title is: compressed domain system and method for compression in encoded data  \n",
      "---\n",
      "Sample of claim text: 1 a method for correcting in magnetic imaging comprising acquiring at least one artifact calibration scan with a timing that corresponds to that of an actual diagnostic imaging scan acquiring at least\n",
      "\n",
      "Predicted title is: method and method for a a a a a a  \n",
      "Actual title is: correction of in magnetic imaging  \n",
      "---\n",
      "Training for epoch 1\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 42s 16ms/step - loss: 3.2278 - val_loss: 3.1951\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 40s 16ms/step - loss: 3.0517 - val_loss: 3.1360\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 41s 16ms/step - loss: 3.2036 - val_loss: 3.2570\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 43s 16ms/step - loss: 3.1496 - val_loss: 3.2548\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 41s 16ms/step - loss: 3.0776 - val_loss: 3.4882\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 40s 16ms/step - loss: 3.2314 - val_loss: 3.1870\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 39s 16ms/step - loss: 3.1242 - val_loss: 2.9646\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 3.2733 - val_loss: 3.0056\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 40s 16ms/step - loss: 3.1904 - val_loss: 3.2293\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 42s 16ms/step - loss: 3.1980 - val_loss: 3.1943\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 38s 16ms/step - loss: 3.0938 - val_loss: 3.0823\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 41s 16ms/step - loss: 3.2110 - val_loss: 2.9999\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 41s 17ms/step - loss: 3.0283 - val_loss: 3.1210\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 41s 16ms/step - loss: 3.0056 - val_loss: 3.2025\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 41s 16ms/step - loss: 3.1435 - val_loss: 3.1291\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 41s 16ms/step - loss: 3.1756 - val_loss: 3.0356\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 38s 16ms/step - loss: 3.0862 - val_loss: 3.1433\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 41s 16ms/step - loss: 3.0870 - val_loss: 3.1611\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 44s 16ms/step - loss: 3.2078 - val_loss: 3.3182\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 40s 16ms/step - loss: 3.1042 - val_loss: 3.4193\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 40s 16ms/step - loss: 3.1811 - val_loss: 3.2685\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 41s 16ms/step - loss: 3.1544 - val_loss: 3.2707\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 39s 16ms/step - loss: 3.1741 - val_loss: 3.2301\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 39s 16ms/step - loss: 3.0677 - val_loss: 3.3834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 40s 16ms/step - loss: 3.0886 - val_loss: 3.1539\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 41s 16ms/step - loss: 3.2480 - val_loss: 3.1027\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 39s 16ms/step - loss: 3.2251 - val_loss: 3.2005\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 41s 16ms/step - loss: 3.2138 - val_loss: 3.2061\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 42s 16ms/step - loss: 3.2048 - val_loss: 3.3016\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 41s 16ms/step - loss: 3.1553 - val_loss: 3.0200\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 40s 16ms/step - loss: 3.2373 - val_loss: 3.1796\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 43s 16ms/step - loss: 3.1959 - val_loss: 3.2657\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 41s 16ms/step - loss: 3.1572 - val_loss: 3.1958\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 41s 16ms/step - loss: 3.1642 - val_loss: 3.1339\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 38s 16ms/step - loss: 3.1896 - val_loss: 3.1840\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 42s 16ms/step - loss: 3.2080 - val_loss: 3.0749\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 41s 16ms/step - loss: 3.1696 - val_loss: 3.1245\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 39s 16ms/step - loss: 3.1333 - val_loss: 3.1572\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 41s 16ms/step - loss: 3.1062 - val_loss: 3.0877\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 40s 16ms/step - loss: 3.1346 - val_loss: 2.9710\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 42s 16ms/step - loss: 3.1878 - val_loss: 3.2469\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 43s 16ms/step - loss: 3.0903 - val_loss: 3.1244\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 40s 16ms/step - loss: 3.1589 - val_loss: 3.0457\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2465/2465 [==============================] - 40s 16ms/step - loss: 3.1462 - val_loss: 3.1066\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 38s 16ms/step - loss: 3.1441 - val_loss: 3.3154\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 40s 16ms/step - loss: 3.1675 - val_loss: 3.1576\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 40s 16ms/step - loss: 3.0792 - val_loss: 3.2398\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 41s 16ms/step - loss: 3.1739 - val_loss: 3.0339\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 38s 16ms/step - loss: 3.0304 - val_loss: 3.3837\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 41s 16ms/step - loss: 3.1445 - val_loss: 3.2370\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 41s 16ms/step - loss: 3.1281 - val_loss: 3.2471\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 38s 16ms/step - loss: 3.1820 - val_loss: 2.9276\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 40s 16ms/step - loss: 3.0936 - val_loss: 3.2531\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 38s 16ms/step - loss: 3.1480 - val_loss: 3.1541\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 39s 16ms/step - loss: 3.1544 - val_loss: 3.2062\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 42s 16ms/step - loss: 3.0928 - val_loss: 3.1812\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 41s 16ms/step - loss: 3.1832 - val_loss: 3.2479\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 41s 16ms/step - loss: 3.1365 - val_loss: 2.9679\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 41s 16ms/step - loss: 3.1723 - val_loss: 3.2239\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 40s 16ms/step - loss: 3.1454 - val_loss: 3.2540\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 40s 16ms/step - loss: 3.1063 - val_loss: 3.2476\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 39s 16ms/step - loss: 3.2316 - val_loss: 3.0711\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 39s 16ms/step - loss: 3.0744 - val_loss: 3.1800\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 40s 16ms/step - loss: 3.0855 - val_loss: 3.1022\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 39s 16ms/step - loss: 3.0422 - val_loss: 3.2656\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 40s 16ms/step - loss: 3.0500 - val_loss: 3.1318\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2562/2562 [==============================] - 42s 16ms/step - loss: 3.1140 - val_loss: 3.1996\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 39s 16ms/step - loss: 3.0808 - val_loss: 3.0758\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 3.2294 - val_loss: 3.2096\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 39s 16ms/step - loss: 3.0787 - val_loss: 3.2973\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 3.1772 - val_loss: 3.0668\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 39s 16ms/step - loss: 3.1004 - val_loss: 3.1539\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 3.0961 - val_loss: 3.2998\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 41s 16ms/step - loss: 3.1300 - val_loss: 3.2245\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 40s 16ms/step - loss: 3.1822 - val_loss: 3.0498\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 3.0125 - val_loss: 3.2230\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 40s 16ms/step - loss: 3.1486 - val_loss: 3.1016\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 41s 16ms/step - loss: 3.1483 - val_loss: 3.2969\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 42s 16ms/step - loss: 3.0804 - val_loss: 3.2554\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 41s 16ms/step - loss: 3.1082 - val_loss: 3.1056\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 41s 16ms/step - loss: 3.1922 - val_loss: 3.0940\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 39s 16ms/step - loss: 3.1244 - val_loss: 2.9258\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 40s 16ms/step - loss: 3.0724 - val_loss: 3.1622\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 42s 16ms/step - loss: 3.0670 - val_loss: 3.0292\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 40s 16ms/step - loss: 3.1622 - val_loss: 3.0035\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 43s 16ms/step - loss: 3.0194 - val_loss: 3.2342\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2534/2534 [==============================] - 41s 16ms/step - loss: 3.0942 - val_loss: 2.9889\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 40s 16ms/step - loss: 3.1924 - val_loss: 3.2250\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 41s 16ms/step - loss: 2.9823 - val_loss: 3.1429\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 40s 16ms/step - loss: 3.1437 - val_loss: 3.0710\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 40s 16ms/step - loss: 3.2398 - val_loss: 3.0160\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 42s 16ms/step - loss: 3.0875 - val_loss: 2.9603\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 38s 16ms/step - loss: 3.0628 - val_loss: 3.1529\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 40s 16ms/step - loss: 3.1071 - val_loss: 3.1776\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 38s 16ms/step - loss: 3.0335 - val_loss: 3.0485\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 42s 16ms/step - loss: 3.0978 - val_loss: 3.1624\n",
      "------------------------------------------\n",
      "Sample of claim text: 1 a chip adapted in a computer system comprising a computer system environment information monitoring module for monitoring a computer system environment information of the computer system a control c\n",
      "\n",
      "Predicted title is: method and apparatus for a data  \n",
      "Actual title is: chip and computer system  \n",
      "---\n",
      "Sample of claim text: 1 a method of manufacturing digital display the method comprising manufacturing a plurality of digital display the plurality of display comprising a first display panel having a first size and shape w\n",
      "\n",
      "Predicted title is: method and apparatus for a data  \n",
      "Actual title is: display with different  \n",
      "---\n",
      "Sample of claim text: 1 a method comprising generating a list of computers that are part of a selected domain wherein the list is automatically with the list of computers by selecting the domain via a job scheduler module \n",
      "\n",
      "Predicted title is: method and system for a data  \n",
      "Actual title is: job scheduler for remote maintenance of servers and  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method for managing device information comprising under control of a computer system configured with executable instructions monitoring by a first sensor of a mobile device mo\n",
      "\n",
      "Predicted title is: method and apparatus for a data  \n",
      "Actual title is: managing information of a user device  \n",
      "---\n",
      "Sample of claim text: output hardware units and meeting corresponding to said plurality of output hardware units said output hardware unit that performed authentication of the user being to output material saved in the sto\n",
      "\n",
      "Predicted title is: method and apparatus for a data  \n",
      "Actual title is: material output system for outputting meeting material for participant in meeting  \n",
      "---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2351826780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_machine.train(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 40s 15ms/step - loss: 3.1682 - val_loss: 3.0517\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 37s 15ms/step - loss: 2.9829 - val_loss: 3.1555\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 40s 16ms/step - loss: 3.1525 - val_loss: 3.3748\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 42s 16ms/step - loss: 3.0892 - val_loss: 3.1288\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 40s 16ms/step - loss: 2.9887 - val_loss: 3.4371\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 40s 16ms/step - loss: 3.1568 - val_loss: 3.0744\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 38s 16ms/step - loss: 3.1364 - val_loss: 2.9819\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 3.1447 - val_loss: 2.9785\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 40s 16ms/step - loss: 3.0267 - val_loss: 3.1802\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 42s 16ms/step - loss: 3.1194 - val_loss: 3.1906\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 40s 16ms/step - loss: 3.0675 - val_loss: 2.7685\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 41s 16ms/step - loss: 3.1106 - val_loss: 3.0272\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 40s 16ms/step - loss: 2.9843 - val_loss: 3.0885\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 41s 16ms/step - loss: 3.1496 - val_loss: 3.2530\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 40s 16ms/step - loss: 3.1466 - val_loss: 3.0103\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 40s 16ms/step - loss: 3.0234 - val_loss: 3.0530\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 40s 16ms/step - loss: 3.0545 - val_loss: 3.2473\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 40s 16ms/step - loss: 3.1500 - val_loss: 3.2253\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 44s 16ms/step - loss: 3.0335 - val_loss: 3.3154\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 40s 16ms/step - loss: 3.1344 - val_loss: 3.1062\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 40s 16ms/step - loss: 3.1316 - val_loss: 3.1059\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 41s 16ms/step - loss: 3.0775 - val_loss: 3.2087\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 39s 16ms/step - loss: 3.1228 - val_loss: 3.1668\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 40s 16ms/step - loss: 3.0295 - val_loss: 3.2340\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 41s 16ms/step - loss: 3.0325 - val_loss: 3.3165\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 42s 16ms/step - loss: 3.1524 - val_loss: 3.0066\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 40s 16ms/step - loss: 3.0206 - val_loss: 3.1282\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 41s 16ms/step - loss: 3.1084 - val_loss: 3.0569\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 42s 16ms/step - loss: 3.1781 - val_loss: 3.1583\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 41s 16ms/step - loss: 3.2046 - val_loss: 2.9568\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 43s 17ms/step - loss: 3.1184 - val_loss: 3.2998\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 43s 16ms/step - loss: 3.1064 - val_loss: 3.2853\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "1472/2567 [================>.............] - ETA: 17s - loss: 2.9281"
     ]
    }
   ],
   "source": [
    "reg_machine.train(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Dropout & Regularisation\n",
    "\n",
    "Interesting. \n",
    "\n",
    "While adding Dropout and regularisation appears to keep the training and test loss in sync, the model seems to struggle more to find a optima - the loss is higher and the model appears to find a pattern and stick to it (trapped in local minima?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
