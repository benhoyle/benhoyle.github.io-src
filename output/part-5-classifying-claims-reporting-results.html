<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  5. Classifying Claims - Reporting Results | Machine Learning Projects
</title>
  <link rel="canonical" href="./part-5-classifying-claims-reporting-results.html">


  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/theme.css">

  
  <meta name="description" content="This post looks at our results and what we have learnt from the project.">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="./">Machine Learning Projects</a></h1>
      <p class="text-muted">A selection of machine learning projects</p>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
  <article class="article">
    <div class="content">
      <h1>5. Classifying Claims - Reporting Results</h1>
<p>Here we will look at what we have learnt during our project to classify patent claims. The aim of doing this is to consolidate what we have learnt, and practice a methodology for presenting our results.</p>
<p>The post <a href="https://machinelearningmastery.com/how-to-use-machine-learning-results/">here</a> suggests the following structure for reporting our results:
1. Context - Define and describe why we are undertaking the project;
2. Problem - Describe the problem we are looking to solve;
3. Solution - Describe the solution our results suggest;
4. Findings - Present our results;
5. Limitations - Present the limitations we uncovered; and
6. Conclusions - Revisit and summarise the context, problem and solution (or why, question and answer)</p>
<p>We will look at these one by one.</p>
<h2>1. Context</h2>
<p>As described in our first post, the project was undertaken to practice applying a methodical approach to real-world patent data. </p>
<p>Within the world of patent law, we have sets of patent claims, which set out the scope of legal protection. These claims, as they are contained in a patent application, are assigned a patent classification, which is a hierarchical code that categorises the subject-matter of the claims. </p>
<p>Patent specifications are published and made available for download in bulk. This provides large labelled datasets we can use to practice supervised machine learning. In particular, we can extract a main independent claim and an International Patent Classification (IPC) code as data for machine learning algorithms. To simplify things, we can start with the first letter of the IPC code, which assigns each patent application to one of eight classes.</p>
<h2>2. Problem</h2>
<p>The problem we are looking to solve is to develop a machine learning algorithm that, given the text of a patent claim, can predict the first letter of the IPC code, i.e. the first level of subject matter classification.</p>
<h2>3. Solution</h2>
<p>Our results indicate that a multi-layer perceptron or a Naive Bayes classifier would be the best at performing this task. </p>
<p>However, from our results it does not seem possible to implement an accurate classifier for this problem. The best solutions return an accuracy of around 60%. While better than chance given the proportions of the data (~20%) this is still not necessarily high enough to confidently apply a classifier in a processing pipeline, e.g. to differentially process the text of the patent claim. </p>
<p>It would, though, appear possible to provide probabilities for classes that could support human decision making.   </p>
<h2>4. Findings</h2>
<p>Our finding will be split into three sections:
1. Findings regarding the nature of the problem and the data;
2. Findings regarding suitable machine learning algorithms; and
3. Findings regarding specific algorithms.</p>
<h3>4.1 Findings regarding the nature of the problem and the data</h3>
<p>A general finding was that changes in algorithms, parameters, and model archectures only led to small improvements in accuracy (e.g. often along the lines of 1-2%). Generally, the best classification accuracy was fixed at 60%.</p>
<p>The project reinforced the finding that success in text classification tasks is more about the data pre-processing than the machine learning algorithms. </p>
<p>For example, a</p>
<h3>4.2 Findings regarding suitable machine learning algorithms</h3>
<p>The results of our spot-checks were as follows:</p>
<table>
<thead>
<tr>
<th>Classifer</th>
<th align="center">Average Accuracy (%)</th>
<th align="right">Standard Deviation (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>col 3 is</td>
<td align="center">right-aligned</td>
<td align="right">$1600</td>
</tr>
<tr>
<td>col 2 is</td>
<td align="center">centered</td>
<td align="right">$12</td>
</tr>
<tr>
<td>zebra stripes</td>
<td align="center">are neat</td>
<td align="right">$1</td>
</tr>
<tr>
<td>col 3 is</td>
<td align="center">right-aligned</td>
<td align="right">$1600</td>
</tr>
<tr>
<td>col 2 is</td>
<td align="center">centered</td>
<td align="right">$12</td>
</tr>
<tr>
<td>zebra stripes</td>
<td align="center">are neat</td>
<td align="right">$1</td>
</tr>
</tbody>
</table>
<ul>
<li>A random choice in line with the label proportions returned an average classification accuracy of 20.14% (standard deviation 0.13%)</li>
<li>A Naive Bayes classifier returned an average classification accuracy of 58.90% (standard deviation 1.12%)</li>
<li>A k-Nearest Neighbours classifier with 8 clusters returned an average classification accuracy of 30.15% (standard deviation 2.74%)</li>
<li>A stochastic gradient descent classifier returned an average classification accuracy of 56.79% (standard deviation 0.75%)</li>
<li>A multilayer perceClassifier MLPClassifier has an average classification accuracy of 61.20 (0.54)
Classifier DecisionTreeClassifier has an average classification accuracy of 47.33 (0.52)
Classifier RandomForestClassifier has an average classification accuracy of 50.01 (1.17)
Classifier AdaBoostClassifier has an average classification accuracy of 34.34 (1.31)
Classifier SVC has an average classification accuracy of 62.52 (0.80)</li>
</ul>
    </div>
    <hr/>
    <footer>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-03-23T10:12:38.836349+00:00">
          <i class="fa fa-clock-o"></i>
          Fri 23 March 2018
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="./category/claim-classification.html">Claim Classification</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="./author/ben-hoyle.html">Ben Hoyle</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="./tag/reporting_results.html">#reporting_results</a>          </li>
      </ul>
    </footer>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <p class="col-sm-6 text-sm-left">
    <a href="https://www.linkedin.com/in/benhoyle/" class="text-muted" target="_blank">Ben Hoyle</a>
  </p>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" class="text-muted" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" class="text-muted" target="_blank">Adapted from &#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>
</html>