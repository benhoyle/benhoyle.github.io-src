{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the Associate - Legal Machine Learning\n",
    "\n",
    "This post is part of a series that look at applying machine learning to legal information. We will start off by looking at case law for exclued subject matter in the United Kingdom and Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "Machine learning algorithms typically operate on a vector of numbers (integers or floats). The patent information we have tends to be text-based. This post will explore how to turn our text into numeric feature vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good place to start is to read these tutorials:\n",
    "* Working with text data (from scikit learn) http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* Learning to Classify Text (from nltk) http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prequisites\n",
    "\n",
    "Before you start you need to install Python and a bucketful of useful libraries. The best way to do this is to use [Anaconda](https://www.continuum.io/downloads). On my ten-year-old laptop running Puppy Linux (which was in the loft for a year or so covered in woodlouse excrement) this simply involved running the script. No compiling from source. No version errors. No messing with pip. \n",
    "\n",
    "I find that Jupyter (formerly iPython) notebooks are a great way to iteratively code. You can test out ideas block by block, shift stuff around, output and document all in the same tool. You can also easily export to HTML with one click (hence this post). To start a notebook having installed Anaconda run the following:\n",
    "```\n",
    "jypyter notebook\n",
    "```\n",
    "This will start the notebook server on your local machine and open your browser. By default the notebooks are served at *localhost:8888*. To access across a local network use the *-ip* flag with your IP address (e.g. -ip 192.168.1.2) and then point your browser at *[your-ip]:8888* (use -p to change the port).\n",
    "\n",
    "This notebook also makes use of a library I hacked together for accessing the EPO OPS API. You can clone this library from: https://github.com/benhoyle/EPOops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to save the retrieved data - either in a database or as a series of flat files. We need to investigate how to build a scikit learn corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we need to retrieve up to 442 full text descriptions, we will adapt George Song's OPS Client that includes support for throttling. The code can be cloned from here: https://github.com/55minutes/python-epo-ops-client (my fork is here: https://github.com/benhoyle/python-epo-ops-client/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the path for my fork of the client to the system path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/root/projects/caselawml/python-epo-ops-client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import epo_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Key and Secret from config file called \"config.ini\" in the same directory as this notebook\n",
    "import configparser\n",
    "parser = configparser.ConfigParser()\n",
    "parser.read(os.path.abspath(os.getcwd() + '/config.ini'))\n",
    "consumer_key = parser.get('Login Parameters', 'C_KEY')\n",
    "consumer_secret = parser.get('Login Parameters', 'C_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup a new registered EPO OPS client that returns JSON\n",
    "registered_client = epo_ops.RegisteredClient(\n",
    "    key=consumer_key, \n",
    "    secret=consumer_secret, \n",
    "    accept_type='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Client Code is working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['241 5387 Cosmetic Uses Of Electromagnetic Radiation The present invention',\n",
       " 'relates to the cosmetic use of electromagnetic radiation for the reduction or alleviation or removal or diminishing of wrinkles or fine lines, especially but not exclusively facial and neck wrinkles and other signs of aging. The present invention also provides the use of electromagnetic radiation for generally rejuvenating skin, retarding signs of aging and improving skin elasticity, tone and appearance. The invention also provides for a method of treating skin so as to reduce or alleviate or retard or reverse visible signs of aging and for beautifying skin and an apparatus for effecting such cosmetic treatments.',\n",
       " 'BACKGROUND']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_client.published_description('GB2415387')[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claims 1. A method of cosmetically treating a superficial area of', 'mammalian skin comprising irradiating the skin with a source of divergent electromagnetic radiation of between 900nm to 1500nm.', '2. A method according to claim 1 where the cosmetic treatment is reducing or alleviating or removing or diminishing wrinkles or fine lines, rejuvenating skin, retarding or reversing visible signs of aging, improving skin elasticity, tone, texture and appearance and beautifying the skin.', '3. A method according to either claim 1 or 2 wherein the skin includes the outermost epidermis, basal layer and dermis of face, breast, arm, buttock, thigh, stomach or neck.', '4. A method according to any preceding claim wherein the divergent light is between 10  to 50 .', '5. A method according to any preceding claim wherein the electromagnetic radiation has a bandwidth of about 10 to 120nm.', '6. A method according to any preceding claim wherein the wavelength of the electromagnetic radiation is centred around any one or more of the specified wavelengths selected from the group comprising 940nm, 950nm, 1040nm, 1060nm, 1072nm and 1267nm.', '7. A method according to any preceding claim wherein the electromagnetic radiation is continuous or pulsed.', '8. A method according to claim 7 wherein when the electromagnetic radiation is continuous the intensity is at least 500 qWatts/cm2 and up to 500 mWatts/cm2.', '9. A method according to claim 7 wherein when the electromagnetic radiation is pulsed the intensity is at least 500 IlWatts/cm2 peak power and the average power is up to 500 mWatts/cm2.']\n"
     ]
    }
   ],
   "source": [
    "claims = registered_client.published_claims('GB2415387')\n",
    "print(claims[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we use some tools from the NLTK to help us process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import stem\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the Punkt tokeniser model and a set of stopwords to make this work. To do this we run nltk.download() below and download the Punkt (from models) and English stopwords (from corpora) via the pop-up interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test case we'll use an Apple case that was the recent subject of a Board of Appeal decision: WO2006084269."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_text = registered_client.published_description('WO2006084269')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SYSTEM FOR BROWSING THROUGH A MUSIC CATALOG USING CORRELATION METRICS OF A KNOWLEDGE BASE OF MEDIASETS',\n",
       " 'Related Applications',\n",
       " '[0001] This application claims priority from U.S. Provisional Application No. 60/649,945 filed February 4, 2005, incorporated herein by this reference in its entirety as though fully set forth.',\n",
       " 'Technical Field',\n",
       " '[0002] This invention relates generally to systems for assisting users to navigate media item catalogs with the ultimate goal of building mediasets and/or discover media items. More specifically, the present invention pertains to computer software methods and products to enable users to interactively browse through an electronic catalog by leveraging media item association metrics.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_text[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we join the paragraphs as one long string separated by new lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_string = \"\\n\".join(desc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MEDIASETS',\n",
       " 'Related',\n",
       " 'Applications',\n",
       " '[',\n",
       " '0001',\n",
       " ']',\n",
       " 'This',\n",
       " 'application',\n",
       " 'claims',\n",
       " 'priority']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(words))\n",
    "words[15:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to strip out punctuation and remove common stop words to reduce this length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without punctuation: 3019\n",
      "Without punctuation and stop words: 1690\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "words_no_punct = [w.lower() for w in words if w.isalpha()]\n",
    "print(\"Without punctuation: \" + str(len(words_no_punct)))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words_no_stop = [w for w in words_no_punct if w not in stopwords]\n",
    "print(\"Without punctuation and stop words: \" + str(len(words_no_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good. We have reduced the length of our vector by half. Looking at some of the words I suspect that a set of patent specific stopwords would reduce this list even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system',\n",
       " 'browsing',\n",
       " 'music',\n",
       " 'catalog',\n",
       " 'using',\n",
       " 'correlation',\n",
       " 'metrics',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'mediasets']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_no_stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a stemming algorithm such that only the stem of a word (i.e. without endings) is retained. Stemming has shown good results with Support Vector Machines. The Porter stemmer was written by [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter), a Cambridge computer scientist, in 1980. It is the de facto standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system',\n",
       " 'brows',\n",
       " 'music',\n",
       " 'catalog',\n",
       " 'use',\n",
       " 'correl',\n",
       " 'metric',\n",
       " 'knowledg',\n",
       " 'base',\n",
       " 'mediaset']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = stem.porter.PorterStemmer()\n",
    "words_stemmed = [porter.stem(w) for w in words_no_stop]\n",
    "words_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a set of Patent Stopwords [I created](https://ipchimp.co.uk/2014/01/29/patents-another-language/). This resulted from an analysis of terms from all US patent publications in 2001. \n",
    "\n",
    "If you create a text file with each word on a new line and save it in ~/nltk_data/corpora/stopwords/ with the filename 'patent' (no extension), then you can load the stopwords in the same manner as the english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patent_stopwords = nltk.corpus.stopwords.words('patent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'use', 'first', 'one', 'form', 'invent', 'thi', 'may', 'second', 'data', 'claim', 'wherein', 'accord', 'control', 'signal', 'present', 'devic', 'provid', 'portion', 'includ', 'embodi', 'compris', 'method', 'layer', 'surfac', 'system', 'process', 'exampl', 'step', 'ha', 'shown', 'connect', 'posit', 'prefer', 'oper', 'gener', 'mean', 'inform', 'circuit', 'imag', 'unit', 'time', 'materi', 'also', 'end', 'wa', 'member', 'line', 'film', 'side', 'least', 'select', 'apparatu', 'output', 'element', 'refer', 'receiv', 'describ', 'direct', 'base', 'light', 'section', 'set', 'show', 'substrat', 'contain', 'display', 'view', 'valu', 'part', 'cell', 'two', 'plural', 'group', 'structur', 'number', 'optic', 'electrod', 'input', 'result', 'abov', 'respect', 'region', 'memori', 'plate', 'case', 'differ', 'user', 'detect', 'illustr', 'determin', 'support', 'within', 'addit', 'record', 'temperatur', 'open', 'power', 'termin', 'area']; "
     ]
    }
   ],
   "source": [
    "print(patent_stopwords, end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering patent stopwords, number of words = 1690\n",
      "After filtering patent stopwords, number of words = 1234\n"
     ]
    }
   ],
   "source": [
    "print(\"Before filtering patent stopwords, number of words = \" + str(len(words_stemmed)))\n",
    "# Filter our list of stemmed terms to remove the patent stopwords\n",
    "no_patent_sws = [word for word in words_stemmed if word not in patent_stopwords]\n",
    "print(\"After filtering patent stopwords, number of words = \" + str(len(no_patent_sws)))\n",
    "words = no_patent_sws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to count the terms. We can use [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) from collections to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'item': 102, 'media': 69, 'mediaset': 49, 'metric': 41, 'knowledg': 25, 'list': 23, 'catalog': 18, 'fig': 15, 'navig': 15, 'pair': 14, 'j': 12, 'music': 12, 'initi': 11, 'consid': 11, 'associ': 11, 'new': 11, 'song': 10, 'collect': 9, 'togeth': 9, 'k': 9, 'book': 8, 'similar': 8, 'interact': 8, 'defin': 8, 'etc': 8, 'approach': 7, 'indic': 7, 'mani': 7, 'express': 6, 'descript': 6, 'way': 6, 'matrix': 6, 'figur': 6, 'appear': 6, 'anoth': 6, 'might': 5, 'given': 5, 'video': 5, 'comput': 5, 'weight': 5, 'build': 5, 'everi': 5, 'softwar': 5, 'order': 5, 'applic': 5, 'follow': 5, 'detail': 5, 'brows': 4, 'limit': 4, 'specif': 4, 'effect': 4, 'certain': 4, 'recommend': 4, 'skill': 4, 'movi': 4, 'normal': 4, 'represent': 4, 'advantag': 4, 'could': 4, 'diagram': 4, 'repres': 4, 'edg': 4, 'interest': 3, 'add': 3, 'herein': 3, 'analyz': 3, 'help': 3, 'simplifi': 3, 'variou': 3, 'discov': 3, 'problem': 3, 'draw': 3, 'guid': 3, 'transit': 3, 'call': 3, 'correspond': 3, 'implement': 3, 'like': 3, 'search': 3, 'co': 3, 'digit': 3, 'domain': 3, 'graph': 3, 'delet': 3, 'creat': 3, 'x': 3, 'person': 3, 'kind': 3, 'assist': 3, 'conceptu': 3, 'relat': 3, 'keyword': 3, 'content': 3, 'well': 3, 'play': 3, 'manner': 3, 'natur': 3, 'pertain': 3, 'playlist': 3, 'howev': 3, 'continu': 3, 'overwhelm': 3, 'deriv': 3, 'art': 3, 'type': 3, 'distanc': 3, 'combin': 3, 'requir': 3, 'analysi': 2, 'identifi': 2, 'understood': 2, 'depend': 2, 'appar': 2, 'row': 2, 'databas': 2, 'made': 2, 'store': 2, 'newspap': 2, 'necessarili': 2, 'next': 2, 'chang': 2, 'carri': 2, 'avail': 2, 'modif': 2, 'numer': 2, 'retriev': 2, 'point': 2, 'classif': 2, 'label': 2, 'network': 2, 'stronger': 2, 'explicit': 2, 'mere': 2, 'immedi': 2, 'news': 2, 'enjoy': 2, 'expertis': 2, 'propos': 2, 'henc': 2, 'edit': 2, 'classifi': 2, 'access': 2, 'peopl': 2, 'commonli': 2, 'program': 2, 'goal': 2, 'flow': 2, 'ultim': 2, 'thu': 2, 'choos': 2, 'technolog': 2, 'among': 2, 'read': 2, 'larg': 2, 'correl': 2, 'three': 2, 'moment': 2, 'ordinari': 2, 'huge': 2, 'implicit': 2, 'exist': 2, 'patent': 2, 'disclos': 2, 'n': 2, 'technic': 2, 'zero': 2, 'choic': 2, 'work': 2, 'face': 2, 'commun': 2, 'column': 2, 'particular': 2, 'therefor': 2, 'paramet': 2, 'homogen': 2, 'purpos': 2, 'otherwis': 2, 'highest': 2, 'arena': 2, 'term': 2, 'instanc': 2, 'human': 2, 'queri': 2, 'sens': 2, 'scope': 2, 'say': 2, 'electron': 2, 'heterogen': 2, 'sequenc': 2, 'note': 2, 'exit': 2, 'previou': 2, 'repeat': 1, 'therebi': 1, 'volum': 1, 'occurr': 1, 'ad': 1, 'summari': 1, 'though': 1, 'explain': 1, 'character': 1, 'album': 1, 'reason': 1, 'especi': 1, 'advanc': 1, 'match': 1, 'product': 1, 'rectangl': 1, 'login': 1, 'forth': 1, 'bound': 1, 'becom': 1, 'articl': 1, 'although': 1, 'react': 1, 'prioriti': 1, 'preliminari': 1, 'conveni': 1, 'length': 1, 'proce': 1, 'inde': 1, 'actual': 1, 'extract': 1, 'provision': 1, 'appli': 1, 'remain': 1, 'known': 1, 'respons': 1, 'presenc': 1, 'unord': 1, 'portabl': 1, 'oval': 1, 'mainli': 1, 'understand': 1, 'applianc': 1, 'appropri': 1, 'annot': 1, 'incorpor': 1, 'methodolog': 1, 'depart': 1, 'head': 1, 'earlier': 1, 'server': 1, 'easier': 1, 'hosken': 1, 'still': 1, 'scienc': 1, 'save': 1, 'standard': 1, 'matter': 1, 'infer': 1, 'omit': 1, 'leverag': 1, 'entireti': 1, 'specifi': 1, 'quickli': 1, 'design': 1, 'industri': 1, 'rock': 1, 'bidimension': 1, 'hallelujah': 1, 'yet': 1, 'player': 1, 'feedback': 1, 'node': 1, 'reflect': 1, 'logic': 1, 'pub': 1, 'situat': 1, 'flowchart': 1, 'obscur': 1, 'relationship': 1, 'individu': 1, 'dun': 1, 'ij': 1, 'drive': 1, 'experi': 1, 'track': 1, 'go': 1, 'link': 1, 'furthermor': 1, 'rather': 1, 'branch': 1, 'file': 1, 'increment': 1, 'degre': 1, 'incred': 1, 'loop': 1, 'distribut': 1, 'buckley': 1, 'current': 1, 'radio': 1, 'explor': 1, 'text': 1, 'desktop': 1, 'previous': 1, 'make': 1, 'descriptor': 1, 'girl': 1, 'transact': 1, 'modul': 1, 'mention': 1, 'tri': 1, 'billi': 1, 'februari': 1, 'consist': 1, 'enabl': 1, 'want': 1, 'improv': 1, 'piec': 1, 'know': 1, 'futur': 1, 'joel': 1, 'employ': 1, 'instruct': 1, 'summer': 1, 'easili': 1, 'graphic': 1, 'intend': 1, 'et': 1, 'entertain': 1, 'definit': 1, 'underli': 1, 'hand': 1, 'rrik': 1, 'found': 1, 'cluster': 1, 'assign': 1, 'accompani': 1, 'adjac': 1, 'background': 1, 'al': 1, 'quantifi': 1, 'microprocessor': 1, 'audio': 1, 'without': 1, 'find': 1, 'algorithm': 1, 'intent': 1, 'append': 1, 'meta': 1, 'need': 1, 'belong': 1, 'comprehend': 1, 'tail': 1, 'seen': 1, 'focu': 1, 'relev': 1, 'newli': 1, 'rang': 1, 'brief': 1, 'look': 1, 'aspect': 1, 'larger': 1, 'weekend': 1, 'begin': 1, 'criteria': 1, 'run': 1, 'goe': 1, 'done': 1, 'mind': 1, 'compos': 1, 'channel': 1, 'cook': 1, 'amount': 1, 'b': 1, 'organ': 1, 'effici': 1, 'mix': 1, 'teach': 1, 'address': 1, 'locat': 1, 'overcom': 1, 'assumpt': 1, 'characterist': 1, 'benefit': 1, 'impli': 1, 'suitabl': 1, 'small': 1, 'clip': 1, 'clariti': 1, 'laptop': 1, 'consequ': 1, 'four': 1, 'obtain': 1, 'jeff': 1, 'evolut': 1, 'dedic': 1, 'consecut': 1, 'enter': 1, 'clearli': 1, 'avoid': 1, 'pda': 1, 'featur': 1, 'path': 1, 'along': 1, 'put': 1, 'come': 1, 'smaller': 1, 'occur': 1, 'observ': 1, 'setup': 1, 'updat': 1, 'delin': 1, 'adopt': 1, 'forego': 1, 'scale': 1, 'uptown': 1, 'principl': 1, 'preprocess': 1, 'field': 1, 'processor': 1, 'ask': 1, 'clear': 1, 'class': 1, 'fulli': 1, 'start': 1, 'throughout': 1, 'pc': 1, 'fact': 1, 'metadata': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have a vector of length: 421\n"
     ]
    }
   ],
   "source": [
    "print(\"We now have a vector of length: \" + str(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together into a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load English stopwords\n",
    "ENG_STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "# Load patent stopwords\n",
    "PAT_STOPWORDS = nltk.corpus.stopwords.words('patent')\n",
    "\n",
    "def word_vector(text):\n",
    "    \"\"\"Take a long string of patent text, process and return a Counter object.\"\"\"\n",
    "    # Tokenise text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    words_no_punct = [w.lower() for w in words if w.isalpha()]\n",
    "    # Remove English stopwords (our Patent stopwords are stemmed so we do them later)\n",
    "    words_no_stop = [w for w in words_no_punct if w not in ENG_STOPWORDS]\n",
    "    # Stem\n",
    "    porter = stem.porter.PorterStemmer()\n",
    "    words_stemmed = [porter.stem(w) for w in words_no_stop]\n",
    "    # Remove patent stopwords\n",
    "    words_no_pat_stop = [w for w in words_stemmed if w not in PAT_STOPWORDS]\n",
    "    # Return counter object\n",
    "    return Counter(words_no_pat_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_words = word_vector(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'item': 102, 'media': 69, 'mediaset': 49, 'metric': 41, 'knowledg': 25, 'list': 23, 'catalog': 18, 'fig': 15, 'navig': 15, 'pair': 14, 'j': 12, 'music': 12, 'initi': 11, 'consid': 11, 'associ': 11, 'new': 11, 'song': 10, 'collect': 9, 'togeth': 9, 'k': 9, 'book': 8, 'similar': 8, 'interact': 8, 'defin': 8, 'etc': 8, 'approach': 7, 'indic': 7, 'mani': 7, 'express': 6, 'descript': 6, 'way': 6, 'matrix': 6, 'figur': 6, 'appear': 6, 'anoth': 6, 'might': 5, 'given': 5, 'video': 5, 'comput': 5, 'weight': 5, 'build': 5, 'everi': 5, 'softwar': 5, 'order': 5, 'applic': 5, 'follow': 5, 'detail': 5, 'brows': 4, 'limit': 4, 'specif': 4, 'effect': 4, 'certain': 4, 'recommend': 4, 'skill': 4, 'movi': 4, 'normal': 4, 'represent': 4, 'advantag': 4, 'could': 4, 'diagram': 4, 'repres': 4, 'edg': 4, 'interest': 3, 'add': 3, 'herein': 3, 'analyz': 3, 'help': 3, 'simplifi': 3, 'variou': 3, 'discov': 3, 'problem': 3, 'draw': 3, 'guid': 3, 'transit': 3, 'call': 3, 'correspond': 3, 'implement': 3, 'like': 3, 'search': 3, 'co': 3, 'digit': 3, 'domain': 3, 'graph': 3, 'delet': 3, 'creat': 3, 'x': 3, 'person': 3, 'kind': 3, 'assist': 3, 'conceptu': 3, 'relat': 3, 'keyword': 3, 'content': 3, 'well': 3, 'play': 3, 'manner': 3, 'natur': 3, 'pertain': 3, 'playlist': 3, 'howev': 3, 'continu': 3, 'overwhelm': 3, 'deriv': 3, 'art': 3, 'type': 3, 'distanc': 3, 'combin': 3, 'requir': 3, 'analysi': 2, 'identifi': 2, 'understood': 2, 'depend': 2, 'appar': 2, 'row': 2, 'databas': 2, 'made': 2, 'store': 2, 'newspap': 2, 'necessarili': 2, 'next': 2, 'chang': 2, 'carri': 2, 'avail': 2, 'modif': 2, 'numer': 2, 'retriev': 2, 'point': 2, 'classif': 2, 'label': 2, 'network': 2, 'stronger': 2, 'explicit': 2, 'mere': 2, 'immedi': 2, 'news': 2, 'enjoy': 2, 'expertis': 2, 'propos': 2, 'henc': 2, 'edit': 2, 'classifi': 2, 'access': 2, 'peopl': 2, 'commonli': 2, 'program': 2, 'goal': 2, 'flow': 2, 'ultim': 2, 'thu': 2, 'choos': 2, 'technolog': 2, 'among': 2, 'read': 2, 'larg': 2, 'correl': 2, 'three': 2, 'moment': 2, 'ordinari': 2, 'huge': 2, 'implicit': 2, 'exist': 2, 'patent': 2, 'disclos': 2, 'n': 2, 'technic': 2, 'zero': 2, 'choic': 2, 'work': 2, 'face': 2, 'commun': 2, 'column': 2, 'particular': 2, 'therefor': 2, 'paramet': 2, 'homogen': 2, 'purpos': 2, 'otherwis': 2, 'highest': 2, 'arena': 2, 'term': 2, 'instanc': 2, 'human': 2, 'queri': 2, 'sens': 2, 'scope': 2, 'say': 2, 'electron': 2, 'heterogen': 2, 'sequenc': 2, 'note': 2, 'exit': 2, 'previou': 2, 'repeat': 1, 'therebi': 1, 'volum': 1, 'occurr': 1, 'ad': 1, 'summari': 1, 'though': 1, 'explain': 1, 'character': 1, 'album': 1, 'reason': 1, 'especi': 1, 'advanc': 1, 'match': 1, 'product': 1, 'rectangl': 1, 'login': 1, 'forth': 1, 'bound': 1, 'becom': 1, 'articl': 1, 'although': 1, 'react': 1, 'prioriti': 1, 'preliminari': 1, 'conveni': 1, 'length': 1, 'proce': 1, 'inde': 1, 'actual': 1, 'extract': 1, 'provision': 1, 'appli': 1, 'remain': 1, 'known': 1, 'respons': 1, 'presenc': 1, 'unord': 1, 'portabl': 1, 'oval': 1, 'mainli': 1, 'understand': 1, 'applianc': 1, 'appropri': 1, 'annot': 1, 'incorpor': 1, 'methodolog': 1, 'depart': 1, 'head': 1, 'earlier': 1, 'server': 1, 'easier': 1, 'hosken': 1, 'still': 1, 'scienc': 1, 'save': 1, 'standard': 1, 'matter': 1, 'infer': 1, 'omit': 1, 'leverag': 1, 'entireti': 1, 'specifi': 1, 'quickli': 1, 'design': 1, 'industri': 1, 'rock': 1, 'bidimension': 1, 'hallelujah': 1, 'yet': 1, 'player': 1, 'feedback': 1, 'node': 1, 'reflect': 1, 'logic': 1, 'pub': 1, 'situat': 1, 'flowchart': 1, 'obscur': 1, 'relationship': 1, 'individu': 1, 'dun': 1, 'ij': 1, 'drive': 1, 'experi': 1, 'track': 1, 'go': 1, 'link': 1, 'furthermor': 1, 'rather': 1, 'branch': 1, 'file': 1, 'increment': 1, 'degre': 1, 'incred': 1, 'loop': 1, 'distribut': 1, 'buckley': 1, 'current': 1, 'radio': 1, 'explor': 1, 'text': 1, 'desktop': 1, 'previous': 1, 'make': 1, 'descriptor': 1, 'girl': 1, 'transact': 1, 'modul': 1, 'mention': 1, 'tri': 1, 'billi': 1, 'februari': 1, 'consist': 1, 'enabl': 1, 'want': 1, 'improv': 1, 'piec': 1, 'know': 1, 'futur': 1, 'joel': 1, 'employ': 1, 'instruct': 1, 'summer': 1, 'easili': 1, 'graphic': 1, 'intend': 1, 'et': 1, 'entertain': 1, 'definit': 1, 'underli': 1, 'hand': 1, 'rrik': 1, 'found': 1, 'cluster': 1, 'assign': 1, 'accompani': 1, 'adjac': 1, 'background': 1, 'al': 1, 'quantifi': 1, 'microprocessor': 1, 'audio': 1, 'without': 1, 'find': 1, 'algorithm': 1, 'intent': 1, 'append': 1, 'meta': 1, 'need': 1, 'belong': 1, 'comprehend': 1, 'tail': 1, 'seen': 1, 'focu': 1, 'relev': 1, 'newli': 1, 'rang': 1, 'brief': 1, 'look': 1, 'aspect': 1, 'larger': 1, 'weekend': 1, 'begin': 1, 'criteria': 1, 'run': 1, 'goe': 1, 'done': 1, 'mind': 1, 'compos': 1, 'channel': 1, 'cook': 1, 'amount': 1, 'b': 1, 'organ': 1, 'effici': 1, 'mix': 1, 'teach': 1, 'address': 1, 'locat': 1, 'overcom': 1, 'assumpt': 1, 'characterist': 1, 'benefit': 1, 'impli': 1, 'suitabl': 1, 'small': 1, 'clip': 1, 'clariti': 1, 'laptop': 1, 'consequ': 1, 'four': 1, 'obtain': 1, 'jeff': 1, 'evolut': 1, 'dedic': 1, 'consecut': 1, 'enter': 1, 'clearli': 1, 'avoid': 1, 'pda': 1, 'featur': 1, 'path': 1, 'along': 1, 'put': 1, 'come': 1, 'smaller': 1, 'occur': 1, 'observ': 1, 'setup': 1, 'updat': 1, 'delin': 1, 'adopt': 1, 'forego': 1, 'scale': 1, 'uptown': 1, 'principl': 1, 'preprocess': 1, 'field': 1, 'processor': 1, 'ask': 1, 'clear': 1, 'class': 1, 'fulli': 1, 'start': 1, 'throughout': 1, 'pc': 1, 'fact': 1, 'metadata': 1})\n"
     ]
    }
   ],
   "source": [
    "print(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to base a TD-IDF algorithm on the resultant frequency distribution across a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
