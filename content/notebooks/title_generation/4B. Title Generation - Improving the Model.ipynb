{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "30000 samples loaded\n",
      "\n",
      "\n",
      "Adding start and stop tokens to output\n",
      "\n",
      "\n",
      "An example title: startseq System and method for session restoration at geo-redundant gateways stopseq\n",
      "----\n",
      "An example claim: \n",
      "1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:\n",
      "periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and\n",
      "in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "PIK = \"claim_and_title.data\"\n",
    "\n",
    "if not os.path.isfile(PIK):\n",
    "    # Download file\n",
    "    !wget https://benhoyle.github.io/notebooks/title_generation/claim_and_title.data\n",
    "\n",
    "with open(PIK, \"rb\") as f:\n",
    "    print(\"Loading data\")\n",
    "    data = pickle.load(f)\n",
    "    print(\"{0} samples loaded\".format(len(data)))\n",
    "    \n",
    "print(\"\\n\\nAdding start and stop tokens to output\")\n",
    "data = [(c, \"startseq {0} stopseq\".format(t)) for c, t in data]\n",
    "                                      \n",
    "print(\"\\n\\nAn example title:\", data[0][1])\n",
    "print(\"----\")\n",
    "print(\"An example claim:\", data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ludwig_model import LudwigModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizers\n",
      "Our input data has shape (30000, 300) and our output data has shape (30000, 22)\n",
      "Generating training and test data\n",
      "Building model\n",
      "Loading GloVe 100d embeddings from file\n",
      "Found 400000 word vectors.\n",
      "Building embedding matrix\n",
      "Compiling model\n",
      "Loaded weights\n"
     ]
    }
   ],
   "source": [
    "lw = LudwigModel(\n",
    "    encoder_texts=[d[0] for d in data],\n",
    "    decoder_texts=[d[1] for d in data],\n",
    "    encoder_seq_length=300,\n",
    "    decoder_seq_length=22,\n",
    "    num_encoder_tokens=2500,\n",
    "    num_decoder_tokens=2500,\n",
    "    latent_dim=128,\n",
    "    weights_file=\"class_ludwigmodel.hfd5\",\n",
    "    training_set_size=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 23s 9ms/step - loss: 2.2396 - val_loss: 2.3552\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 21s 9ms/step - loss: 2.1123 - val_loss: 2.3149\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 22s 9ms/step - loss: 2.2179 - val_loss: 2.5619\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2613/2613 [==============================] - 23s 9ms/step - loss: 2.2897 - val_loss: 2.3054\n",
      "Training on batch 1000 to 1250 of 24000\n",
      "Train on 2532 samples, validate on 687 samples\n",
      "Epoch 1/1\n",
      "2532/2532 [==============================] - 22s 9ms/step - loss: 2.2084 - val_loss: 2.8254\n",
      "Training on batch 1250 to 1500 of 24000\n",
      "Train on 2481 samples, validate on 699 samples\n",
      "Epoch 1/1\n",
      "2481/2481 [==============================] - 22s 9ms/step - loss: 2.2349 - val_loss: 2.5079\n",
      "Training on batch 1500 to 1750 of 24000\n",
      "Train on 2425 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2425/2425 [==============================] - 21s 9ms/step - loss: 2.1874 - val_loss: 2.3976\n",
      "Training on batch 1750 to 2000 of 24000\n",
      "Train on 2575 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 22s 9ms/step - loss: 2.3466 - val_loss: 2.6758\n",
      "Training on batch 2000 to 2250 of 24000\n",
      "Train on 2449 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2449/2449 [==============================] - 23s 9ms/step - loss: 2.2410 - val_loss: 2.6075\n",
      "Training on batch 2250 to 2500 of 24000\n",
      "Train on 2568 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 24s 9ms/step - loss: 2.2935 - val_loss: 2.6175\n",
      "Training on batch 2500 to 2750 of 24000\n",
      "Train on 2459 samples, validate on 607 samples\n",
      "Epoch 1/1\n",
      "2459/2459 [==============================] - 21s 9ms/step - loss: 2.2419 - val_loss: 2.3932\n",
      "Training on batch 2750 to 3000 of 24000\n",
      "Train on 2530 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 22s 9ms/step - loss: 2.2986 - val_loss: 2.3571\n",
      "Training on batch 3000 to 3250 of 24000\n",
      "Train on 2466 samples, validate on 721 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 22s 9ms/step - loss: 2.1352 - val_loss: 2.4688\n",
      "Training on batch 3250 to 3500 of 24000\n",
      "Train on 2491 samples, validate on 643 samples\n",
      "Epoch 1/1\n",
      "2491/2491 [==============================] - 22s 9ms/step - loss: 2.2472 - val_loss: 2.5161\n",
      "Training on batch 3500 to 3750 of 24000\n",
      "Train on 2568 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2568/2568 [==============================] - 22s 9ms/step - loss: 2.1998 - val_loss: 2.3492\n",
      "Training on batch 3750 to 4000 of 24000\n",
      "Train on 2507 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2507/2507 [==============================] - 23s 9ms/step - loss: 2.2694 - val_loss: 2.4055\n",
      "Training on batch 4000 to 4250 of 24000\n",
      "Train on 2444 samples, validate on 564 samples\n",
      "Epoch 1/1\n",
      "2444/2444 [==============================] - 21s 9ms/step - loss: 2.1957 - val_loss: 2.4294\n",
      "Training on batch 4250 to 4500 of 24000\n",
      "Train on 2502 samples, validate on 615 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 23s 9ms/step - loss: 2.1754 - val_loss: 2.4675\n",
      "Training on batch 4500 to 4750 of 24000\n",
      "Train on 2690 samples, validate on 673 samples\n",
      "Epoch 1/1\n",
      "2690/2690 [==============================] - 24s 9ms/step - loss: 2.2152 - val_loss: 2.5685\n",
      "Training on batch 4750 to 5000 of 24000\n",
      "Train on 2516 samples, validate on 635 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 22s 9ms/step - loss: 2.1817 - val_loss: 2.5984\n",
      "Training on batch 5000 to 5250 of 24000\n",
      "Train on 2514 samples, validate on 711 samples\n",
      "Epoch 1/1\n",
      "2514/2514 [==============================] - 23s 9ms/step - loss: 2.2648 - val_loss: 2.5416\n",
      "Training on batch 5250 to 5500 of 24000\n",
      "Train on 2516 samples, validate on 712 samples\n",
      "Epoch 1/1\n",
      "2516/2516 [==============================] - 24s 9ms/step - loss: 2.2006 - val_loss: 2.5500\n",
      "Training on batch 5500 to 5750 of 24000\n",
      "Train on 2448 samples, validate on 624 samples\n",
      "Epoch 1/1\n",
      "2448/2448 [==============================] - 22s 9ms/step - loss: 2.2211 - val_loss: 2.6281\n",
      "Training on batch 5750 to 6000 of 24000\n",
      "Train on 2476 samples, validate on 606 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 23s 9ms/step - loss: 2.1438 - val_loss: 2.6177\n",
      "Training on batch 6000 to 6250 of 24000\n",
      "Train on 2531 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 24s 9ms/step - loss: 2.2387 - val_loss: 2.3484\n",
      "Training on batch 6250 to 6500 of 24000\n",
      "Train on 2574 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2574/2574 [==============================] - 22s 9ms/step - loss: 2.2518 - val_loss: 2.2006\n",
      "Training on batch 6500 to 6750 of 24000\n",
      "Train on 2479 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2479/2479 [==============================] - 23s 9ms/step - loss: 2.2511 - val_loss: 2.6082\n",
      "Training on batch 6750 to 7000 of 24000\n",
      "Train on 2548 samples, validate on 625 samples\n",
      "Epoch 1/1\n",
      "2548/2548 [==============================] - 24s 9ms/step - loss: 2.2886 - val_loss: 2.5635\n",
      "Training on batch 7000 to 7250 of 24000\n",
      "Train on 2603 samples, validate on 671 samples\n",
      "Epoch 1/1\n",
      "2603/2603 [==============================] - 23s 9ms/step - loss: 2.2921 - val_loss: 2.4855\n",
      "Training on batch 7250 to 7500 of 24000\n",
      "Train on 2541 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 22s 9ms/step - loss: 2.2854 - val_loss: 2.2850\n",
      "Training on batch 7500 to 7750 of 24000\n",
      "Train on 2537 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 24s 9ms/step - loss: 2.1951 - val_loss: 2.5626\n",
      "Training on batch 7750 to 8000 of 24000\n",
      "Train on 2616 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 2.1909 - val_loss: 2.5214\n",
      "Training on batch 8000 to 8250 of 24000\n",
      "Train on 2567 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2567/2567 [==============================] - 22s 9ms/step - loss: 2.2081 - val_loss: 2.6049\n",
      "Training on batch 8250 to 8500 of 24000\n",
      "Train on 2631 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2631/2631 [==============================] - 24s 9ms/step - loss: 2.2707 - val_loss: 2.3904\n",
      "Training on batch 8500 to 8750 of 24000\n",
      "Train on 2427 samples, validate on 618 samples\n",
      "Epoch 1/1\n",
      "2427/2427 [==============================] - 22s 9ms/step - loss: 2.2407 - val_loss: 2.4235\n",
      "Training on batch 8750 to 9000 of 24000\n",
      "Train on 2627 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2627/2627 [==============================] - 23s 9ms/step - loss: 2.2283 - val_loss: 2.2081\n",
      "Training on batch 9000 to 9250 of 24000\n",
      "Train on 2536 samples, validate on 629 samples\n",
      "Epoch 1/1\n",
      "2536/2536 [==============================] - 22s 9ms/step - loss: 2.2446 - val_loss: 2.4242\n",
      "Training on batch 9250 to 9500 of 24000\n",
      "Train on 2473 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2473/2473 [==============================] - 23s 9ms/step - loss: 2.1954 - val_loss: 2.3341\n",
      "Training on batch 9500 to 9750 of 24000\n",
      "Train on 2564 samples, validate on 647 samples\n",
      "Epoch 1/1\n",
      "2564/2564 [==============================] - 24s 9ms/step - loss: 2.2250 - val_loss: 2.3819\n",
      "Training on batch 9750 to 10000 of 24000\n",
      "Train on 2456 samples, validate on 603 samples\n",
      "Epoch 1/1\n",
      "2456/2456 [==============================] - 21s 9ms/step - loss: 2.1679 - val_loss: 2.2044\n",
      "Training on batch 10000 to 10250 of 24000\n",
      "Train on 2648 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2648/2648 [==============================] - 24s 9ms/step - loss: 2.2347 - val_loss: 2.3134\n",
      "Training on batch 10250 to 10500 of 24000\n",
      "Train on 2719 samples, validate on 659 samples\n",
      "Epoch 1/1\n",
      "2719/2719 [==============================] - 24s 9ms/step - loss: 2.1950 - val_loss: 2.4557\n",
      "Training on batch 10500 to 10750 of 24000\n",
      "Train on 2483 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2483/2483 [==============================] - 22s 9ms/step - loss: 2.1797 - val_loss: 2.5193\n",
      "Training on batch 10750 to 11000 of 24000\n",
      "Train on 2465 samples, validate on 671 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465/2465 [==============================] - 22s 9ms/step - loss: 2.2284 - val_loss: 2.5518\n",
      "Training on batch 11000 to 11250 of 24000\n",
      "Train on 2399 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2399/2399 [==============================] - 21s 9ms/step - loss: 2.1256 - val_loss: 2.5017\n",
      "Training on batch 11250 to 11500 of 24000\n",
      "Train on 2476 samples, validate on 669 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 22s 9ms/step - loss: 2.2369 - val_loss: 2.4428\n",
      "Training on batch 11500 to 11750 of 24000\n",
      "Train on 2450 samples, validate on 648 samples\n",
      "Epoch 1/1\n",
      "2450/2450 [==============================] - 23s 9ms/step - loss: 2.1414 - val_loss: 2.4737\n",
      "Training on batch 11750 to 12000 of 24000\n",
      "Train on 2553 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2553/2553 [==============================] - 23s 9ms/step - loss: 2.1628 - val_loss: 2.3205\n",
      "Training on batch 12000 to 12250 of 24000\n",
      "Train on 2395 samples, validate on 595 samples\n",
      "Epoch 1/1\n",
      "2395/2395 [==============================] - 21s 9ms/step - loss: 2.1439 - val_loss: 2.5915\n",
      "Training on batch 12250 to 12500 of 24000\n",
      "Train on 2497 samples, validate on 617 samples\n",
      "Epoch 1/1\n",
      "2497/2497 [==============================] - 23s 9ms/step - loss: 2.1414 - val_loss: 2.6683\n",
      "Training on batch 12500 to 12750 of 24000\n",
      "Train on 2535 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2535/2535 [==============================] - 24s 9ms/step - loss: 2.1369 - val_loss: 2.4681\n",
      "Training on batch 12750 to 13000 of 24000\n",
      "Train on 2426 samples, validate on 604 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 22s 9ms/step - loss: 2.1723 - val_loss: 2.3103\n",
      "Training on batch 13000 to 13250 of 24000\n",
      "Train on 2445 samples, validate on 605 samples\n",
      "Epoch 1/1\n",
      "2445/2445 [==============================] - 21s 9ms/step - loss: 2.1265 - val_loss: 2.2839\n",
      "Training on batch 13250 to 13500 of 24000\n",
      "Train on 2408 samples, validate on 672 samples\n",
      "Epoch 1/1\n",
      "2408/2408 [==============================] - 22s 9ms/step - loss: 2.1483 - val_loss: 2.4615\n",
      "Training on batch 13500 to 13750 of 24000\n",
      "Train on 2426 samples, validate on 646 samples\n",
      "Epoch 1/1\n",
      "2426/2426 [==============================] - 22s 9ms/step - loss: 2.1535 - val_loss: 2.3881\n",
      "Training on batch 13750 to 14000 of 24000\n",
      "Train on 2573 samples, validate on 626 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 22s 9ms/step - loss: 2.1300 - val_loss: 2.4087\n",
      "Training on batch 14000 to 14250 of 24000\n",
      "Train on 2571 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2571/2571 [==============================] - 24s 9ms/step - loss: 2.1633 - val_loss: 2.4342\n",
      "Training on batch 14250 to 14500 of 24000\n",
      "Train on 2559 samples, validate on 620 samples\n",
      "Epoch 1/1\n",
      "2559/2559 [==============================] - 24s 9ms/step - loss: 2.2686 - val_loss: 2.3034\n",
      "Training on batch 14500 to 14750 of 24000\n",
      "Train on 2519 samples, validate on 619 samples\n",
      "Epoch 1/1\n",
      "2519/2519 [==============================] - 24s 9ms/step - loss: 2.0902 - val_loss: 2.3191\n",
      "Training on batch 14750 to 15000 of 24000\n",
      "Train on 2537 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2537/2537 [==============================] - 25s 10ms/step - loss: 2.2098 - val_loss: 2.4925\n",
      "Training on batch 15000 to 15250 of 24000\n",
      "Train on 2541 samples, validate on 630 samples\n",
      "Epoch 1/1\n",
      "2541/2541 [==============================] - 23s 9ms/step - loss: 2.1388 - val_loss: 2.4350\n",
      "Training on batch 15250 to 15500 of 24000\n",
      "Train on 2478 samples, validate on 585 samples\n",
      "Epoch 1/1\n",
      "2478/2478 [==============================] - 24s 9ms/step - loss: 2.1693 - val_loss: 2.4309\n",
      "Training on batch 15500 to 15750 of 24000\n",
      "Train on 2443 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 25s 10ms/step - loss: 2.1971 - val_loss: 2.3330\n",
      "Training on batch 15750 to 16000 of 24000\n",
      "Train on 2502 samples, validate on 551 samples\n",
      "Epoch 1/1\n",
      "2502/2502 [==============================] - 25s 10ms/step - loss: 2.1929 - val_loss: 2.2708\n",
      "Training on batch 16000 to 16250 of 24000\n",
      "Train on 2494 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 23s 9ms/step - loss: 2.0947 - val_loss: 2.4343\n",
      "Training on batch 16250 to 16500 of 24000\n",
      "Train on 2471 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2471/2471 [==============================] - 23s 9ms/step - loss: 2.1417 - val_loss: 2.3464\n",
      "Training on batch 16500 to 16750 of 24000\n",
      "Train on 2562 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2562/2562 [==============================] - 24s 9ms/step - loss: 2.1532 - val_loss: 2.4074\n",
      "Training on batch 16750 to 17000 of 24000\n",
      "Train on 2482 samples, validate on 651 samples\n",
      "Epoch 1/1\n",
      "2482/2482 [==============================] - 23s 9ms/step - loss: 2.1425 - val_loss: 2.4663\n",
      "Training on batch 17000 to 17250 of 24000\n",
      "Train on 2656 samples, validate on 616 samples\n",
      "Epoch 1/1\n",
      "2656/2656 [==============================] - 24s 9ms/step - loss: 2.2080 - val_loss: 2.5600\n",
      "Training on batch 17250 to 17500 of 24000\n",
      "Train on 2466 samples, validate on 654 samples\n",
      "Epoch 1/1\n",
      "2466/2466 [==============================] - 23s 9ms/step - loss: 2.1597 - val_loss: 2.4457\n",
      "Training on batch 17500 to 17750 of 24000\n",
      "Train on 2575 samples, validate on 645 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 24s 9ms/step - loss: 2.2788 - val_loss: 2.3012\n",
      "Training on batch 17750 to 18000 of 24000\n",
      "Train on 2452 samples, validate on 706 samples\n",
      "Epoch 1/1\n",
      "2452/2452 [==============================] - 21s 9ms/step - loss: 2.1220 - val_loss: 2.5145\n",
      "Training on batch 18000 to 18250 of 24000\n",
      "Train on 2575 samples, validate on 568 samples\n",
      "Epoch 1/1\n",
      "2575/2575 [==============================] - 23s 9ms/step - loss: 2.1062 - val_loss: 2.5060\n",
      "Training on batch 18250 to 18500 of 24000\n",
      "Train on 2592 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2592/2592 [==============================] - 22s 9ms/step - loss: 2.0786 - val_loss: 2.4699\n",
      "Training on batch 18500 to 18750 of 24000\n",
      "Train on 2527 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "2527/2527 [==============================] - 24s 9ms/step - loss: 2.0601 - val_loss: 2.3866\n",
      "Training on batch 18750 to 19000 of 24000\n",
      "Train on 2495 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2495/2495 [==============================] - 23s 9ms/step - loss: 2.0748 - val_loss: 2.5477\n",
      "Training on batch 19000 to 19250 of 24000\n",
      "Train on 2460 samples, validate on 638 samples\n",
      "Epoch 1/1\n",
      "2460/2460 [==============================] - 23s 9ms/step - loss: 2.2173 - val_loss: 2.6723\n",
      "Training on batch 19250 to 19500 of 24000\n",
      "Train on 2573 samples, validate on 652 samples\n",
      "Epoch 1/1\n",
      "2573/2573 [==============================] - 23s 9ms/step - loss: 2.0648 - val_loss: 2.6777\n",
      "Training on batch 19500 to 19750 of 24000\n",
      "Train on 2643 samples, validate on 668 samples\n",
      "Epoch 1/1\n",
      "2643/2643 [==============================] - 23s 9ms/step - loss: 2.1666 - val_loss: 2.4679\n",
      "Training on batch 19750 to 20000 of 24000\n",
      "Train on 2531 samples, validate on 623 samples\n",
      "Epoch 1/1\n",
      "2531/2531 [==============================] - 23s 9ms/step - loss: 2.1994 - val_loss: 2.4235\n",
      "Training on batch 20000 to 20250 of 24000\n",
      "Train on 2530 samples, validate on 642 samples\n",
      "Epoch 1/1\n",
      "2530/2530 [==============================] - 22s 9ms/step - loss: 2.1053 - val_loss: 2.6668\n",
      "Training on batch 20250 to 20500 of 24000\n",
      "Train on 2494 samples, validate on 674 samples\n",
      "Epoch 1/1\n",
      "2494/2494 [==============================] - 23s 9ms/step - loss: 2.1402 - val_loss: 2.2063\n",
      "Training on batch 20500 to 20750 of 24000\n",
      "Train on 2476 samples, validate on 641 samples\n",
      "Epoch 1/1\n",
      "2476/2476 [==============================] - 21s 9ms/step - loss: 2.0932 - val_loss: 2.5552\n",
      "Training on batch 20750 to 21000 of 24000\n",
      "Train on 2566 samples, validate on 666 samples\n",
      "Epoch 1/1\n",
      "2566/2566 [==============================] - 22s 9ms/step - loss: 2.0679 - val_loss: 2.3387\n",
      "Training on batch 21000 to 21250 of 24000\n",
      "Train on 2468 samples, validate on 602 samples\n",
      "Epoch 1/1\n",
      "2468/2468 [==============================] - 23s 9ms/step - loss: 2.1435 - val_loss: 2.2598\n",
      "Training on batch 21250 to 21500 of 24000\n",
      "Train on 2678 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      "2678/2678 [==============================] - 25s 9ms/step - loss: 2.0629 - val_loss: 2.4521\n",
      "Training on batch 21500 to 21750 of 24000\n",
      "Train on 2534 samples, validate on 648 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 22s 9ms/step - loss: 2.1311 - val_loss: 2.3984\n",
      "Training on batch 21750 to 22000 of 24000\n",
      "Train on 2515 samples, validate on 693 samples\n",
      "Epoch 1/1\n",
      "2515/2515 [==============================] - 24s 9ms/step - loss: 2.2098 - val_loss: 2.5869\n",
      "Training on batch 22000 to 22250 of 24000\n",
      "Train on 2586 samples, validate on 627 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 22s 9ms/step - loss: 1.9605 - val_loss: 2.3664\n",
      "Training on batch 22250 to 22500 of 24000\n",
      "Train on 2543 samples, validate on 676 samples\n",
      "Epoch 1/1\n",
      "2543/2543 [==============================] - 23s 9ms/step - loss: 2.0668 - val_loss: 2.5377\n",
      "Training on batch 22500 to 22750 of 24000\n",
      "Train on 2487 samples, validate on 582 samples\n",
      "Epoch 1/1\n",
      "2487/2487 [==============================] - 21s 9ms/step - loss: 2.1962 - val_loss: 2.2233\n",
      "Training on batch 22750 to 23000 of 24000\n",
      "Train on 2557 samples, validate on 610 samples\n",
      "Epoch 1/1\n",
      "2557/2557 [==============================] - 22s 9ms/step - loss: 2.0859 - val_loss: 2.2690\n",
      "Training on batch 23000 to 23250 of 24000\n",
      "Train on 2364 samples, validate on 661 samples\n",
      "Epoch 1/1\n",
      "2364/2364 [==============================] - 22s 9ms/step - loss: 2.1096 - val_loss: 2.4803\n",
      "Training on batch 23250 to 23500 of 24000\n",
      "Train on 2538 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2538/2538 [==============================] - 23s 9ms/step - loss: 2.1727 - val_loss: 2.4922\n",
      "Training on batch 23500 to 23750 of 24000\n",
      "Train on 2373 samples, validate on 637 samples\n",
      "Epoch 1/1\n",
      "2373/2373 [==============================] - 24s 10ms/step - loss: 2.0931 - val_loss: 2.3527\n",
      "Training on batch 23750 to 24000 of 24000\n",
      "Train on 2593 samples, validate on 590 samples\n",
      "Epoch 1/1\n",
      "2593/2593 [==============================] - 26s 10ms/step - loss: 2.1007 - val_loss: 2.3397\n",
      "Sample of claim text: 1 a data storage system for use by a file system consumer according to a file system interface comprising one or more physical non transitory nonvolatile storage devices and a processing subsystem exe\n",
      "\n",
      "Predicted title is: file system and method  \n",
      "Actual title is: file system over volume file in direct mode  \n",
      "---\n",
      "Sample of claim text: 1 a display device comprising a display panel comprising a first display substrate and a second display substrate facing the first substrate the display panel being divided into a blocking area and a \n",
      "\n",
      "Predicted title is: touch screen display device and display device  \n",
      "Actual title is: display device and method of driving the same in two modes  \n",
      "---\n",
      "Sample of claim text: 1 a computer implemented method for contrast registration of images the method comprising a receiving an image sequence of frames b selecting a reference frame from the image sequence of frames c defi\n",
      "\n",
      "Predicted title is: method and system for enhancing three dimensional images  \n",
      "Actual title is: contrast registration of and magnetic images  \n",
      "---\n",
      "Sample of claim text: 1 a method with to a computing device with a keyboard and a display and employing the keyboard to enter text characters into the device within a text field in a browser on the display of the device th\n",
      "\n",
      "Predicted title is: touch screen and method of displaying text  \n",
      "Actual title is: systems and methods for entering data into electronic device with keyboard  \n",
      "---\n",
      "Sample of claim text: 1 a system configured to generate a composite image of a signature of a subject the system comprising a comprising a first light reflecting surface a light source configured to light to a subject plac\n",
      "\n",
      "Predicted title is: method and apparatus for detecting and tracking objects in a  \n",
      "Actual title is: low power fingerprint capture system apparatus and method  \n",
      "---\n",
      "Training for epoch 1\n",
      "Training on batch 0 to 250 of 24000\n",
      "Train on 2639 samples, validate on 655 samples\n",
      "Epoch 1/1\n",
      "2639/2639 [==============================] - 25s 10ms/step - loss: 2.1771 - val_loss: 2.3467\n",
      "Training on batch 250 to 500 of 24000\n",
      "Train on 2484 samples, validate on 658 samples\n",
      "Epoch 1/1\n",
      "2484/2484 [==============================] - 24s 10ms/step - loss: 1.9746 - val_loss: 2.4022\n",
      "Training on batch 500 to 750 of 24000\n",
      "Train on 2586 samples, validate on 583 samples\n",
      "Epoch 1/1\n",
      "2586/2586 [==============================] - 25s 10ms/step - loss: 2.0996 - val_loss: 2.5014\n",
      "Training on batch 750 to 1000 of 24000\n",
      "Train on 2613 samples, validate on 667 samples\n",
      "Epoch 1/1\n",
      " 704/2613 [=======>......................] - ETA: 15s - loss: 2.0094"
     ]
    }
   ],
   "source": [
    "lw.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = lw\n",
    "input_data = self.encoder_texts[0]\n",
    "if isinstance(input_data, str):\n",
    "    input_seq = self._text2seq([input_data], encoder=True)\n",
    "    print(len(input_seq))\n",
    "    input_seq = self._pad_seqs(\n",
    "            input_seq,\n",
    "            self.encoder_seq_length, \n",
    "            'pre'\n",
    "            )\n",
    "    print(input_seq.shape)\n",
    "else:\n",
    "    input_seq = input_data\n",
    "predicted_output_seq = self._predict_from_seq(input_seq)\n",
    "predicted_text = self._seq2text(predicted_output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(self.encoder_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
