<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Classifying Claims - Preparing the Data | Machine Learning Projects
</title>
  <link rel="canonical" href="./claim-class-get-data.html">


  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/theme.css">

  
  <meta name="description" content="This post looks at how we prepare the data for our claim classification example.">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="./">Machine Learning Projects</a></h1>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
          <li class="list-inline-item"><a href="http://python.org/" target="_blank">Python.org</a></li>
          <li class="list-inline-item"><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
          <li class="list-inline-item"><a href="#" target="_blank">You can modify those links in your config file</a></li>
              <li class="list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a href="./pages/about.html">About</a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Classifying Claims - Preparing the Data
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-03-20T12:31:03.785370+00:00">
          <i class="fa fa-clock-o"></i>
          Tue 20 March 2018
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="./category/claim-classification.html">Claim Classification</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="./author/ben-hoyle.html">Ben Hoyle</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="./tag/preparing_data.html">#preparing_data</a>          </li>
      </ul>
    </header>
    <div class="content">
      <h1>2. Classifying Claims - Preparing the Data</h1>
<p>In the previous post we looked at the problem for our current project: how to assign an IPC/CPC Section to a first claim of a patent application.</p>
<p>In this post we will look at the steps required to prepare some data for our machine learning algorithms. These steps roughly follow the guide <a href="https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/">here</a>:</p>
<ul>
<li>Step 1: Select Data</li>
<li>Step 2: Preprocess Data</li>
<li>Step 3: Transform Data</li>
</ul>
<p>These steps can take up a fair proportion of the project time. The idea is to obtain a manageable data set and place it in a form where we can apply common machine learning libraries.</p>
<hr />
<h2>1. Select Data</h2>
<p>For our project we want a manageable in-memory dataset. A quick back-of-the-envelope calculation provides the following insight: claims contain around 200 words (we will check this assumption a bit later); each word contains around 5 characters; each character is typically (e.g. using UTF-8 encoding) encoded as 1 Byte (8 bits); we thus have 200 * 5 * 1B = 1kB per claim. 10,000 claims thus make 10MB, which we can easily hold in a 4GB memory.</p>
<p>There are over 9 million US patent publications, which can be downloaded from the [USPTO Bulk Data Archives] (https://www.uspto.gov/learning-and-resources/bulk-data-products). The data is supplied in the form of compressed file archives, where the actual patent data is supplied within these archives as an XML file.</p>
<p>Luckily, we have access to a Python library that provides a wrapper around the USPTO patent publication data as obtained from the bulk data download page. This library can be found on GitHub here (https://github.com/benhoyle/patentdata). This code allows us to parse the compressed files and extract claim text and classification data from the raw XML files.</p>
<p>If you don't want to use or install the patentdata library, you can find a pickle of the extracted data in the accompanying GitHub repository.</p>
<p>One future extension of this project would be to try the classification training on a set of 100,000 or more data samples.</p>
<h3>Code to Get the Data</h3>
<p>The code below gets 12,000 random patent publications from data for years 2001 to 2017 across all classifications.<br />
Once we have the patent publications, we need to extract the text for claim 1 and the classifications. This can be performed using the PatentDoc.Claimset object "get_claim(1)" method and using the "as_string()" method for each classification object in the PatentDoc.Classifications list.</p>
<div class="highlight"><pre><span class="c1"># Start with getting 12,000 patent publications at random</span>
<span class="kn">from</span> <span class="nn">patentdata.corpus</span> <span class="kn">import</span> <span class="n">USPublications</span>
<span class="kn">from</span> <span class="nn">patentdata.models.patentcorpus</span> <span class="kn">import</span> <span class="n">LazyPatentCorpus</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># Get the claim 1 and classificationt text</span>

<span class="n">PIK</span> <span class="o">=</span> <span class="s2">&quot;claim_and_class.data&quot;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">PIK</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading data&quot;</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} claims and classifications loaded&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Load our list of records</span>
    <span class="n">PIK</span> <span class="o">=</span> <span class="s2">&quot;12000records.data&quot;</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">PIK</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading data&quot;</span><span class="p">)</span>
            <span class="n">records</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} records loaded&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;/media/SAMSUNG1/Patent_Downloads&#39;</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">USPublications</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">records</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">get_records</span><span class="p">([],</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">12000</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">records</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} records saved&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)))</span>

    <span class="n">lzy</span> <span class="o">=</span> <span class="n">LazyPatentCorpus</span><span class="p">()</span>
    <span class="n">lzy</span><span class="o">.</span><span class="n">init_by_filenames</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">records</span><span class="p">)</span>

    <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pd</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lzy</span><span class="o">.</span><span class="n">documents</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">classifications</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">as_string</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">pd</span><span class="o">.</span><span class="n">classifications</span><span class="p">]</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">classifications</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">claim1_text</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">claimset</span><span class="o">.</span><span class="n">get_claim</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">claim1_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">current_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">claim1_text</span><span class="p">,</span> <span class="n">classifications</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">500</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Saving a checkpoint at {0} files&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Current data = &quot;</span><span class="p">,</span> <span class="n">current_data</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} claims saved&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre>Loading data
12000 claims and classifications loaded
</pre></div>


<h2>2. Pre-Processing Data</h2>
<p>The code above generates a list of data samples of the form (claim1_text, classifications), where claim1_text is a string representing the text of claim 1 and classifications is a list of strings.</p>
<p>From an inspection of the data, there is some preprocessing to be performed:
- we need to filter out "cancelled" claims - these have the text '(canceled)' rather than any claim text;
- we need to convert the list of string classifications to just a Section letter; and
- sometimes we have strange or non-standard character encodings that can screw up our text processing, the character encodings can thus be cleaned and any non-standard characters replaced.</p>
<h3>Removing Cancelled Claimes</h3>
<p>The following codes removes claims that are cancelled (these just have "(canceled)" as text).</p>
<div class="highlight"><pre><span class="c1"># Check for and remove &#39;cancelled&#39; claims</span>
<span class="n">no_cancelled</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="s1">&#39;(canceled)&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;There are now {0} claims after filtering out cancelled claims&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">no_cancelled</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre>There are now 11239 claims after filtering out cancelled claims
</pre></div>


<h3>Extract Section Letter</h3>
<p>Then we filter the data to extract the first level classification. For simplicity we take the first classification (if there are several classifications) where the data exists. An example data entry is set out below.</p>
<div class="highlight"><pre><span class="c1"># Get classification in the form of A-H</span>
<span class="n">cleaner_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">no_cancelled</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">classification</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">cleaner_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">classification</span><span class="p">)</span>
            <span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;There are now {0} claims after extracting classifications&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cleaner_data</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre>There are now 11238 claims after extracting classifications
</pre></div>


<h3>Cleaning Characters</h3>
<p>To help further processing we will clean the characters of the text. The patentdata library has a function that replaces certain characters (e.g. different versions of '"' or '-') with a reduced set of common printable characters.</p>
<div class="highlight"><pre><span class="c1"># Clean the characters in the data to use a reduced set of printable characters</span>
<span class="c1"># There is a function in patentdata to do this</span>
<span class="kn">from</span> <span class="nn">patentdata.models.lib.utils</span> <span class="kn">import</span> <span class="n">clean_characters</span>

<span class="n">cleaner_data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">clean_characters</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">cleaner_data</span><span class="p">]</span>
</pre></div>


<h3>Example</h3>
<p>Here is an example of one data sample following processing:</p>
<div class="highlight"><pre><span class="n">cleaner_data</span><span class="p">[</span><span class="mi">55</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="o">(</span><span class="s1">&#39;\n1. A sensing assembly for sensing a level of liquid in a reservoir, said sensing assembly comprising: \na first input port for receiving a first input voltage signal; \na second input port for receiving a second input voltage signal; \nan excitation circuit electrically connected to said first and second input ports for receiving the first and second input voltage signals and for generating a first excitation signal and a second excitation signal, said excitation circuit includes first and second excitation electrodes extending along a portion of the reservoir, said first and second excitation electrodes disposed adjacent to and separated by said first receiving electrode; and \na receiving circuit disposed adjacent said excitation circuit defining a variable capacitance with said excitation circuit, wherein said receiving circuit includes first and second receiving electrodes extending along a portion of the reservoir and a first trace connected to ground and extending between said first receiving electrode and said first and second excitation electrodes, wherein said first receiving electrode extends along a first non-linear path and said second receiving electrode extends along a second non-linear path differing from said first non-linear path, said receiving circuit producing an output voltage signal variable with the level of liquid in the reservoir due to capacitance changes between said excitation circuit and said receiving circuit due to dielectric changes created by the liquid. \n\n&#39;</span><span class="o">,</span>
 <span class="s1">&#39;G&#39;</span><span class="o">)</span>
</pre></div>


<h3>Saving the Data - Interim</h3>
<p>Let's quickly save our claims and classifications as a Pickle file called "raw_data". This saves us having to do the steps set out above again if something goes wrong.</p>
<div class="highlight"><pre><span class="n">raw_data</span> <span class="o">=</span> <span class="n">cleaner_data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;raw_data.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>


<h2>Interlude - Exploring the Data</h2>
<p>Here we will take a little interlude to explore our data and get a feel for its structure.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">data_in_words</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="c1"># Here is an example</span>
<span class="n">data_in_words</span><span class="p">[</span><span class="mi">55</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre>[&#39;1&#39;, &#39;.&#39;, &#39;A&#39;, &#39;sensing&#39;, &#39;assembly&#39;, &#39;for&#39;, &#39;sensing&#39;, &#39;a&#39;, &#39;level&#39;, &#39;of&#39;]
</pre></div>


<h4>Claim Length</h4>
<p>Let's start with looking at the longest claim.</p>
<div class="highlight"><pre><span class="c1"># What is our maximum claim length?</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Max claim length = {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data_in_words</span><span class="p">])))</span>
</pre></div>


<div class="highlight"><pre>Max claim length = 6134
</pre></div>


<p>That's long! 6134 words.   </p>
<p>Let's have a look at how the length (in words) of our claims is distributed.</p>
<div class="highlight"><pre><span class="n">claim_length_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data_in_words</span><span class="p">])</span>
</pre></div>


<p>We can quickly have a look at a histogram of the lengths.</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">claim_length_counter</span><span class="o">.</span><span class="n">elements</span><span class="p">()),</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">claim_length_counter</span><span class="o">.</span><span class="n">elements</span><span class="p">()),</span> <span class="n">bins</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Claim 1 - Word Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;No. of claims&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Claim Word Length&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.text.Text at 0x7f456d132da0&gt;
</pre></div>


<p><img alt="png" src="output_23_1.png" /></p>
<p>As we can see the 6134 length claim is a clear outlier. The distribution is focused around 0 to around 600 words. Let's zoom in on that.</p>
<div class="highlight"><pre><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">claim_length_counter</span><span class="o">.</span><span class="n">elements</span><span class="p">()),</span> <span class="n">bins</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Claim 1 - Word Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;No. of claims&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Claim Word Length&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.text.Text at 0x7f456d0a3278&gt;
</pre></div>


<p><img alt="png" src="output_25_1.png" /></p>
<p>When applying deep learning algorithms we often need to set a maximum sequence length. When dealing with words, this is equal to the maximum number of words expected in a claim. A lower maximum sequence length will help reduce our training time. </p>
<p>If we take all claims of less than 250 words this would still include most of the data.  </p>
<h4>Words</h4>
<p>Now let's have a look at the words themselves.</p>
<div class="highlight"><pre><span class="c1"># Look at the words in our claims</span>
<span class="n">word_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data_in_words</span><span class="p">],</span> <span class="nb">list</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;There are {0} different tokens in our dataset.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_counter</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;The 100 most common words are:&quot;</span><span class="p">,</span> <span class="n">word_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span class="nt">There</span> <span class="nt">are</span> <span class="nt">34075</span> <span class="nt">different</span> <span class="nt">tokens</span> <span class="nt">in</span> <span class="nt">our</span> <span class="nt">dataset</span><span class="o">.</span>

<span class="nt">The</span> <span class="nt">100</span> <span class="nt">most</span> <span class="nt">common</span> <span class="nt">words</span> <span class="nt">are</span><span class="o">:</span> <span class="cp">[</span><span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="mi">103372</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">83989</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="mi">66878</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="mi">55405</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="mi">44003</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="mi">37112</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;;&#39;</span><span class="p">,</span> <span class="mi">30142</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">22809</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;said&#39;</span><span class="p">,</span> <span class="mi">22647</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="mi">20018</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="mi">18044</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="mi">17946</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;an&#39;</span><span class="p">,</span> <span class="mi">17519</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="mi">16033</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;for&#39;</span><span class="p">,</span> <span class="mi">15347</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="mi">14532</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="mi">12720</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;comprising&#39;</span><span class="p">,</span> <span class="mi">12718</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;at&#39;</span><span class="p">,</span> <span class="mi">12212</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="mi">12028</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="mi">11834</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="mi">11096</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;or&#39;</span><span class="p">,</span> <span class="mi">10910</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;from&#39;</span><span class="p">,</span> <span class="mi">10621</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;with&#39;</span><span class="p">,</span> <span class="mi">10478</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="mi">9265</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;least&#39;</span><span class="p">,</span> <span class="mi">8529</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;by&#39;</span><span class="p">,</span> <span class="mi">8240</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;wherein&#39;</span><span class="p">,</span> <span class="mi">8237</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="mi">8031</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="mi">7425</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;having&#39;</span><span class="p">,</span> <span class="mi">6846</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;device&#39;</span><span class="p">,</span> <span class="mi">6228</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;which&#39;</span><span class="p">,</span> <span class="mi">6177</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="mi">5856</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;plurality&#39;</span><span class="p">,</span> <span class="mi">5494</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;each&#39;</span><span class="p">,</span> <span class="mi">5283</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;portion&#39;</span><span class="p">,</span> <span class="mi">4687</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="mi">4675</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;surface&#39;</span><span class="p">,</span> <span class="mi">4492</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;signal&#39;</span><span class="p">,</span> <span class="mi">4193</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;being&#39;</span><span class="p">,</span> <span class="mi">3993</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="mi">3977</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="mi">3739</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;between&#39;</span><span class="p">,</span> <span class="mi">3621</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="mi">3548</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">,</span> <span class="mi">3547</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="mi">3497</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;member&#39;</span><span class="p">,</span> <span class="mi">3353</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="mi">3209</span><span class="p">)</span><span class="cp">]</span>
</pre></div>


<h4>Section Labels</h4>
<p>Let us now have a look at our Section classes. </p>
<div class="highlight"><pre><span class="n">class_count</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">cleaner_data</span><span class="p">])</span>
<span class="n">class_count</span>
</pre></div>


<div class="highlight"><pre>Counter({&#39;A&#39;: 1777,
         &#39;B&#39;: 1449,
         &#39;C&#39;: 865,
         &#39;D&#39;: 54,
         &#39;E&#39;: 269,
         &#39;F&#39;: 735,
         &#39;G&#39;: 3335,
         &#39;H&#39;: 2754})
</pre></div>


<p>What is quite interesting is we see that the data is mainly clusted around class A, B, G and H. Classes C, D, E and F have a limited number of associated claims.  </p>
<p><strong><em>A future extension to this project may be to balance the data samples. This may improve performance.</em></strong></p>
<h1>3. Transform the Data</h1>
<p>Machine learning algorithms generally work with numbers (typically floats of 32 or 64 bits). These numbers are arranged in multi-dimensional arrays called tensors.</p>
<p>Most machine learning algorithms expect a training set of data to be formatted into an 'X' tensor. One dimension of this tensor represents the different training samples. Another set of dimenions then represent the data of each training sample. For example, if each claim was represented as a one-dimensional vector [3, 1, 2, ..., 3] then X might comprise a matrix where rows of the matrix represent different samples and the columns of the matrix represent different features of the sample.</p>
<p>For supervised classification tasks each sample has an associated label. These are typically expected in a 'Y' tensor. Again, one dimension of this tensor represents the different training samples, while another set of dimensions represents the actual classification value for a particular sample. For example, in a simple binary case, Y may comprise a vector of 0 or 1 values.</p>
<p>In many supervised learning cases, we are thus looking to map an array (a row of our X tensor) to a scalar representing a group label.</p>
<h2>Working with Text</h2>
<p>When classifying images or measurement data, we generally already have numeric input data. However, text is different - we generally have a long string of characters as input. We thus need to convert our text data into a numeric vector. Moreover, we generally need a fixed-length input vector, whereas our text data can vary in length. </p>
<p>In this project we will look at two ways this can be done:
- counting words; and
- using recurrent neural networks.</p>
<p>As differences in converting the text may lead to differences in performance, their implementation may be revisited when trying to improve our results. Initially we will use a standardised set of function, then look to tweak these later.</p>
<h3>Counting Words</h3>
<p>A common techique to converting text to a numeric vector is by creating an array of length <em>l</em> that stores count values for the <em>l</em> most common words across the corpus of data. These are sometimes referred to as "bag-of-words" models, as they treat the text as a collection of words without considering word order.</p>
<p>There are <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">two widely-used flavours</a> of word counts:
- term frequency (TF) vectors: these count the occurrences of each word; and
- term frequency - document inverse frequency vectors: these weight the term counts to decrease the relevance of words that commonly occur across the corpus.</p>
<p>There are functions in gensim, scikit-learn and keras that generate numeric vectors from text data based on these metrics. We will apply some below.</p>
<h3>Using Recurrent Neural Networks</h3>
<p>Counts of words do not take word order into account. Recurrent neural networks are a form of neural network designed to process sequences of data values. In our case, we can convert each word in a claim into an integer index in a dictionary, and then use the resulting list of integers as input for a recurrent neural network.</p>
<p>In this post we will look at how to shape numeric X and Y vectors based on our "claim and classification" data (the "raw_data" above). In other posts we may use built in functions of machine learning libraries to do this for us.</p>
<hr />
<h2>Creating Y Vector</h2>
<p>Creating numeric Section label data is relatively straightforward and doesn't leave much room for tinkering. We will apply this first.</p>
<p>We can encode our numeric data in several ways, and different machine learning algorithms prefer different formats. Two popular formats are:
- a single integer value representing the class (e.g. A = 0, B = 1 etc); and
- a one-hot encoding where a 1 in an array indicates the class (e.g. [1, 0, ...0] = A, [0, 1, ...0] = B).</p>
<p>We will create and save both.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">Y_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>

<span class="c1"># encode class values as integers</span>
<span class="n">label_e</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_e</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y_class</span><span class="p">)</span>
<span class="n">encoded_Y</span> <span class="o">=</span> <span class="n">label_e</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Y_class</span><span class="p">)</span>
<span class="c1"># convert integers to dummy variables (i.e. one hot encoded)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">encoded_Y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our classes are now a matrix of {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Original label: {0}; Converted label: {1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">Y_class</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre><span class="nt">Our</span> <span class="nt">classes</span> <span class="nt">are</span> <span class="nt">now</span> <span class="nt">a</span> <span class="nt">matrix</span> <span class="nt">of</span> <span class="o">(</span><span class="nt">11238</span><span class="o">,</span> <span class="nt">8</span><span class="o">)</span>
<span class="nt">Original</span> <span class="nt">label</span><span class="o">:</span> <span class="nt">A</span><span class="o">;</span> <span class="nt">Converted</span> <span class="nt">label</span><span class="o">:</span> <span class="cp">[</span> <span class="mi">1</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span>  <span class="mi">0</span><span class="nx">.</span><span class="cp">]</span>
</pre></div>


<h2>Creating X Vector - TD-IDF</h2>
<p>Initially, we will use a TD-IDF count to convert our text into a fixed-length vector. We can then see later whether recurrent neural networks improve our classification accuracy.  </p>
<p>Luckily keras provides a <a href="https://keras.io/preprocessing/text/#tokenizer">handy utility</a> to tokenise our text data and output a fixed-length TD-IDF count based on the top <em>l</em> words. We saw above that our claims had ~35,000 unique tokens. We'll initially try with a vector limited to 5000 entries. The length of our vector is another hyperparameter we can play with later. The keras tokeniser also converts the text to lowercase and filters out punctuation (see the link above to change the default parameters).</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>

<span class="c1"># create the tokenizer</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="c1"># fit the tokenizer on the documents</span>
<span class="n">t</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our data has the following dimensionality: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;An example array is: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre>Our data has the following dimensionality:  (11238, 5000)
An example array is:  [ 0.          0.          2.43021996  2.08331543  1.71570602  2.52741068
  4.87087867  2.99092954  2.24937914  0.          2.06184914  2.64736857
  0.6933252   1.79712096  1.15982119  0.          1.47794342  1.26289711
  0.          1.16317528  0.          1.29314655  0.          0.
  1.38569402  1.36640725  0.          3.80265818  0.          0.
  2.92091401  0.          0.          0.          8.0221356   5.73719819
  0.          0.          0.          0.          0.          1.89505548
  0.          0.          0.          3.76681667  1.94774291  0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          2.32203465
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          2.45289367  0.          0.          0.
  0.          0.          4.79789823  0.          0.          3.16753991
  0.          0.          0.          0.          5.80378253  2.41571931
  2.36056393  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.        ]
</pre></div>


<h2>Save Our Data</h2>
<p>Now we can save our X and Y data to load for testing a set of machine learning algorithms.</p>
<div class="highlight"><pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;encoded_data.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">),</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
      <li class="list-inline-item"><a href="./authors.html">Authors</a></li>
    <li class="list-inline-item"><a href="./archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="./categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="./tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>